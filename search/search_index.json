{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Computa\u00e7\u00e3o em Nuvem","text":"Edi\u00e7\u00e3o <p>2025.1</p>"},{"location":"#kit-a","title":"KIT-A","text":"<p>Antonio Lucas Michelon de Almeida</p> <p>Pedro Nery Affonso dos Santos</p>"},{"location":"#entregas","title":"Entregas","text":"<ul> <li> Roteiro 1 </li> <li> Roteiro 2</li> <li> Roteiro 3</li> <li> Roteiro 4</li> <li> Projeto</li> </ul>"},{"location":"#referencias","title":"Refer\u00eancias","text":"<p>Material for MkDocs</p>"},{"location":"projeto/main/","title":"Projeto 2025.1","text":"<p> O projeto consiste na constru\u00e7\u00e3o e desenvolvimento de uma API RESTful para cadastro e autentica\u00e7\u00e3o de usu\u00e1rios, utilizando ferramentas do framework FastAPI </p> <p> Ap\u00f3s a constru\u00e7\u00e3o da API, o projeto deve ser dockerizado, publicado no Docker Hub e, por fim, implantado no AWS. </p> <p> Para uma melhor organiza\u00e7\u00e3o do projeto, ele foi dividido em tr\u00eas entregas. </p>"},{"location":"projeto/main/#objetivo-da-avaliacao","title":"Objetivo da Avalia\u00e7\u00e3o","text":"<p> Avaliar o dom\u00ednio dos alunos em: </p> <ul> <li>Containeriza\u00e7\u00e3o local com Docker Compose</li> <li>Deploy em ambiente de nuvem com AWS Lightsail</li> <li>Conex\u00e3o segura com banco de dados</li> <li>Estrutura\u00e7\u00e3o de aplica\u00e7\u00e3o web com FastAPI</li> <li>Boas pr\u00e1ticas de c\u00f3digo, documenta\u00e7\u00e3o e custo</li> </ul>"},{"location":"projeto/main/#entrega-1","title":"Entrega 1","text":""},{"location":"projeto/main/#construcao-da-api","title":"Constru\u00e7\u00e3o da API","text":"<p> Este projeto consiste em desenvolver uma API RESTful em FastAPI, com tr\u00eas endpoints b\u00e1sicos: </p> <ul> <li> POST /registrar: Usu\u00e1rio entra com <code>nome</code>, <code>email</code> e <code>senha</code>.         <ul> <li>C\u00f3digo 200: (Sucesso) <code>email</code> n\u00e3o encontrado na base de dados, faz o registro de um novo usu\u00e1rio, com senha codificada em formato <code>HASH</code>, e retorna o JWT Token.</li> <li>C\u00f3digo 409: (Error) <code>email</code> j\u00e1 se enconta cadastrado, cancela opera\u00e7\u00e3o.</li> </ul> </li> <li> POST /login: Usu\u00e1rio entra com <code>email</code> e <code>senha</code>.         <ul> <li>C\u00f3digo 200: (Sucesso) Conta associada ao <code>email</code> \u00e9 encontrada e a <code>senha</code> recebida bate com a senha codificada no banco de dados, faz o login do usu\u00e1rio e retorna o JWT Token..</li> <li>C\u00f3digo 401: (Error) <code>email</code> recebido n\u00e3o \u00e9 encontrado na base de dados</li> <li>C\u00f3digo 401: (Error) <code>senha</code> n\u00e3o confere com a senha codificada na base</li> </ul> </li> <li> GET /consultar:          <ul> <li>C\u00f3digo 200: (Sucesso) Verifica\u00e7\u00e3o bem sucedida do JWT Token e requisi\u00e7\u00e3o feita com sucesso \u00e0 Awesome API, cuja resposta \u00e9 devolvida ao cliente: a cota\u00e7\u00e3o atual do Euro e do Dol\u00e1r em rela\u00e7\u00e3o ao Real</li> <li>C\u00f3digo 403: (Error) JWT Token inv\u00e1lido ou ausente no header.</li> </ul> </li> </ul> <p></p> <p>Documenta\u00e7\u00e3o dos endpoints da API</p> <p> Ap\u00f3s a implementa\u00e7\u00e3o de cada um dos endpoints, tratando cada caso poss\u00edvel mencionado anteriormente, e integra\u00e7\u00e3o com o banco de dados PostgreSQL, \u00e9 necess\u00e1rio testar os endpoints para passar para a pr\u00f3xima etapa. </p> V\u00eddeo mostrando funcionalidades da API (Testes) <p></p> <p>Teste do endpoint \"/registrar\" (Success: Cadastro realizado)</p> <p></p> <p>Teste do endpoint \"/registrar\" (Fail: Email j\u00e1 registrado)</p> <p></p> <p>Teste do endpoint \"/login\" (Success: Login bem-sucedido)</p> <p></p> <p>Teste do endpoint \"/login\" (Fail: Email n\u00e3o registrado)</p> <p></p> <p>Teste do endpoint \"/login\" (Fail: Senha incorreta)</p> <p></p> <p>Teste do endpoint \"/consultar\" (Success: [Retorna cota\u00e7\u00e3o Dolar e Euro])</p> <p></p> <p>Teste do endpoint \"/consultar\" (Fail: JWT ausente ou inv\u00e1lido)</p>"},{"location":"projeto/main/#dockerizing","title":"Dockerizing","text":"<p> Com o c\u00f3digo da API pronto, a pr\u00f3xima etapa \u00e9 dockerizar/containerizar dos 2 servi\u00e7os juntos: a aplica\u00e7\u00e3o e o banco de dados. Dentro do docker, \u00e9 necess\u00e1rio a cria\u00e7\u00e3o de um arquivo <code>Dockerfile</code> (de acordo com a linguagem e ambiente de execu\u00e7\u00e3o escolhidos) e <code>compose.yaml</code> para a execu\u00e7\u00e3o da aplica\u00e7\u00e3o, sendo capaz de se conectar ao banco de dados e realizar as opera\u00e7\u00f5es de CRUD.  </p> <p> N\u00e3o s\u00f3 isso, mas a aplica\u00e7\u00e3o deve ser autocontida, ou seja, deve ser poss\u00edvel executar a aplica\u00e7\u00e3o apenas com o comando <code>docker compose up</code>. </p> <p> Ao final da Dockeriza\u00e7\u00e3o, a sa\u00edda desse comando devolve: </p> <pre><code>&gt; docker compose up -d\n[+] Running 2/2\n \u2714 Container inspercloud-projeto-db-1   Running\n \u2714 Container inspercloud-projeto-app-1  Running  \n\n&gt; docker compose ps\nNAME                        IMAGE                   SERVICE   STATUS         PORTS\ninspercloud-projeto-app-1   antoniolma/app:v1.0.1   app       Up 3 seconds   0.0.0.0:8080-&gt;80/tcp\ninspercloud-projeto-db-1    postgres:17             db        Up 8 seconds   0.0.0.0:5432-&gt;5432/tcp\n</code></pre> <p> A organiza\u00e7\u00e3o do diret\u00f3rio ao final do projeto: </p> <pre><code>\u251c\u2500\u2500 api/\n\u2502   \u251c\u2500\u2500 Dockerfile          # Dockerfile que instala depend\u00eancias e copia api/app/\n\u2502   \u251c\u2500\u2500 requirements.txt    # depend\u00eancias para o python\n\u2502   \u2514\u2500\u2500 app/                # c\u00f3digo com FastAPI\n\u2502       \u251c\u2500\u2500 app.py\n\u2502       \u2514\u2500\u2500 models.py\n\u251c\u2500\u2500 .env                    # vari\u00e1veis de ambiente (POSTGRES_*, DATABASE_URL, SECRET_KEY\u2026)\n\u2514\u2500\u2500 docker-compose.yaml     # orquestra\u00e7\u00e3o dos servi\u00e7os db + app\n</code></pre> <p> Para subir o container no Docker Hub, foram utilizados os seguintes comandos: </p> <ol> <li> Login no Docker Hub:     <pre><code>docker login\n</code></pre> </li> <li> Build da imagem e inicializa\u00e7\u00e3o dos containers:     <pre><code>docker compose up --build\n</code></pre> </li> <li> Push da vers\u00e3o atual para o Docker Hub:     <pre><code>docker push antoniolma/app  \n</code></pre> </li> </ol> <p> Por\u00e9m, para apenas utilizar a aplica\u00e7\u00e3o, ao baixar o projeto no reposit\u00f3rio do Github, basta utilizar o comando: </p> <pre><code>docker compose up\n</code></pre> <p> Caso tenha curiosidade, \u00e9 poss\u00edvel encontrar o projeto nos links: <ul> <li> Link projeto Github </li> <li> Link projeto DockerHub </li> </ul> </p>"},{"location":"projeto/main/#entrega-2","title":"Entrega 2","text":""},{"location":"projeto/main/#projeto-fastapi-no-aws-lightsail","title":"Projeto FastAPI no AWS Lightsail","text":"<p> A parte 2 do projeto consiste em: </p> <ul> <li>         Implantar sua aplica\u00e7\u00e3o (ex:FastAPI) utilizando o AWS Lightsail Container Service.     </li> <li>         Configurar um banco de dados gerenciado (ex:PostgreSQL) no Lightsail.     </li> <li>         Conectar sua aplica\u00e7\u00e3o ao banco de dados.     </li> <li>         Gerenciar e monitorar o custo do servi\u00e7o em produ\u00e7\u00e3o. (Sua conta n\u00e3o pode gastar mais de 50 dolares m\u00eas)     </li> </ul> <p> Ap\u00f3s criar o usu\u00e1rio na AWS e obter as credenciais necess\u00e1rias para fazer o login na conta, foi iniciado o processo de cria\u00e7\u00e3o do Container da aplica\u00e7\u00e3o feita na Entrega 1. </p>"},{"location":"projeto/main/#container","title":"Container","text":"<p> Dentro do Lightsail, clicando em Create container service, foram escolhidas as seguintes configura\u00e7\u00f5es: </p> <ul> <li> Service name: <code>fastapi-service</code>.     </li> <li> Power: <code>Nano</code> (varia de acordo com a necessidade do projeto).     </li> <li> Scale: <code>1</code> (N\u00famero de inst\u00e2ncias).     </li> </ul> <p></p> <p>Configura\u00e7\u00e3o do container (Lightsail)</p> <p> Ao criar o Container, inicialmente n\u00e3o foi definido nenhum deploy inicial em sua configura\u00e7\u00e3o, isso foi feito somente ap\u00f3s a cria\u00e7\u00e3o do Database e ter seu endpoint. </p>"},{"location":"projeto/main/#database","title":"Database","text":"<p> Na aba Databases, dentro do Lightsail, foi criado um banco de dados com as seguintes configura\u00e7\u00f5es: </p> <ul> <li> Database engine: <code>PostgreSQL</code> (porta <code>5432</code>).     </li> <li> Database name: <code>cloud</code>.     </li> <li> Master username: <code>cloud</code>.     </li> <li> Availability zone: <code>A mesma da aplica\u00e7\u00e3o</code>.     </li> <li> Public mode: <code>Ative para permitir conex\u00f5es externas</code>.     </li> </ul> <p></p> <p>Configura\u00e7\u00e3o do database (Lightsail)</p> <p> Ap\u00f3s um breve per\u00edodo de espera, o Database estava no ar. </p> <p> Com isso, anotamos o <code>endpoint</code> do banco de dados, o <code>nome de usu\u00e1rio</code> e <code>senha</code>, para realizar o deploy final do container com a imagem do DockerHub se comunicando com o banco de dados na AWS. </p>"},{"location":"projeto/main/#first-deployment-container","title":"First Deployment (Container)","text":"<p> Entrando no nosso Container, foi realizado a modifica\u00e7\u00e3o do deploy para que a aplica\u00e7\u00e3o conseguisse conversar com a aplica\u00e7\u00e3o. </p> <p> Essa modifica\u00e7\u00e3o consistiu na altera\u00e7\u00e3o de alguns campos: </p> <ul> <li> Container name: <code>fastapi-container</code>.     </li> <li> Image: <code>antoniolma/app:v3.0.0</code>.     </li> <li> Environment variables:         <ul> <li> DATABASE_URL:                  <code>postgresql://cloud:${POSTGRES_PASSWORD}@${AWS_ENDPOINT}:5432/dbmaster</code>.                  </li> <li> ALGORITHM: <code>HS256</code>.             </li> <li> SECRET_KEY: Key secreta para codificar as senhas dos usu\u00e1rios para HASH's.             </li> <li> AWESOME_API_KEY: Key para API de consulta do <code>d\u00f3lar</code> e <code>euro</code>.             </li> </ul> </li> <li>         Open Ports:         <ul> Port: <code>80</code> - Protocol: <code>HTTP</code> </ul> </li> <li>         Healt check path:         <ul> <code>/health-check</code> </ul> </li> </ul> <p></p> <p>Configura\u00e7\u00e3o do Deployment do container (Lightsail)</p> <p></p> <p>Deploy finalizado com Status: \u2705 Active</p> <p></p> <p>Diagrama </p> <p> Com a aplica\u00e7\u00e3o agora no ar, podemos fazer alguns testes parar verificar se tudo est\u00e1 funcionando como deveria. </p>"},{"location":"projeto/main/#testando-o-deploy","title":"Testando o deploy","text":"V\u00eddeo testando o Deploy na AWS <p> Caso tenha curiosidade, \u00e9 poss\u00edvel encontrar o deploy no link: <ul> <li>              https://fastapi-service.41apj1zjxa7qj.us-east-1.cs.amazonlightsail.com/          </li> </ul> </p>"},{"location":"roteiro1/main/","title":"Roteiro 1","text":""},{"location":"roteiro1/main/#introducao-e-objetivo","title":"Introdu\u00e7\u00e3o e Objetivo","text":"<p>O roteiro 1 contempla a funda\u00e7\u00e3o de toda a infraestrutura da Cloud que foi montada a partir de um KIT que contava com os seguintes componentes:</p> <ul> <li>1 NUC (main) com 10Gb e 1 SSD (120 Gb)</li> <li>1 NUC (server1) com 12Gb e 1 SSD (120 Gb)</li> <li>1 NUC (server2) com 16Gb e 2 SSD (120 Gb + 120 Gb)</li> <li>3 NUCs (server3, server4 e server5) com 32 Gb e 2 SSD (120 Gb + 120 Gb)</li> <li>1 Switch DLink DSG-1210-28 de 28 portas</li> <li>1 Roteador TP-Link TL-R470T+</li> </ul> <p>Ao longo deste roteiro, passaremos pelos seguintes passos:</p> <ul> <li> <p>Configura\u00e7\u00e3o do KIT via cabo</p> <ul> <li>Instala\u00e7\u00e3o do Ubuntu Server</li> <li>Instala\u00e7\u00e3o do MAAS</li> <li>Configura\u00e7\u00e3o do MAAS</li> <li>Reconfigura\u00e7\u00e3o do DHCP</li> <li>Cadastro dos servidores via MAAS</li> <li>Cria\u00e7\u00e3o das pontes OVS    </li> </ul> </li> <li> <p>Configura\u00e7\u00e3o do KIT via acesso remoto</p> <ul> <li>Configura\u00e7\u00e3o de um servidor de banco de dados Postgres</li> <li>Deploy de uma aplica\u00e7\u00e3o Django</li> <li>Utiliza\u00e7\u00e3o do Ansible</li> <li>...</li> </ul> </li> </ul> <p> Ao final deste roteiro, o objetivo principal \u00e9 termos, portanto, uma Cloud com um primeiro gerenciador de deploy instalado. A partir disso, o cliente j\u00e1 ser\u00e1 capaz de realizar requisi\u00e7\u00f5es ao servidor se estiver conectado \u00e0 rede Wi-Fi do Insper.  </p>"},{"location":"roteiro1/main/#montagem-do-roteiro","title":"Montagem do Roteiro","text":"<p> Todo roteiro apresenta uma primeira parte denonimnada Infra e uma segunda chamada de App. Os pontos tarefas dentro de cada parte s\u00e3o os passos seguidos para a realiza\u00e7\u00e3o do roteiro.  Este modelo de organiza\u00e7\u00e3o orientado por partes e tarefas ser\u00e1 utilizado em todos os roteiros. </p>"},{"location":"roteiro1/main/#infra","title":"Infra","text":""},{"location":"roteiro1/main/#parte-0-enderecos-mac-das-nucs-e-ip-do-roteador","title":"Parte 0: Endere\u00e7os MAC das NUCs e IP do roteador","text":"<p> Antes de iniciar qualquer instala\u00e7\u00e3o, foi essencial capturar imagens de todos os endere\u00e7os MAC dos servidores (1 a 5).  </p> <p> Essa a\u00e7\u00e3o ocorreu simultaneamente \u00e0 Tarefa 1, pois, a partir da Tarefa 2, as fontes de alimenta\u00e7\u00e3o n\u00e3o podem mais ser desconectadas e, devido \u00e0 forma como o kit foi montado, n\u00e3o seria poss\u00edvel remover as NUCs de suas posi\u00e7\u00f5es para visualizar os endere\u00e7os sem deslig\u00e1-las da tomada. </p> <p> Al\u00e9m disso, foi realizado um pr\u00e9-roteiro relativo \u00e0 montagem de um cabo de rede. Com a conex\u00e3o ethernet, foi acessada a interface do roteador do KIT e alterou-se o seu endere\u00e7o de IP para 172.16.0.1. Mais detalhes sobre a escolha deste IP ser\u00e3o fornecidos na Tarefa 1.  </p> <p></p> <p>Tela de configura\u00e7\u00e3o de IP do roteador e de m\u00e1scara de subrede</p>"},{"location":"roteiro1/main/#parte-1-instalacao-do-ubuntu-server","title":"Parte 1: Instala\u00e7\u00e3o do Ubuntu Server","text":"<p>Para a realiza\u00e7\u00e3o desta primeira tarefa, foram seguidos os passos descritos a seguir para configurar a NUC main:</p> <ol> <li> <p>Download da imagem do Ubuntu Server </p> <ul> <li>Download da vers\u00e3o 22.04 LTS do Ubuntu Server a partir do site oficial do Ubuntu.</li> </ul> </li> <li> <p>Cria\u00e7\u00e3o um pendrive boot\u00e1vel </p> <ul> <li>Uso do software Rufus para gravar a imagem no pendrive.</li> </ul> </li> <li> <p>Acesso \u00e0 BIOS </p> <ul> <li>Com o Pendrive conectado, a BIOS foi acessada a partir a tecla <code>F12</code> durante a inicializa\u00e7\u00e3o.  </li> <li>Configura\u00e7\u00e3o de ordem de boot para priorizar o pendrive.  </li> </ul> </li> <li> <p>Configura\u00e7\u00f5es iniciais de instala\u00e7\u00e3o do Ubuntu Server </p> <ul> <li>Reinicializa\u00e7\u00e3o da NUC.  </li> <li>Sele\u00e7\u00e3o da op\u00e7\u00e3o \"Install Ubuntu Server\" no menu inicial.  </li> <li>Sele\u00e7\u00e3o de idioma e layout do teclado.</li> </ul> </li> <li> <p>Configura\u00e7\u00f5es de subrede, IP, Gateway e DNS </p> <ul> <li>M\u00e1scara de Rede: 172.16.0.0/20</li> <li>Endere\u00e7o IP da NUC main</li> <li>Gateway: IP do roteador</li> <li>Name servers: DNS do Insper</li> </ul> </li> </ol> <p></p> <p>Tela de configura\u00e7\u00e3o de M\u00e1scara de Rede, IP, Gateway (roteador) e DNS do Insper  </p> <p>Explica\u00e7\u00e3o te\u00f3rica das configura\u00e7\u00f5es feitas:</p> <p> Para que dispositivos possam estabelecer uma comunica\u00e7\u00e3o entre si, \u00e9 primordial que eles se encontrem na mesma rede. A configura\u00e7\u00e3o da subrede, IP, gateway e servidores DNS permite a correta comunica\u00e7\u00e3o entre os dispositivos e o acesso \u00e0 internet ou a outros servi\u00e7os de rede. </p> <p> A m\u00e1scara de rede define o intervalo de endere\u00e7os IP dispon\u00edveis dentro da subrede. No caso da configura\u00e7\u00e3o com a m\u00e1scara `/20`, a rede pode conter at\u00e9 4.096 endere\u00e7os IP, garantindo escalabilidade para futuras expans\u00f5es. </p> <p> O endere\u00e7o IP da NUC main \u00e9 um IP est\u00e1tico atribu\u00eddo manualmente ao servidor principal (nesse caso, 172.16.0.3, pois o switch assumiu o IP 172.16.0.2), garantindo que ele tenha sempre o mesmo endere\u00e7o na rede local, facilitando a administra\u00e7\u00e3o e a comunica\u00e7\u00e3o com outros dispositivos. </p> <p> O gateway corresponde ao IP do roteador (que foi configurado na Tarefa 0 como 172.16.0.1), que atua como a ponte entre a rede interna e redes externas, como a internet. Sem a configura\u00e7\u00e3o correta do gateway, os dispositivos na rede local n\u00e3o conseguiriam acessar servi\u00e7os externos. </p> <p> Os name servers (DNS) s\u00e3o respons\u00e1veis pela resolu\u00e7\u00e3o de nomes de dom\u00ednio, convertendo endere\u00e7os amig\u00e1veis, como `www.google.com`, em endere\u00e7os IP. Utilizar os servidores DNS do Insper garante uma resolu\u00e7\u00e3o eficiente e confi\u00e1vel dentro do ambiente da institui\u00e7\u00e3o. </p> <p>Na tela seguinte, referente ao archive mirror, foram aceitas as configura\u00e7\u00f5es que vieram por padr\u00e3o, conforme a print a seguir:</p> <p></p> <p>Tela de configura\u00e7\u00e3o padr\u00e3o do Ubuntu archive mirror </p> <p>Passo final: Cria\u00e7\u00e3o de usu\u00e1rio e configura\u00e7\u00f5es finais </p> <pre><code>hostname: main\n\nlogin: cloud\n\nsenha: clouda\n\nName Servers (DNS): DNS do Insper\n</code></pre> <p></p> <p>Tela de configura\u00e7\u00e3o do usu\u00e1rio da NUC main segundo as especifica\u00e7\u00f5es passadas</p> <p> Ap\u00f3s a conclus\u00e3o de todos os passos da instala\u00e7\u00e3o, foi realizado um reboot da NUC main e removido o pendrive.  </p>"},{"location":"roteiro1/main/#parte-2-maas-acesso-local","title":"Parte 2: MAAS - acesso local","text":""},{"location":"roteiro1/main/#instalacao","title":"Instala\u00e7\u00e3o","text":"<p> Para a instala\u00e7\u00e3o do MAAS na NUC main (que agora tem um sistema operacional), optou-se pela vers\u00e3o 3.5.3. No terminal do Ubuntu Server, foram utilizados os comandos a seguir: </p> <pre><code>$ sudo apt update &amp;&amp; sudo apt upgrade -y\n\n$ sudo snap install maas --channel=3.5/stable\n\n$ sudo snap install maas-test-db\n</code></pre> <p>Para verificar o devido funcionamento do MAAS instalado, foram realizados dois testes com o comando ping, ilustrados na foto a seguir:</p> <p></p> <p>Tela de teste de funcionamento por meio de pings</p> <p>Ap\u00f3s o teste feito com sucesso, foi realizado um acesso da NUC main via SSH com o comando a seguir (ssh usuario@IP):</p> <pre><code>$ ssh cloud@172.16.0.3\n</code></pre>"},{"location":"roteiro1/main/#configuracao","title":"Configura\u00e7\u00e3o","text":"<p> Dentro da rede local, o MAAS foi inicializado e criou-se o administrador cloud, que ser\u00e1 necess\u00e1rio para poteriormente ser poss\u00edvel acessar o dashboard. Antes da inicializa\u00e7\u00e3o foi necess\u00e1rio realizar um reboot. </p> <pre><code>$ sudo maas init region+rack --maas-url http://172.16.0.3:5240/MAAS --database-uri maas-test-db:///\n\n$ sudo maas createadmin\n</code></pre> <p>Em seguida, foi gerado um par de chaves para autentica\u00e7\u00e3o. Ap\u00f3s gerada, a chave p\u00fablica foi copiada.</p> <pre><code>$ ssh-keygen -t rsa\n\n$ cat ./.ssh/id_rsa.pub\n</code></pre> <p> Utilizando o IP atribu\u00eddo \u00e0 NUC main e a porta padr\u00e3o do MAAS, foi poss\u00edvel acessar o Dashboard via protocolo HTTP (http://172.16.0.3:5240/MAAS). O login foi realizado atrav\u00e9s do admin criado nos passos anteriores. </p> <p></p> <p>Dashboard do MAAS</p> <p> Conforme ilustrado acima, a tela inicial do MAAS apresenta um dashboard com informa\u00e7\u00f5es sobre o estado atual dos servidores gerenciados. O dashboard \u00e9 composto por diversos pain\u00e9is, cada um exibindo informa\u00e7\u00f5es sobre um aspecto espec\u00edfico do ambiente gerenciado.  </p> <p> Utilizando a interface gr\u00e1fica do Dashboard, primeiramente, foi configurado um DNS forwarder utilizando o DNS do Insper (Networking &gt; DNS). </p> <p> Em seguida, foram importadas imagens do Ubuntu 22.04 LTS e Ubuntu 20.04 LTS em Configuration &gt; Images &gt; Ubuntu Releases e feito o upload da chave copiada no terminal SSH.  </p> <p> Por fim, foi passado o par\u00e2metro kernel net.ifnames=0 em Settings &gt; Configuration &gt; Kernel Parameters. </p>"},{"location":"roteiro1/main/#chaveando-o-dhcp","title":"Chaveando o DHCP","text":"<p> O DHCP (Dynamic Host Configuration Protocol) \u00e9 um protocolo de rede que permite a configura\u00e7\u00e3o autom\u00e1tica de dispositivos em uma rede IP. Ele \u00e9 principalmente respons\u00e1vel por atribuir dinamicamente endere\u00e7os IP, eliminando a necessidade de configura\u00e7\u00e3o manual, mas tamb\u00e9m pode assumir fun\u00e7\u00f5es como, por exemplo, definir a m\u00e1scara de sub-rede e fornecer servidores DNS. </p> <p> At\u00e9 o momento, o dispositivo da nossa sub-rede contendo este protocolo \u00e9 o roteador. Por\u00e9m, nesta etapa isto foi modificado. </p> <p> Primeiramente, dentro do MAAS Controller, o DHCP foi habilitado na NUC Main e, conforme ilustra a imagem a seguir, o Reserved Range foi alterado para iniciar em 172.16.11.1 e acabar em 172.16.14.255. </p> <p></p> <p>Tela de configura\u00e7\u00e3o dos Reserved Ranges dentro do MAAS Controller</p> <p> Al\u00e9m disso, como mais de um dispositivo n\u00e3o pode conter o protocolo DHCP dentro de uma mesma sub-rede (mais de um dispositivo tentando atribuir um IP a outro dispositivo automaticamente), tamb\u00e9m foi necess\u00e1rio desativar o DHCP no roteador. </p> <p></p> <p>Tela de desabilita\u00e7\u00e3o do DHCP no roteador</p> <p>A sa\u00fade do sistema tamb\u00e9m foi verificada a partir da p\u00e1gina de Controladores no Dashboard, ilustrada a seguir.</p> <p></p> <p>Tela de verifica\u00e7\u00e3o da sa\u00fade do sistema</p>"},{"location":"roteiro1/main/#comissionando-servidores","title":"Comissionando servidores","text":"<p> Com o DHCP agora devidamente chaveado, os servers 1 a 5 foram cadastrados como machines no Dashboard do MAAS.  Para tanto, foram resgatados os endere\u00e7os MAC capturados na Tarefa 0, alterada a op\u00e7\u00e3o Power Type para Intel AMT, configurada a senha `CloudComp6s!` para todos os servidores e o IP 172.16.15.X (X sendo o n\u00famero do servidor configurado). </p> <p> Ap\u00f3s a comiss\u00e3o autom\u00e1tica, todos os n\u00f3s apareceram com o status Ready e as especifica\u00e7\u00f5es de armazenamento das NUCs foram confirmadas. Al\u00e9m disso, o roteador foi adicionado como device. </p> <p></p> <p>Adicionando roteador como device</p>"},{"location":"roteiro1/main/#ovs-bridge","title":"OVS Bridge","text":"<p> Antes de possibilitar o acesso remoto ao KIT, um passo final foi criar, para cada servidor, uma ponte Open vSwitch (OVS). Todas as pontes tiveram o nome \"br-ex\" atribu\u00eddo a elas. A seguir, tem-se uma ilustra\u00e7\u00e3o de como a bridge ficou configurada para o server 1, na interface do dashboard do MAAS. </p> <p></p> <p>Interface do Server 1 ap\u00f3s a cria\u00e7\u00e3o da ponte Open vSwitch (OVS)</p>"},{"location":"roteiro1/main/#parte-3-maas-acesso-remoto","title":"Parte 3: MAAS - acesso remoto","text":"<p>Para que seja poss\u00edvel acessar o KIT remotamente, e n\u00e3o mais atrav\u00e9s do cabo na rede local, \u00e9 necess\u00e1rio realizar a cria\u00e7\u00e3o de um gateway NAT. </p> <p> A inten\u00e7\u00e3o por tr\u00e1s do acesso remoto \u00e9 que o computador seja capaz de conversar com o servidor main dentro da subrede configurada at\u00e9 agora apenas por meio de uma conex\u00e3o com a Rede Wi-Fi do Insper.  </p> <p> O NAT \u00e9 justamente o servi\u00e7o que possibilita que dispositivos dentro de uma rede privada acessem redes externas (como a internet) atrav\u00e9s de um \u00fanico endere\u00e7o IP p\u00fablico. Ele traduz os endere\u00e7os IP privados da subrede para o endere\u00e7o IP do roteador ao enviar pacotes para fora da rede e realiza o processo inverso ao receber respostas, garantindo assim a comunica\u00e7\u00e3o adequada entre os dispositivos. </p> <p></p> <p>Ilustra\u00e7\u00e3o te\u00f3rica de funcionamento da subrede</p> <p>Na ilustra\u00e7\u00e3o acima, a linha tracejada verde representa a conex\u00e3o entre um computador (que se encontra conectado \u00e0 LAN do Insper) com a NUC main da subrede privada que foi configurada at\u00e9 o momento. </p> <p> Para que fosse estabelecida tal conex\u00e3o, foram seguidas as instru\u00e7\u00f5es contidas no manual de uso do roteador do KIT para possibilitar o redirecionamento do dispositivo que tentasse se conectar \u00e0 portas 22 (padr\u00e3o SSH) e 5240 (padr\u00e3o MAAS) a partir do IP p\u00fablico 10.103.1.10, que \u00e9 o endere\u00e7o do roteador fora da rede privada. </p> <p>Ao final da configura\u00e7\u00e3o do NAT, portanto, a interface do roteador ficou da seguinte forma:</p> <p></p> <p>Interface do roteador ap\u00f3s a configura\u00e7\u00e3o do NAT</p> <p>A partir deste momento, portanto, passou a ser poss\u00edvel conectar-se ao dashboard do MAAS e aos demais servidores por meio do seguinte comando SSH no terminal do computador:</p> <pre><code>$ ssh cloud@10.103.1.10\n</code></pre>"},{"location":"roteiro1/main/#app","title":"App","text":"<p>Na parte da aplica\u00e7\u00e3o deste primeiro roteiro, foi realizado um deploy manual de uma aplica\u00e7\u00e3o simples em Django nos servidores.</p>"},{"location":"roteiro1/main/#parte-1-criacao-do-banco-de-dados","title":"Parte 1: Cria\u00e7\u00e3o do banco de dados","text":"<p>Acessando o terminal do server 1 via SSH, foi criado um usu\u00e1rio (senha: cloud) por meio dos comandos:</p> <pre><code>$ sudo apt update\n\n$ sudo apt install postgresql postgresql-contrib -y\n\n$ sudo su - postgres\n\n$ createuser -s cloud -W\n</code></pre> <p>Em seguida, foi criado o banco de dados e exposto o servi\u00e7o para acesso:</p> <pre><code>$ createdb -O cloud tasks\n\n$ nano /etc/postgresql/14/main/postgresql.conf\n</code></pre> <p>Saindo do usu\u00e1rio postgres, liberou-se o firewall e o servi\u00e7o foi reiniciado:</p> <pre><code>$ sudo ufw allow 5432/tcp\n\n$ sudo systemctl restart postgresql\n</code></pre> <p>Tarefa 1.1) Dentro do server1, status do banco de dados se mostra ativo.</p> <pre><code>$ sudo systemctl status postgresql\n</code></pre> <p></p> <p>Status do PostgreSQL vendo do server1</p> <p>Tarefa 1.2) Inicia a sess\u00e3o e utiliza do computador/servi\u00e7o remotamente, atrav\u00e9s da porta 5240, na MAIN. Servi\u00e7o acess\u00edvel da MAIN.</p> <pre><code>$ telnet localhost 5240\n</code></pre> <p></p> <p>Conex\u00e3o estabelecida entre a MAIN e o servi\u00e7o remoto</p> <p>Tarefa 1.3) Servi\u00e7o acess\u00edvel na pr\u00f3pria m\u00e1quina onde o postgresql foi instalado.</p> <pre><code>$ sudo su - postgres\n</code></pre> <p></p> <p>PostgreSQL acess\u00edvel de dentro do server em que est\u00e1 alocado</p> <p>Tarefa 1.4) Acessando a configura\u00e7\u00e3o do postgresql, foi poss\u00edvel verificar a porta na sess\u00e3o \u2018CONNECTIONS AND AUTHENTICATION\u2019</p> <pre><code>Comando: $ nano /etc/postgresql/14/main/postgresql.conf\n</code></pre> <p></p> <p>Configura\u00e7\u00f5es do PostgreSQL (default)</p>"},{"location":"roteiro1/main/#parte-2-aplicacao-django","title":"Parte 2: Aplica\u00e7\u00e3o Django","text":"<p> Ap\u00f3s requisitar acesso a uma m\u00e1quina em nosso servidor (Comando 1), e inserir o token da aba \u2018API keys\u2019 presente no Dashboard, solicitamos ao MaaS a aloca\u00e7\u00e3o de uma m\u00e1quina (Comando 2) e realizamos o deploy da nossa aplica\u00e7\u00e3o (Comando 3), sendo \u2018system_id\u2019 o id do server alocado, vis\u00edvel no link do Dashboard ao clicar na m\u00e1quina desejada. </p> <pre><code>$ maas login [login] http://172.16.0.3:5240/MAAS/    # Requisi\u00e7\u00e3o de m\u00e1quina\n$ maas [login] machines allocate name=[server_name]  # Solicita aloca\u00e7\u00e3o a MAIN\n$ maas [login] machine deploy [system_id]            # Deploy da aplica\u00e7\u00e3o\n</code></pre> <p> Acessando o servidor via SSH, clonamos o reposit\u00f3rio onde teremos a aplica\u00e7\u00e3o Django. Entrando no diret\u00f3rio tasks, fazemos a instala\u00e7\u00e3o das depend\u00eancias do reposit\u00f3rio.  </p> <pre><code>$ git clone https://github.com/raulikeda/tasks.git\n\n$ ./install.sh # Instala\u00e7\u00e3o das depend\u00eancias\n</code></pre> <p> Ap\u00f3s um breve reboot da m\u00e1quina, iremos acessar o arquivo \u2018/etc/hosts\u2019 para darmos permiss\u00e3o a nossa MAIN de utilizar a aplica\u00e7\u00e3o como administrador. Podemos verificar a conex\u00e3o com a aplica\u00e7\u00e3o com o comando: </p> <pre><code>$ wget http://[IP server_app]:8080/admin/ # Verificando conex\u00e3o com a aplica\u00e7\u00e3o\n</code></pre> <p> Agora, ao acessar o MaaS podemos criar um t\u00fanel do servi\u00e7o do servidor da aplica\u00e7\u00e3o na porta 8080 para nosso localhost na porta 8001 usando a conex\u00e3o SSH, desde que a porta 8001 n\u00e3o esteja sendo utilizada. Podemos ent\u00e3o acessar a p\u00e1gina de administrador do Django acessando no navegador o link: http://localhost:8001/admin/. </p> <pre><code>$ ssh cloud@10.103.0.X -L 8001:[IP server_app]:8080 # conectando via SSH\n</code></pre> <p>Tarefa 2.1) Dashboard do MAAS com as m\u00e1quinas.</p> <p></p> <p>Servidores na interface do MAAS</p> <p>Tarefa 2.2) Aba imagens, com as imagens sincronizadas</p> <p></p> <p>Imagens sincronizadas na interface do MAAS</p> <p>Tarefa 2.3) Da Aba de cada maquina mostrando os testes de hardware e commissioning com Status \"OK\"</p> <p>Server 1:</p> <p></p> <p>Testes de hardware do server 1</p> <p></p> <p>Comissioning do server 1</p> <p>Server 2:</p> <p></p> <p>Testes de hardware do server 2</p> <p></p> <p>Comissioning do server 2</p> <p>Server 3:</p> <p></p> <p>Testes de hardware do server 3</p> <p></p> <p>Comissioning do server 3</p> <p>Server 4:</p> <p></p> <p>Testes de hardware do server 4</p> <p></p> <p>Comissioning do server 4</p> <p>Server 5:</p> <p></p> <p>Testes de hardware do server 5</p> <p></p> <p>Comissioning do server 5</p>"},{"location":"roteiro1/main/#parte-3-checkpoint-status-dos-servidores-apos-a-instalacao-manual-do-django","title":"Parte 3: CHECKPOINT - Status dos servidores ap\u00f3s a instala\u00e7\u00e3o manual do Django","text":"<p> Com as mudan\u00e7as feitas na parte 2, antes de continuarmos precisamos garantir algumas coisas: </p> <p>Tarefa 3.1) As 2 M\u00e1quinas se mostram ativas e com seus IPs definidos no Dashboard do MaaS</p> <p></p> <p>Servidores na interface do MAAS</p> <p>Tarefa 3.2) A aplica\u00e7\u00e3o Django se encontra no ar, est\u00e1 conectada ao server e acess\u00edvel a partir do t\u00fanel</p> <p></p> <p>Aplica\u00e7\u00e3o Django conectada ao server e acess\u00edvel atrav\u00e9s do tunel</p>"},{"location":"roteiro1/main/#parte-4-ansible-deploy-automatizado-de-aplicacao","title":"Parte 4: Ansible - deploy automatizado de aplica\u00e7\u00e3o","text":"<p> Vamos partir para uma abordagem diferente agora. At\u00e9 o momento, temos apenas uma aplica\u00e7\u00e3o Django que foi instalada manualmente apenas no servidor 2. Contudo, \u00e9 comum que uma mesma aplica\u00e7\u00e3o seja alocada em mais de uma m\u00e1quina, pois podemos dividir a carga de acesso entre os n\u00f3s e, al\u00e9m disso, se um node cair o outro est\u00e1 no ar, para que nosso cliente acesse. </p> <p> Dessa forma, nesta parte foi criada uma segunda aplica\u00e7\u00e3o Django no servidor 3 que compartilha com o servidor 2 o mesmo banco de dados criado no servidor 1. Por\u00e9m, em vez de realizar os mesmos passos da instala\u00e7\u00e3o manual feita na parte 2, optou-se por utilizar o Ansible, um gerenciador de deploy que traz benef\u00edcios que ser\u00e3o detalhados mais adiante. </p> <p> Ap\u00f3s feito o deploy do terceiro servidor, o Ansible foi instalado na MAIN e um playbook foi criado no n\u00f3 3 para a instala\u00e7\u00e3o do Django. Os comandos utilizados foram: </p> <pre><code>$ sudo apt install ansible # Instala\u00e7\u00e3o do Ansible\n\n$ wget https://raw.githubusercontent.com/raulikeda/tasks/master/tasks-install-playbook.yaml # Playbook do Ansible\n\n$ ansible-playbook tasks-install-playbook.yaml --extra-vars server=[IP server3] # Instala\u00e7\u00e3o do playbook na m\u00e1quina 3\n</code></pre> <p>Tarefa 4.1) Print da tela do Dashboard do MAAS com as 3 M\u00e1quinas e seus respectivos IPs.</p> <p></p> <p>Dashboard do MaaS com os tr\u00eas servidores com deploy feito e seus respectivos IPs</p> <p>Tarefa 4.2) Print da aplica\u00e7\u00e3o Django, provando a conex\u00e3o com o server2</p> <p></p> <p>Aplica\u00e7\u00e3o Django funcionando localmente via servidor 2</p> <p>Tarefa 4.3) Print da aplica\u00e7\u00e3o Django, provando a conex\u00e3o com o server3</p> <p></p> <p>Aplica\u00e7\u00e3o Django funcionando localmente via servidor 3</p> <p>Tarefa 4.4) Diferen\u00e7a entre instalar manualmente a aplica\u00e7\u00e3o Django e utilizando o Ansible</p> <p> A diferen\u00e7a principal entre instalar manualmente uma aplica\u00e7\u00e3o Django em um host e utilizar o Ansible est\u00e1 na automa\u00e7\u00e3o, repetibilidade e efici\u00eancia da configura\u00e7\u00e3o do ambiente. </p> <p> No lugar de ter que manualmente configurar um ambiente virtual, instalar as depend\u00eancias necess\u00e1rias (armazenadas em um requirements.txt) e pacotes como Python, pip e depend\u00eancias do Django, o Ansible permite automatizar todo esse processo atrav\u00e9s de playbooks. </p> <p> Al\u00e9m de automatizar todas as etapas da instala\u00e7\u00e3o e configura\u00e7\u00e3o, o Ansible segue instru\u00e7\u00f5es precisas que proporcionam um gerenciamento de m\u00faltiplos servidores ao mesmo tempo, que \u00e9 \u00fatil para escalar a aplica\u00e7\u00e3o, e garantem um processo id\u00eantico de configura\u00e7\u00e3o na necessidade de mais m\u00e1quinas que cuidam da aplica\u00e7\u00e3o.  </p> <p> A grande vantagem de utilizar esta ferramenta \u00e9, portanto, a sua capacidade de superar uma abordagem manual que \u00e9 propensa a erros humanos, demorada e dif\u00edcil de reproduzir no momento de expans\u00e3o para outros servidores. </p>"},{"location":"roteiro1/main/#parte-5-balancamento-de-carga-usando-proxy-reverso","title":"Parte 5: Balancamento de carga usando Proxy Reverso","text":"<p> Agora que fizemos a instala\u00e7\u00e3o do Django, tanto manualmente como utilizando o Ansible, podemos criar um servidor para agir como um ponto \u00fanico de entrada, verificando a disponibilidade de cada server e garantindo uma melhor experi\u00eancia de uso para o usu\u00e1rio. O nome deste mecanismo \u00e9 Loadbalancing e ele \u00e9 especialmente \u00fatil para realizar esta distribui\u00e7\u00e3o de tr\u00e1fego de entrada por v\u00e1rios servidores privados, garantindo toler\u00e2ncia a falhas e maior estabilidade. </p> <p> Para isso, vamos utilizar o NGINX, que usa o algortimo Round Robin para balanceamento de carga para um conjunto IPs dispon\u00edveis cadastrados. Ele \u00e9 relativamente simples de implementar e n\u00e3o considera fatores como tempo de resposta do servidor e a regi\u00e3o geogr\u00e1fica de acesso, mas ser\u00e1 suficiente para nossa aplica\u00e7\u00e3o. </p> <p> Iniciando, vamos fazer o deploy de um novo servidor (server4), que ir\u00e1 realizar fazer este trabalho de Loadbalancing para os servidores em que instalamos nossas aplica\u00e7\u00f5es Django, em nosso Dashboard do MaaS. </p> <p>Tarefa 5.1) Print da tela do Dashboard do MAAS com as 4 M\u00e1quinas e seus respectivos IPs.</p> <p></p> <p>Dashboard do MaaS com os quatro servidores com deploy feito e seus respectivos IPs</p> <p> Ap\u00f3s terminar o deploy, vamos acessar o servidor 4 via SSH e, em seu terminal, vamos realizar a instala\u00e7\u00e3o do nginx. </p> <pre><code>$ ssh ubuntu@{IP Server4}\n$ sudo apt-get install nginx\n</code></pre> <p> Para definir quais servidores o Loadbalancer Round Robin utilizar\u00e1 para o balanceamento de cargas, precisamos editar o arquivo de configura\u00e7\u00e3o de sites dispon\u00edveis do Nginx: </p> <pre><code>$ sudo nano /etc/nginx/sites-available/default\n</code></pre> <p> Primeiro, vamos adicionar, antes do bloco server, um bloco upstream para que o nginx saiba quais servidores ser\u00e3o aqueles que iremos utilizar como backend para tratar as requisi\u00e7\u00f5es recebidas. No nosso caso, deixaremos assim: </p> <pre><code>upstream backend {\n    server2 {IP server2}:8080;\n    server3 {IP server3}:8080;\n}\n</code></pre> <p> Depois, vamos substituir o bloco server presente no arquivo por padr\u00e3o e vamos configura-lo para ficar escutando a porta 80 e para utilizar um proxy_pass para direcionar as requisi\u00e7\u00f5es aos server backend. </p> <pre><code>server {\n    listen 80;\n\n    location / {\n        proxy_pass http://backend;\n    }\n}\n</code></pre> <p> Para registrar as altera\u00e7\u00f5es, precisamos dar um restart no nginx com o seguinte comando: </p> <pre><code>$ sudo service nginx restart\n</code></pre> <p> Pronto! O nginx j\u00e1 est\u00e1 configurado e pronto uso. </p> <p>  Agora, s\u00f3 nos falta verificar a conex\u00e3o com os servidores 2 e 3 a partir do servidor 4. Para isso vamos iniciar modificando o views.py de cada servidor onde instalamos o Django. Vamos trocar o print presente, para podermos identificar se estamos conectados no servidor 2 ou no servidor 3. Veja um exemplo para o servidor 2: </p> <pre><code>$ sudo nano tasks/tasks/views.py\n</code></pre> <pre><code># Exemplo de resolu\u00e7\u00e3o\nfrom django.shortcuts import render\n\nfrom django.http import HttpResponse\n\ndef index(request):\n\n  return HttpResponse(\"Hello from server 2!\")\n</code></pre> <p>Tarefa 5.2) Conte\u00fado da mensagem contida na fun\u00e7\u00e3o <code>index</code> do arquivo <code>tasks/views.py</code> de cada server para distinguir ambos os servers.</p> <p></p> <p>Conex\u00e3o com o server2 a partir do t\u00fanel</p> <p></p> <p>Conex\u00e3o com o server3 a partir do t\u00fanel</p> <p>  Ap\u00f3s fazer a mesma coisa para o servidor 3, vamos sair dos servidores, incluindo a main, usando o comando: </p> <pre><code>$ exit\n</code></pre> <p>  Vamos acessar novamente a main, por\u00e9m utilizando uma pipeline para o server4: </p> <pre><code>$ ssh cloud@{IP server1} -L 8081:{IP server4}:80\n</code></pre> <p>  Este comando, basicamente, cria uma conex\u00e3o SSH com redirecionamento de porta, se conectando com o server1 (ssh cloud@{IP server1}), com redirecionamento de porta local (-L) e fazendo um t\u00fanel que: </p> <li>  Toda conex\u00e3o feita na porta 8081 da m\u00e1quina local \u00e9 encaminhada para o server1. </li> <li>  A partir de server1, o tr\u00e1fego \u00e9 redirecionado para o server4 na porta 80. </li> <p> OBS: Todas as portas mencionadas aqui, foram setadas e configuradas previamente no roteiro. </p> <p>  Com a nginx configurado e o t\u00fanel criado, podemos acessar ambos os servidores em nosso navegador com o link: </p> <li> localhost:8081/tasks/ </li> <p>  Pronto! Agora podemos verificar que ao acessar este link, uma hora somos recebidos com a mensagem do server 2, outra hora com a mensagem do server 3. </p> <p>Tarefa 5.3) Prints das respostas de cada request, provando que voce est\u00e1 conectado ao server 4, que \u00e9 o Proxy Reverso e que ele bate cada vez em um server diferente server2 e server3.</p> <p></p> <p>Conex\u00e3o com o server2 a partir do t\u00fanel</p> <p></p> <p>Conex\u00e3o com o server3 a partir do t\u00fanel</p>"},{"location":"roteiro2/main/","title":"Roteiro 2","text":""},{"location":"roteiro2/main/#introducao-e-objetivo","title":"Introdu\u00e7\u00e3o e Objetivo","text":"<p> O roteiro 2 contempla uma abordagem nova em rela\u00e7\u00e3o ao roteiro 1: a utiliza\u00e7\u00e3o de uma nova plataforma de gerenciamento de aplica\u00e7\u00f5es distribu\u00eddas. Assim, em vez de realizar as instala\u00e7\u00f5es de toda a infraestrutura manualmente (conforme foi feito no roteiro anterior), ser\u00e1 utilizado o Juju, um outro orquestrador de deploy que integra com o MaaS. </p> <p> Por conta desta nova abordagem, todas as m\u00e1quinas contendo as modifica\u00e7\u00f5es do roteiro 1 foram liberadas, retornando para o status \"Ready\" no dashboard do MaaS. </p> <p> Ao final deste roteiro, o objetivo principal \u00e9 termos, portanto, uma Cloud com um novo gerenciador de deploy instalado. </p>"},{"location":"roteiro2/main/#montagem-do-roteiro","title":"Montagem do Roteiro","text":"<p> Todo roteiro apresenta uma primeira parte denonimnada Infra e uma segunda chamada de App. Os pontos tarefas dentro de cada parte s\u00e3o os passos seguidos para a realiza\u00e7\u00e3o do roteiro.  Este modelo de organiza\u00e7\u00e3o orientado por partes e tarefas ser\u00e1 utilizado em todos os roteiros. </p>"},{"location":"roteiro2/main/#infra","title":"Infra","text":""},{"location":"roteiro2/main/#parte-1-instalacao-do-juju","title":"Parte 1: Instala\u00e7\u00e3o do Juju","text":"<p> A instala\u00e7\u00e3o do Juju foi realizada na m\u00e1quina main, acessada via SSH. O seguinte comando foi utilizado: </p> <pre><code>$ sudo snap install juju --channel 3.6\n</code></pre>"},{"location":"roteiro2/main/#parte-2-arquivos-de-definicao-de-cloud","title":"Parte 2: Arquivos de defini\u00e7\u00e3o de cloud","text":"<p> Como o Juju utilizar\u00e1 o MaaS como provedor de m\u00e1quinas e sistema operacional, inicialmente foi necess\u00e1rio garantir que o Juju reconhecesse o MaaS como um provedor de recursos v\u00e1lido. </p> <p>Ap\u00f3s essa verifica\u00e7\u00e3o, criou-se um arquivo de configura\u00e7\u00e3o chamado maas-cloud.yaml com o seguinte conte\u00fado:</p> <pre><code>  clouds:\n    maas-one:\n      type: maas\n      auth-types: [oauth1]\n      endpoint: http://192.168.0.3:5240/MAAS/\n</code></pre> <p>Em seguida, foi adicionada a cloud, utilizando o seguinte comando:</p> <pre><code>$ juju add-cloud --client -f maas-cloud.yaml maas-one\n</code></pre> <p>Por fim, foi necess\u00e1rio adicionar as credenciais MaaS para que o Juju pudesse interagir com a nova cloud adicionada. Um novo arquivo, denominado como maas-creds.yaml, foi criado com esta finalidade:</p> <pre><code>credentials:\n  maas-one:\n  anyuser:\n    auth-type: oauth1\n    maas-oauth: &lt;API KEY gerado no menu do usu\u00e1rio do MaaS&gt;\n</code></pre> <p>Essas credenciais foram aplicadas com o comando:</p> <pre><code>$ juju add-credential --client -f maas-creds.yaml maas-one\n</code></pre>"},{"location":"roteiro2/main/#parte-3-criacao-do-controlador","title":"Parte 3: Cria\u00e7\u00e3o do controlador","text":"<p>Para finalizar a infraestrutura necess\u00e1ria, foi criado um controlador no server 1 da nossa rede privada.</p> <p> Para que o Juju saiba em qual servidor o controlador ir\u00e1 ficar, criou-se a tag 'juju' no server 1 atrav\u00e9s dashboard do Maas e, em seguida, foi executado o comando a seguir: </p> <pre><code>$ juju bootstrap --bootstrap-series=jammy --constraints tags=juju maas-one maas-controller\n</code></pre> <p> Esse comando utiliza a s\u00e9rie jammy e define o nome do controlador como maas-controller, vinculado \u00e0 cloud maas-one. </p>"},{"location":"roteiro2/main/#app","title":"App","text":"<p>Para uma melhor visualiza\u00e7\u00e3o do passo a passo feito a seguir, recomendamos manter um terminal aberto ao lado rodando o seguinte comando:</p> <pre><code>$ watch -n 1 -c juju status --color\n</code></pre> <p>Nele, ser\u00e1 poss\u00edvel visualizar cada mudan\u00e7a feita na infraestrutura em tempo real.</p>"},{"location":"roteiro2/main/#parte-1-instalacao-do-juju-dashboard-para-o-controller","title":"Parte 1: Instala\u00e7\u00e3o do Juju dashboard para o controller","text":"<p> Para a instala\u00e7\u00e3o do juju dashboard, deve-se, primeiro, ter certeza de que estamos no controlador e, sem seguida, executar o comando necess\u00e1rio: </p> <pre><code>$ juju switch maas-controller:admin/maas\n\n$ juju deploy juju-dashboard dashboard\n</code></pre>"},{"location":"roteiro2/main/#parte-2-deploy-da-aplicacao-grafana-e-prometheus","title":"Parte 2: Deploy da aplica\u00e7\u00e3o Grafana e Prometheus","text":"<p>Com o Juju dashboard instalado, prosseguimos para a aplica\u00e7\u00e3o e a configura\u00e7\u00e3o do banco de dados.</p> <p> Neste contexto, ser\u00e1 utilizado o Prometheus como banco de dados e o Grafana como plataforma de apresenta\u00e7\u00e3o visual dos n\u00fameros (tipicamente gr\u00e1ficos e pain\u00e9is). </p> <p> O primeiro passo tomado foi a cria\u00e7\u00e3o um novo modelo, que chamaremos de <code>openstack</code>, onde iremos baixar e realizar as configura\u00e7\u00f5es das aplica\u00e7\u00f5es. </p> <pre><code>$ juju add-model --config default-series=jammy openstack\n\n$ juju switch openstack \n</code></pre> <p> Ap\u00f3s isso, foi criado uma pasta chamada charms para baixar o charm do Grafana e do Prometheus do reposit\u00f3rio charm-hub: </p> <pre><code>$ mkdir -p /home/cloud/charms\n\n$ cd /home/cloud/charms\n</code></pre> <p>Em seguida, foi realizado o download das duas ferramentas:</p> <pre><code>$ juju download grafana\n\n$ juju download prometheus2\n</code></pre> <p>E, por fim, o deploy dos charms de cada uma:</p> <pre><code>$ juju deploy ./grafana_r69.charm --base=ubuntu@20.04\n\n$ juju deploy ./prometheus2_r60.charm\n</code></pre> <p>Nota:  Nos \u00faltimos comandos, utilizamos o deploy do grafana em uma vers\u00e3o espec\u00edfica, para <code>ubuntu@20.04</code>, por conta de problemas de compatibilidade com vers\u00f5es mais novas.</p>"},{"location":"roteiro2/main/#parte-3-integracao-do-grafana-com-o-prometheus","title":"Parte 3: Integra\u00e7\u00e3o do Grafana com o Prometheus","text":"<p>Para que o Grafana mostre os dados contidos no Prometheus, foi utilizado o seguinte comando de integra\u00e7\u00e3o:</p> <pre><code>$ juju integrate grafana:grafana-source prometheus2:grafana-source\n</code></pre> <p></p> <p>Dashboard do MAAS com as m\u00e1quinas e seus respectivos IPs</p> <p></p> <p>Tela do comando \"juju status\" com o Grafana ativo</p> <p>Se desconectando da m\u00e1quina MAIN, demos um <code>ssh</code> para entrar na MAIN novamente, por\u00e9m fazendo um t\u00fanel da porta do Grafana (<code>3000</code> por padr\u00e3o) para uma porta da nossa localhost (<code>8001</code>):</p> <pre><code>$ ssh cloud@10.103.1.X -L 8001:{IP Server Grafana}:3000\n</code></pre> <p>Acessando o Dashboard em nosso navegador (localhost:8001/login), foi utilizado o seguinte procedimento para fazer o login:</p> <ul> <li>Login:        <ul> <li>             Por padr\u00e3o: <code>admin</code> </li> </ul> </li> <li>Senha:        <ul> <li>             Para obter a senha do Grafana, temos que pedir ao juju para rodar o comando <code>get-admin-password</code> na unidade onde est\u00e1 o servi\u00e7o:           </li> </ul> </li> </ul> <pre><code>$ juju run grafana/1 get-admin-password\n</code></pre> <p>Agora, dentro do Dashboard do <code>Grafana</code>, o \u00faltimo passo foi conferir se a integra\u00e7\u00e3o foi feita corretamente. Para isso, foi criado um dashboard dentro do Grafana e foi selecionado o <code>Prometheus</code> como source.</p> <p></p> <p>Tela do Dashboard do Grafana com o Prometheus aparecendo como source</p> <p></p> <p>Acesso ao Dashboard do Grafana a partir da rede do Insper</p> <p></p> <p>Aplica\u00e7\u00f5es sendo gerenciadas pelo Juju </p> <p> Para seguir adiante para o pr\u00f3ximo roteiro, o controlador foi deletado usando o comando juju destroy-controller main e o ambiente foi, novamente, reconfigurado para a estaca zero. </p>"},{"location":"roteiro3/main/","title":"Roteiro 3","text":""},{"location":"roteiro3/main/#introducao-e-objetivo","title":"Introdu\u00e7\u00e3o e Objetivo","text":"<p> O roteiro 3 contempla uma abordagem nova, mantendo a utliza\u00e7\u00e3o do Juju para gerenciamento de aplica\u00e7\u00f5es distribu\u00eddas mas introduzindo agora o openstack. </p> <p> Ao final deste roteiro, o objetivo principal \u00e9 termos, portanto, uma Cloud com um novo gerenciador de deploy instalado, utilizando o OpenStack como plataforma de nuvem privada.  </p> <p>  O roteiro \u00e9 dividido em tr\u00eas etapas: cria\u00e7\u00e3o da infraestrutura (deploy do OpenStack), configura\u00e7\u00e3o dos servi\u00e7os e redes da nuvem, e, por fim, a utiliza\u00e7\u00e3o da infraestrutura com o deployment de aplica\u00e7\u00f5es dentro de m\u00e1quinas virtuais gerenciadas pela nuvem OpenStack. </p>"},{"location":"roteiro3/main/#montagem-do-roteiro","title":"Montagem do Roteiro","text":"<p> Todo roteiro apresenta uma primeira parte denominada Infra e uma segunda chamada de App. No caso deste roteiro especificamente, tamb\u00e9m h\u00e1 uma se\u00e7\u00e3o intermedi\u00e1ria de configura\u00e7\u00e3o pr\u00e9via para o App, denominada de Setup. Os pontos tarefas dentro de cada parte s\u00e3o os passos seguidos para a realiza\u00e7\u00e3o do roteiro.  Este modelo de organiza\u00e7\u00e3o orientado por partes e tarefas ser\u00e1 utilizado em todos os roteiros. </p>"},{"location":"roteiro3/main/#infra","title":"Infra","text":""},{"location":"roteiro3/main/#parte-0-detalhamento-dos-servicos-implantados-na-cloud","title":"Parte 0: Detalhamento dos servi\u00e7os implantados na cloud","text":"<p> Foi implantada uma infraestrutura de nuvem privada utilizando OpenStack, orquestrada com MAAS e Juju, e com armazenamento distribu\u00eddo via Ceph. Antes de tudo, foi necess\u00e1rio garantir que todas as bridges da nossa nuvem estavam corretamente configuradas com a denomina\u00e7\u00e3o \"br-ex\", j\u00e1 que esta \u00e9 a interface de rede que conectar\u00e1 o OpenStack \u00e0 rede externa. </p> <p>  Tamb\u00e9m, foi utilizada a interface do MaaS para garantir as especifica\u00e7\u00f5es necess\u00e1rias para as nossas m\u00e1quinas f\u00edsicas que est\u00e3o listadas a seguir: </p> Servidor Tags CPUs NICs RAM Disks Storage server1 controller, juju 2 1 12 1 80 server2 reserva 2 1 16 2 80 server3 compute 2 1 32 2 80 server4 compute 2 1 32 2 80 server5 compute 2 1 32 2 80 <p>  Por fim, foi definido o modelo do deploy e seguido o tutorial oficial do OpenStack para instalar as seguintes depend\u00eancias:  </p> <pre><code>juju add-model --config default-series=jammy openstack\n\njuju switch maas-controller:openstack\n</code></pre> <ul> <li>Ceph OSD</li> <li>Nova Compute</li> <li>MySQL InnoDB Cluster</li> <li>Vault</li> <li>Neutron Networking</li> <li>Keystone </li> <li>RabbitMQ</li> <li>Nova Cloud Controller</li> <li>Placement</li> <li>Horizon - OpenStack Dashboard</li> <li>Glance </li> <li>Ceph Monitor</li> <li>Cinder</li> <li>Ceph RADOS Gateway</li> </ul> <p>Com as devidas integra\u00e7\u00f5es realizadas, foi executado um \u00faltimo comando de finaliza\u00e7\u00e3o da Infra:</p> <pre><code>juju config ceph-osd osd-devices='/dev/sdb'\n</code></pre>"},{"location":"roteiro3/main/#setup","title":"Setup","text":"<p>Note</p> <p>Setup baseado na documenta\u00e7\u00e3o oficial do openstack: https://docs.openstack.org/project-deploy-guide/charm-deployment-guide/latest/configure-openstack.html</p> <p>No setup, foi feita a configura\u00e7\u00e3o dos servi\u00e7os que controlam:</p> <ul> <li>as VMs (Nova);</li> <li>os volumes de disco (Cinder);</li> <li>a estrutura de rede virtual (Neutron).</li> </ul>"},{"location":"roteiro3/main/#parte-1-autenticacao","title":"Parte 1: Autentica\u00e7\u00e3o","text":"<p> Antes de configurar Nova, Cinder e Neutron, \u00e9 preciso realizar a autentica\u00e7\u00e3o no Keystone. </p> <p> Para isso, foi criado um arquivo <code>openrc.sh</code> contendo as vari\u00e1veis de ambiente necess\u00e1rias para que a CLI do OpenStack possa se autenticar automaticamente. </p> <p> Sempre que for necess\u00e1rio fazer alguma modifica\u00e7\u00e3o no OpenStack via terminal, \u00e9 preciso carregar nossas credenciais, presentes no arquivo criado, e obter as permiss\u00f5es necess\u00e1rias. Para isso, foi utilizado o comando: </p> <pre><code>source ~/openrc\n</code></pre> <p> Para confirmar que as vari\u00e1veis de ambiente foram carregadas, rodamos o seguinte comando: </p> <pre><code>env | grep OS_\n</code></pre> <p> Isso deve retornar todas as vari\u00e1veis <code>OS_</code>, como no exemplo a seguir:  </p> <pre><code>OS_AUTH_URL=https://{KEYSTONE_IP}:5000/v3\nOS_USERNAME=admin\nOS_PASSWORD=MY_PASSWORD\nOS_USER_DOMAIN_NAME=admin_domain\nOS_PROJECT_NAME=admin\nOS_PROJECT_DOMAIN_NAME=admin_domain\nOS_AUTH_VERSION=3\nOS_IDENTITY_API_VERSION=3\nOS_REGION_NAME=RegionOne\nOS_AUTH_PROTOCOL=https\nOS_CACERT=/home/ubuntu/.../root-ca.crt\nOS_AUTH_TYPE=password\n</code></pre>"},{"location":"roteiro3/main/#parte-2-horizon","title":"Parte 2: Horizon","text":"<p> Para acessar o Horizon (Dashboard), foi criado um t\u00fanel via <code>ssh</code> at\u00e9 onde o servi\u00e7o se encontrava: </p> <p></p> <p>Endere\u00e7o IP e porta do Horizon, no Juju Status</p> <pre><code>ssh cloud@{IP_MAIN} -L 8001:{IP_DASHBOARD}:{PORTA_DASHBOARD}\n</code></pre> <p> Ap\u00f3s isso, no navegador (http://localhost:8001/horizon), foi poss\u00edvel entrar no Horizon Dashboard utilizando as credenciais: </p> <ul> <li>Login: <code>admin</code></li> <li>Senha: <code>OS_PASSWORD</code> vis\u00edvel no <code>openrc.sh</code></li> <li>Domain: <code>admin_domain</code></li> </ul> <p> Acessando o Horizon, \u00e9 poss\u00edvel encontrar algo parecido com isso:  </p> <p></p> <p>Aba compute overview no OpenStack Dashboard.</p> <p></p> <p>Aba compute instances no OpenStack Dashboard.</p> <p></p> <p>Aba network topology no OpenStack Dashboard.</p> <p> Al\u00e9m disso, no Juju status e no Dashboard do MAAS as aloca\u00e7\u00f5es das m\u00e1quinas se encontravam da seguinte forma: </p> <p></p> <p>Juju Status</p> <p></p> <p>Dashboard do MAAS com as m\u00e1quinas alocadas.</p>"},{"location":"roteiro3/main/#parte-3-imagens-e-flavors","title":"Parte 3: Imagens e Flavors","text":"<p> A partir desse ponto, foi necess\u00e1rio instalar o client do Openstack no main via snap. </p> <p> O comando utilizado foi o seguinte: </p> <pre><code>sudo snap install openstackclients\n</code></pre> <p> Carregamos as credenciais, como feito anteriormente via <code>source</code>, e verificamos os servi\u00e7os dispon\u00edveis no Openstack, para garantir que o ambiente estava setado corretamente. </p> <pre><code>openstack service list\n</code></pre> <p></p> <p>Servi\u00e7os dispon\u00edveis no Openstack</p> <p> Al\u00e9m disso, foi necess\u00e1rio realizar alguns pequenos ajustes na rede antes de prosseguir: </p> <pre><code>juju config neutron-api enable-ml2-dns=\"true\"\njuju config neutron-api-plugin-ovn dns-servers=\"172.16.0.1\"\n</code></pre> <p> Baixamos a imagem de boot do Ubuntu Jammy amd64 com o comando: </p> <pre><code># Cria diretorio para salvar a imagem\nmkdir ~/cloud-images\n\n# Baixa imagem do Jammy \nwget http://cloud-images.ubuntu.com/jammy/current/jammy-server-cloudimg-amd64.img \\\n   -O ~/cloud-images/jammy-amd64.img\n</code></pre> <p> Ent\u00e3o importamos a imagem para o servi\u00e7o Glance do Openstack e o chamamos de <code>jammy-amd64</code>, via: </p> <pre><code>openstack image create --public --container-format bare \\\n   --disk-format qcow2 --file ~/cloud-images/jammy-amd64.img \\\n   jammy-amd64\n</code></pre> <p> Para finalizar, criamos os flavors: </p> <pre><code>openstack flavor create --vcpus 1 --ram 1024 --disk 20  m1.tiny    # 1 GB\nopenstack flavor create --vcpus 1 --ram 2048 --disk 20  m1.small   # 2 GB\nopenstack flavor create --vcpus 2 --ram 4096 --disk 20  m1.medium  # 4 GB\nopenstack flavor create --vcpus 4 --ram 8192 --disk 20  m1.large   # 8 GB\n</code></pre>"},{"location":"roteiro3/main/#parte-4-rede-externa","title":"Parte 4: Rede Externa","text":"<p> Neste passo, criamos uma Rede externa (p\u00fablica), <code>ext_net</code>. </p> <pre><code>openstack network create --external --share \\\n   --provider-network-type flat --provider-physical-network physnet1 \\\n   ext_net\n</code></pre> <p> Para configurar a rede externa, estabelecemos uma faixa de aloca\u00e7\u00e3o entre <code>172.16.7.0</code> e <code>172.16.8.255</code>. </p> <p> Isso foi feito a partir da cria\u00e7\u00e3o de uma Subnet, <code>ext_subnet</code>, utilizando o comando: </p> <pre><code>openstack subnet create --network ext_net --no-dhcp \\\n   --gateway 172.16.0.1 --subnet-range 172.16.7.0/20 \\\n   --allocation-pool start=172.16.7.0,end=172.16.8.255 \\\n   ext_subnet\n</code></pre>"},{"location":"roteiro3/main/#parte-5-rede-interna-e-roteador","title":"Parte 5: Rede Interna e Roteador","text":"<p> Ap\u00f3s criar a Rede externa, criamos a Rede Interna (privada) e Roteador. </p> <p> Para a Rede Interna, foi criada uma Rede, <code>user1_net</code>, e uma Subnet, <code>user1_subnet</code>, com range <code>192.169.0.0/24</code>. </p> <pre><code># Rede Interna\nopenstack network create --internal user1_net\n</code></pre> <pre><code># Subnet\nopenstack subnet create --network user1_net \\\n   --subnet-range 192.169.0.0/24 \\\n   --allocation-pool start=192.169.0.2,end=192.169.0.254 \\\n   user1_subnet\n</code></pre> <p> Agora, para criar o Roteador, foi utilizado o comando: </p> <pre><code>openstack router create user1_router\n</code></pre> <p> Para finalizar, foi adicionado a subnet privada e configurado para que use a Rede Externa como gateway: </p> <pre><code>openstack router add subnet user1_router user1_subnet\nopenstack router set user1_router --external-gateway ext_net\n</code></pre>"},{"location":"roteiro3/main/#parte-6-conexao","title":"Parte 6: Conex\u00e3o","text":"<p> Antes de criar as inst\u00e2ncias, \u00e9 necess\u00e1rio ter uma forma de acess\u00e1-las. </p> <p> O key-pair \u00e9 uma chave p\u00fablica que permite o acesso SSH seguro \u00e0s inst\u00e2ncias criadas no OpenStack. Ao importar a chave p\u00fablica, voc\u00ea pode se conectar \u00e0s inst\u00e2ncias sem precisar usar senhas, aumentando a seguran\u00e7a e facilitando o gerenciamento de acesso. </p> <p> Por conta disso, foi criado um diret\u00f3rio onde seriam armazenadas essas chaves e, depois disso, importamos um Keypair usando public key (<code>id_rsa.pub</code>) da m\u00e1quina onde est\u00e1 o MaaS. </p> <p>Note</p> <p>A cria\u00e7\u00e3o dessa public key foi realizada no Roteiro 1.</p> <pre><code># Cria o diret\u00f3rio\nmkdir ~/cloud-keys\n\n# Pega a chave na main e cria o Keypair\nopenstack keypair create --public-key ~/.ssh/id_rsa.pub user1\n</code></pre> <p> Para realizar a libera\u00e7\u00e3o do <code>SSH</code> e <code>ALL ICMP</code>, entramos no Horizon (dashboard) e modificamos o security group default. </p>"},{"location":"roteiro3/main/#parte-7-instancia","title":"Parte 7: Inst\u00e2ncia","text":"<p> Para finalizar o setup, foi disparado uma inst\u00e2ncia <code>m1.tiny</code> (flavor), utilizando o nome client e sem Novo Volume. </p> <pre><code>openstack server create --image jammy-amd64 --flavor m1.tiny \\\n   --key-name user1 --network user1_net --security-group default \\\n   client\n</code></pre> <p> Ap\u00f3s isso, foi solicitado e alocado um floating IP para a inst\u00e2ncia. </p> <pre><code># Solicita um FLOATING_IP\nFLOATING_IP=$(openstack floating ip create -f value -c floating_ip_address ext_net)\n</code></pre> <pre><code># Aloca o FLOATING_IP a inst\u00e2ncia\nopenstack server add floating ip client $FLOATING_IP\n</code></pre> <p> Para verificar se a inst\u00e2ncia foi criada e configurada corretamente, a listagem de inst\u00e2ncias dentro do projeto e testamos a conex\u00e3o via <code>SSH</code>. </p> <pre><code># Mostra a listagem de todas as inst\u00e2ncias criadas\nopenstack server list\n</code></pre> <p> O comando anterior deveria devolver algo assim: </p> <pre><code>+--------------------------------------+--------+--------+--------------------------------------+-------------+---------+\n| ID                                   | Name   | Status | Networks                             | Image       | Flavor  |\n+--------------------------------------+--------+--------+--------------------------------------+-------------+---------+\n| a631ff73-b997-4972-9d03-fc376a882120 | client | ACTIVE | user1_net=172.16.8.193, 192.169.0.73 | jammy-amd64 | m1.tiny |\n+--------------------------------------+--------+--------+--------------------------------------+-------------+---------+\n</code></pre> <p> J\u00e1 para o teste de conex\u00e3o via <code>SSH</code>: </p> <p></p> <p>Teste de conex\u00e3o via SSH</p>"},{"location":"roteiro3/main/#checkpoint","title":"Checkpoint","text":"<p> Ap\u00f3s a cria\u00e7\u00e3o dos flavors, das Redes (Interna e Externa) e da Inst\u00e2ncia, al\u00e9m da cria\u00e7\u00e3o do Keypair, \u00e9 poss\u00edvel ver uma not\u00e1vel diferen\u00e7a nas abas do Horizon. </p> <p></p> <p>Dashboard MAAS, ap\u00f3s setup</p> <p> Na Aba compute overview do Horizon, \u00e9 poss\u00edvel notar a mudan\u00e7a no gr\u00e1fico de Instances, VCPUs e RAM, isso se deve a cria\u00e7\u00e3o da nova inst\u00e2ncia <code>client</code> que criamos no projeto e, no caso da RAM e VCPUs, as especif\u00edca\u00e7\u00f5es que demos ao flavor (<code>m1.tiny</code>) utilizado na inst\u00e2ncia. </p> <p> Al\u00e9m disso, na parte inferior do Compute &gt; Overview, na se\u00e7\u00e3o de Network, agora mostra a exist\u00eancia de um floating IP (referente a <code>Parte 7: Inst\u00e2ncia</code>),  de duas novas Networks (<code>Parte 4: Rede Externa</code> e <code>Parte 5: Rede Interna</code>), um novo Router (<code>Parte 5: Roteador</code>) e tamb\u00e9m de um Security Group, o grupo <code>default</code> criado por padr\u00e3o pelo OpenStack, al\u00e9m das regras atreladas a esse grupo, algumas criadas automaticamente e outras configuradas manualmente, <code>SSH</code> e <code>ALL ICMP</code> (<code>Parte 6: Conex\u00e3o</code>). </p> <p> Para os 4 novos Ports: <ul> <li> <p>             Port de <code>client</code> na rede <code>user1_net</code>, ao subir a inst\u00e2ncia.         </p> </li> <li> <p>             Port do roteador <code>user1_router</code> \u2192 <code>user1_subnet</code> (final de <code>Parte 5: Roteador</code>).         </p> </li> <li> <p>             Port do roteador <code>user1_router</code> \u2192 Gateway externo (<code>ext_net</code>) (final de <code>Parte 5: Roteador</code>).         </p> </li> <li> <p>             Port do Floating IP (<code>Parte 7: Inst\u00e2ncia</code>).         </p> </li> </ul> </p> <p></p> <p>Aba compute overview do OpenStack, ap\u00f3s setup</p> <p> Na aba Compute &gt; Instances, assim como na Overview, podemos perceber que agora foi registrado a cria\u00e7\u00e3o da inst\u00e2ncia <code>client</code>. </p> <p></p> <p>Aba compute instances do OpenStack, ap\u00f3s setup</p> <p> Na aba Network Topology, \u00e9 poss\u00edvel verificar de forma visual todas as liga\u00e7\u00f5es feitas entre os objetos em cada passo. </p> <p> As colunas coloridas s\u00e3o referentes as Redes: <code>user1_net</code> (laranja, Rede Interna) e <code>ext_net</code> (azul, Rede Externa). </p> <p> O <code>user1_router</code>, seria o roteador (<code>Parte 5: Roteador</code>). O IP a sua direita, conectando o roteador a Rede Interna, seria seu IP Interno <code>192.169.0.1</code>, j\u00e1 o IP a sua esquerda, conectando o roteador a Rede Externa, seria seu IP Externo <code>172.16.7.118</code>. </p> <p> A direita do diagrama, \u00e9 poss\u00edvel ver a inst\u00e2ncia <code>client</code>, criada na <code>Parte 7: Inst\u00e2ncia</code>, juntamente a seu IP atribuido na subnet de <code>user1_net</code>, com valor <code>192.169.0.73</code>. </p> <p></p> <p>Aba network topology do OpenStack, ap\u00f3s setup</p>"},{"location":"roteiro3/main/#escalando-os-nos","title":"Escalando os n\u00f3s","text":"<p> Ap\u00f3s verificar se ainda h\u00e1 disponibilidade de alguma m\u00e1quina no Dashboard do MaaS, alguma m\u00e1quina que ainda esteja somente allocated, fizemos release desta m\u00e1quina para poder seguir ao passo final: instalar o Hipervisor. </p> <p> Ap\u00f3s finalizar o realease da m\u00e1quina, utilizamos o comando a seguir para instalar o <code>hypervisor</code> e realizar o deploy na m\u00e1quina. </p> <pre><code>juju add-unit nova-compute\n</code></pre> <p> Verificando o juju status, anotamos o n\u00famero da m\u00e1quina adicionada e fizemos a instala\u00e7\u00e3o do <code>block storage</code>. </p> <pre><code>juju add-unit --to &lt;ID_MAQUINA&gt; ceph-osd\n</code></pre>"},{"location":"roteiro3/main/#desenho-da-arquitetura-de-rede","title":"Desenho da arquitetura de rede","text":"<p>Tarefa 3) Desenho da arquitetura de rede, desde a sua conex\u00e3o com o Insper at\u00e9 a inst\u00e2ncia alocada.</p> <p></p> <p>Arquitetura de rede</p>"},{"location":"roteiro3/main/#app","title":"App","text":""},{"location":"roteiro3/main/#preparacao-da-arquitetura","title":"Prepara\u00e7\u00e3o da arquitetura","text":"<p> A principal tarefa a ser completada no App foi a utiliza\u00e7\u00e3o da infraestrutura configurada at\u00e9 o momento para colocar o projeto da disciplina na cloud criada.  </p> <p>  Desenvolvido em paralelo, o projeto da disciplina \u2014 dispon\u00edvel neste reposit\u00f3rio \u2014 consistiu em uma aplica\u00e7\u00e3o FastAPI composta por tr\u00eas endpoints simples. Para simular uma nuvem mais pr\u00f3xima da realidade, foi adotada a seguinte topologia:  </p> <ul> <li>2 inst\u00e2ncias com a API desenvolvida;</li> <li>1 inst\u00e2ncia com banco de dados Postgres;</li> <li>1 inst\u00e2ncia com LoadBalancer, Nginx.</li> </ul> <p>  Em ambientes de produ\u00e7\u00e3o, utilizar duas inst\u00e2ncias da API \u00e9 considerado uma boa pr\u00e1tica, pois permite garantir balanceamento de carga, toler\u00e2ncia a falhas e alta disponibilidade. Dessa forma, em vez de a requisi\u00e7\u00e3o do cliente chegar diretamente a uma inst\u00e2ncia da API, ela \u00e9 encaminhada primeiro ao Load Balancer, que distribui as requisi\u00e7\u00f5es entre as inst\u00e2ncias dispon\u00edveis, as quais est\u00e3o conectadas ao banco de dados da aplica\u00e7\u00e3o. A seguir, apresenta-se um diagrama que ilustra essa arquitetura:  </p> <p></p> <p>Topologia de uso da infraestrutura</p> <p>  Para iniciar a implementa\u00e7\u00e3o, o dashboard do Horizon foi acessado (via NAT) e, nele, foram criadas as quatro inst\u00e2ncias com os seguintes nomes: load-balancer, api-1, api-2 e database. </p> <p></p> <p>Local de visualiza\u00e7\u00e3o e cria\u00e7\u00e3o de inst\u00e2ncias no dashboard do Horizon</p> <p>  Ap\u00f3s criadas as inst\u00e2ncias, foi necess\u00e1rio atribuir a cada uma um IP Flutuante para que fosse poss\u00edvel atingir todas por meio da NUC Main. Afinal, conforme mostrado no diagrama de topologia da rede anteriormente, as inst\u00e2ncias se encontram em uma subrede e possuem IPs que n\u00e3o s\u00e3o \u00fateis para conversar com m\u00e1quinas que se encontram na rede externa. </p> <p> Para isso, foram atribu\u00eddos IPs flutuantes para cada inst\u00e2ncia (uma por vez), reutilizando os comandos vistos no Setup:  </p> <pre><code>FLOATING_IP=$(openstack floating ip create -f value -c floating_ip_address ext_net)\n\nopenstack server add floating ip nome-da-instancia $FLOATING_IP\n</code></pre> <p> Al\u00e9m disso, vale ressaltar que, a fim de baratear o custo de manuten\u00e7\u00e3o da arquitetura ao m\u00e1ximo sem prejudicar o desempenho do cliente, optou-se pelo flavor m1.tiny para todas as inst\u00e2ncias.  </p> <p> Contudo, para aplica\u00e7\u00f5es com maior demanda e um volume de dados maior para armazenamento, seria prov\u00e1vel a necessidade de escolher flavors maiores do que uma aplica\u00e7\u00e3o em ambiente controlado de aprendizado. Afinal, em uma cloud comercial, o custo \u00e9 proporcional ao tamanho do flavor e seu tempo de uso. </p>"},{"location":"roteiro3/main/#configuracao-da-instancia-loadbalancer","title":"Configura\u00e7\u00e3o da inst\u00e2ncia LoadBalancer","text":"<p> Primeiramente, foi instalado o Nginx por meio do seguinte comandos: </p> <pre><code>sudo apt-get install nginx\n</code></pre> <p> Em seguida, foi necess\u00e1rio editar o arquivo que permitiria o nginx a enxergar as duas inst\u00e2ncias para as quais o Load Balancer apontaria, conforme o diagrama ilustrado no in\u00edcio do relat\u00f3rio: </p> <pre><code># Acessando arquivo de configura\u00e7\u00e3o do nginx\nsudo nano /etc/nginx/sites-available/default\n</code></pre> <p> Dentro do arquivo, as altera\u00e7\u00f5es feitas foram as seguintes: </p> <pre><code>upstream backend {\n        server [IP de subrede da inst\u00e2ncia da API 1]:8080;\n        server [IP de subrede da inst\u00e2ncia da API 2]:8080;\n}\n\nserver {\n        listen 80 default_server;\n        listen [::]:80 default_server;\n\n        location / { proxy_pass http://backend; \n}\n</code></pre> <p> Por fim, o servi\u00e7o do nginx foi devidamente reinicializado para salvar as altera\u00e7\u00f5es feitas no arquivo: </p> <pre><code>sudo service nginx restart\n</code></pre>"},{"location":"roteiro3/main/#instalacao-do-docker","title":"Instala\u00e7\u00e3o do Docker","text":"<p> Tanto para as inst\u00e2ncias de API quanto para o banco de dados, foi necess\u00e1rio realizar a instala\u00e7\u00e3o do docker. Afinal, puxaremos a mesma imagem para as duas inst\u00e2ncias de API do projeto pelo docker hub. Para isso, foi seguido o tutorial oficial no site do docker. Os comandos executados, portanto, foram: </p> <pre><code># Add Docker's official GPG key:\nsudo apt-get update\nsudo apt-get install ca-certificates curl\nsudo install -m 0755 -d /etc/apt/keyrings\nsudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc\nsudo chmod a+r /etc/apt/keyrings/docker.asc\n\n# Add the repository to Apt sources:\necho \\\n  \"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu \\\n  $(. /etc/os-release &amp;&amp; echo \"${UBUNTU_CODENAME:-$VERSION_CODENAME}\") stable\" | \\\n  sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null\nsudo apt-get update\n\nsudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin \n</code></pre>"},{"location":"roteiro3/main/#configuracao-das-instancias-de-api","title":"Configura\u00e7\u00e3o das inst\u00e2ncias de API","text":"<p> Primeiramente, foram configuradas as vari\u00e1veis de ambiente necess\u00e1rias no arquivo .env. Aqui, al\u00e9m da URL para a intera\u00e7\u00e3o com o banco de dados, tamb\u00e9m foram informadas as chaves necess\u00e1rias para fazer as requisi\u00e7\u00f5es para a API de cota\u00e7\u00e3o do d\u00f3lar e do euro (um dos endpoints da aplica\u00e7\u00e3o): </p> <pre><code>AWESOME_API_KEY=\"API_KEY_AQUI\"\nSECRET_KEY=\"SECRET_KEY_AQUI\"\nALGORITHM=HS256\n\nPOSTGRES_USER=cloud\nPOSTGRES_PASSWORD=senha\nPOSTGRES_DB=cloud\nDATABASE_URL=postgresql://cloud:senha@[IP de subrede do Banco de Dados]:5432/cloud\n</code></pre> <p> Em seguida, a imagem do projeto publicada no docker hub foi puxada e o container foi executada nas duas inst\u00e2ncias. </p> <pre><code>sudo docker pull antoniolma/app\nsudo docker run -p 8080:80 --env-file .env -d antoniolma/app\n\n# Verificando se o container est\u00e1 sendo executado na m\u00e1quina\nsudo docker ps -a\n</code></pre>"},{"location":"roteiro3/main/#configuracao-da-instancia-do-banco-de-dados","title":"Configura\u00e7\u00e3o da inst\u00e2ncia do Banco de Dados","text":"<p> Primeiramente, foi instalado o PostgreSQL por meio dos seguintes comandos: </p> <pre><code>sudo apt update\nsudo apt install postgresql postgresql-contrib -y\n</code></pre> <p> Em seguida, assim como feito nas APIs, foram configuradas as vari\u00e1veis de ambiente necess\u00e1rias no arquivo .env: </p> <pre><code>POSTGRES_USER=cloud\nPOSTGRES_PASSWORD=senha\nPOSTGRES_DB=cloud\n</code></pre> <p> Por fim, foi executado o docker na porta padr\u00e3o do postgres para que as inst\u00e2ncias de API possam enxergar o banco de dados: </p> <pre><code>sudo docker run -p 5432:5432 --env-file .env -d postgres\n</code></pre>"},{"location":"roteiro3/main/#verificacao-final","title":"Verifica\u00e7\u00e3o final","text":"<p> Para conferir o devido funcionamento da infraestrutura, foi criado um t\u00fanel SSH que conectasse o computador local a uma das inst\u00e2ncias de API criada. Para isso, foi executado o mesmo comando de t\u00fanel utilizado no roteiro 1, com as devidas adapta\u00e7\u00f5es necess\u00e1rias: </p> <pre><code>$ ssh cloud@10.103.1.10 -L 8080:[IP flutuante load-balancer]:80 \n</code></pre> <p>Tarefa 4.2) Arquitetura de rede da infraestrutura dentro do Dashboard do OpenStack</p> <p></p> <p>Arquitetura de rede final</p> <p>Tarefa 4.3) Lista de VMs utilizadas com nome e IPs alocados,</p> <p></p> <p>Visualiza\u00e7\u00e3o dos nomes e IPs alocados via dashboard do Horizon</p> <p>Tarefa 4.4) Dashboard do FastAPI conectado via m\u00e1quina Nginx/LB.</p> <p></p> <p>Dashboard do FastAPI acessado ap\u00f3s a aplica\u00e7\u00e3o do t\u00fanel SSH via m\u00e1quina Nginx</p> <p>Tarefa 4.5) Servidores (m\u00e1quina fisica) alocados para cada inst\u00e2ncia pelo OpenStack.</p> <p></p> <p>M\u00e1quina f\u00edsica alocada para o Load Balancer pelo OpenStack: Server 5</p> <p></p> <p>M\u00e1quina f\u00edsica alocada para a API 1 pelo OpenStack: Server 4</p> <p></p> <p>M\u00e1quina f\u00edsica alocada para a API 2 pelo OpenStack: Server 2</p> <p></p> <p>M\u00e1quina f\u00edsica alocada para o Banco de dados Postgres pelo OpenStack: Server 3</p>"},{"location":"roteiro4/main/","title":"Roteiro 4","text":""},{"location":"roteiro4/main/#introducao-e-objetivo","title":"Introdu\u00e7\u00e3o e Objetivo","text":"<p> O roteiro 4 contempla uma abordagem nova, mantendo a utliza\u00e7\u00e3o do Juju para gerenciamento de aplica\u00e7\u00f5es distribu\u00eddas e o openstack implementado no roteiro anterio, mas explorando agora o conceito de Infraestrutura como c\u00f3digo atrav\u00e9s do Terraform. </p>"},{"location":"roteiro4/main/#montagem-do-roteiro","title":"Montagem do Roteiro","text":"<p> Todo roteiro apresenta uma primeira parte denominada Infra e uma segunda chamada de App. Os pontos tarefas dentro de cada parte s\u00e3o os passos seguidos para a realiza\u00e7\u00e3o do roteiro.  Este modelo de organiza\u00e7\u00e3o orientado por partes e tarefas ser\u00e1 utilizado em todos os roteiros. </p>"},{"location":"roteiro4/main/#infra","title":"Infra","text":"<p> At\u00e9 o momento, foram utilizados o dashboard e a interface de linha de comando (CLI) para criar rede, subrede, inst\u00e2ncias, roteadorres e outros recursos. Para criar a infraestrutura necess\u00e1ria agora, conforme ser\u00e1 visto mais a frente no App, foi utilizado somente c\u00f3digo. </p> <p> Primeiramente, para n\u00e3o confundir os recursos de cada usu\u00e1rio, foi criada uma separa\u00e7\u00e3o l\u00f3gica de dois usu\u00e1rios inseridos em um mesmo dom\u00ednio (assim como deveria acontecer em nuvem): aluno1 e aluno2.  </p>"},{"location":"roteiro4/main/#parte-1-criando-um-unico-domain","title":"Parte 1: Criando um \u00fanico Domain","text":"<p> Via Horizon Dashboard, foi criado o dom\u00ednio AlunosDomain: </p> <p></p> <p>Cria\u00e7\u00e3o de dom\u00ednio novo do Horizon Dashboard</p> <p> Em seguida, o novo dom\u00ednio criado foi definido como o novo contexto de uso, conforme mostra a imagem a seguir: </p> <p></p> <p>Novo dom\u00ednio como contexto de uso</p>"},{"location":"roteiro4/main/#parte-2-criando-um-projeto-para-cada-aluno","title":"Parte 2: Criando um projeto para cada Aluno","text":"<p> Para separar o aluno1 do aluno2, foram feitos dois projetos que respeitassem o padr\u00e3o Kit + letra do kit + nome_do_aluno. </p> <p></p> <p>Interface de cria\u00e7\u00e3o dos projetos para cada usu\u00e1rio aluno</p> <p> Por fim, os dois usu\u00e1rios foram criados tamb\u00e9m utilizando a interface do Horizon Dashboard, dando aten\u00e7\u00e3o especial para garantir que ambos tivessem pap\u00e9is administrativos tanto no momento de cria\u00e7\u00e3o, quanto na configura\u00e7\u00e3o do dom\u00ednio. O dom\u00ednio AlunosDomain e os respectivos projetos criados foram informados para cada um. </p> <p></p> <p>Interface de cria\u00e7\u00e3o dos usu\u00e1rios</p> <p></p> <p>Concedendo papel administrativo para ambos os usu\u00e1rios no dom\u00ednio criado</p>"},{"location":"roteiro4/main/#app","title":"App","text":""},{"location":"roteiro4/main/#parte-1-criando-arquivos-do-terraform","title":"Parte 1: Criando arquivos do Terraform","text":"<p> Utilizando a estrutura de pastas mostrada a seguir, cada aluno realizou a cria\u00e7\u00e3o de sua pr\u00f3pria infraestrutura em uma pasta pr\u00f3pria (seguindo o padr\u00e3o Kit + Letra do Kit + Primeiro Nome) dentro da m\u00e1quina MAIN. </p> <p></p> <p>Estrutura de pastas seguida por cada aluno na configura\u00e7\u00e3o do terraform</p> <p>Os arquivos utilizados est\u00e3o detalhados abaixo:</p> <p>provider.tf <pre><code># Define required providers\nterraform {\nrequired_version = \"&gt;= 0.14.0\"\n  required_providers {\n    openstack = {\n      source  = \"terraform-provider-openstack/openstack\"\n      version = \"~&gt; 1.35.0\"\n    }\n  }\n}\n\n\n# Configure the OpenStack Provider\n\nprovider \"openstack\" {\n  region              = \"RegionOne\"\n  user_name           = \"SEU_USUARIO\" # Aqui, alterou-se para aluno1/aluno2\n}\n</code></pre></p> <p>instance1.tf</p> <pre><code>resource \"openstack_compute_instance_v2\" \"instancia_1\" {\n  name            = \"basic\"\n  image_name      = \"jammy-amd64\" # Para saber a imagem adequada, acessamos Project &gt; Compute &gt; Images no Dashboard do Openstack\n  flavor_name     = \"m1.small\"\n  key_pair        = \"mykey\"\n  security_groups = [\"default\"]\n\n  network {\n    name = \"network_1\" # Aqui, o nome da rede mudou de um aluno para outro\n  }\n\n  depends_on = [openstack_networking_network_v2.network_1]\n\n}\n</code></pre> <p>instance2.tf</p> <pre><code>resource \"openstack_compute_instance_v2\" \"instancia_2\" {\n  name            = \"basic2\"\n  image_name      = \"jammy-amd64\" # Para saber a imagem adequada, acessamos Project &gt; Compute &gt; Images no Dashboard do Openstack\n  flavor_name     = \"m1.tiny\"\n  key_pair        = \"mykey\"\n  security_groups = [\"default\"]\n\n  network {\n    name = \"network_1\" # Aqui, o nome da rede mudou de um aluno para outro\n  }\n\n  depends_on = [openstack_networking_network_v2.network_1]\n\n}\n</code></pre> <p>network.tf</p> <pre><code>resource \"openstack_networking_network_v2\" \"network_1\" {\n  name           = \"network_1\" # Aqui, o nome da rede mudou de um aluno para outro\n  admin_state_up = \"true\"\n}\n\nresource \"openstack_networking_subnet_v2\" \"subnet_1\" {\n  network_id = \"${openstack_networking_network_v2.network_1.id}\"\n  cidr       = \"192.167.199.0/24\"\n}\n</code></pre> <p>router.tf</p> <pre><code>resource \"openstack_networking_router_v2\" \"router_1\" {\n  name                = \"my_router\"\n  admin_state_up      = true\n  external_network_id = &lt;\"ID_EXT_NETWORK\"&gt; # Aqui, foi utilizado o ID da rede externa, dispon\u00edvel no Dashboard do Openstack ou via CLI openstack\n}\n\nresource \"openstack_networking_router_interface_v2\" \"int_1\" {\n  router_id = \"${openstack_networking_router_v2.router_1.id}\"\n  subnet_id = \"${openstack_networking_subnet_v2.subnet_1.id}\"\n}\n</code></pre> <p> Importante:  Al\u00e9m da cria\u00e7\u00e3o desses arquivos, cada aluno precisou criar um par de chaves pr\u00f3prio (denominado de \"mykey\" nos arquivos relativos \u00e0s inst\u00e2ncias), acessando o Dashboard do OpenStack em Project &gt; Compute &gt; Key Pairs &gt; Create Key Pair. </p>"},{"location":"roteiro4/main/#parte-2-credenciais-dos-respectivos-usuarios","title":"Parte 2: Credenciais dos respectivos usu\u00e1rios","text":"<p>  Antes de aplicar as mudan\u00e7as definidas nos arquivos Terraform, era necess\u00e1rio realizar um \u00faltimo passo: adicionar o arquivo de credenciais de cada usu\u00e1rio. Para isso, foi feito o download do arquivo OpenStack RC diretamente pelo Dashboard, no caminho Project \u2192 API Access, selecionando a op\u00e7\u00e3o \"Download OpenStack RC File\" para cada usu\u00e1rio (aluno1 e aluno2).  </p> <p>  Ap\u00f3s o download, o conte\u00fado de cada arquivo foi copiado e salvo em um novo arquivo dentro do mesmo diret\u00f3rio dos demais, utilizando o padr\u00e3o <code>openrc.sh</code>. Em seguida, foi atribu\u00eddo permiss\u00e3o de execu\u00e7\u00e3o com o comando <code>chmod +x arquivo.sh</code>. Por fim, as vari\u00e1veis de ambiente foram carregadas executando <code>source arquivo.sh</code>, garantindo que as credenciais daquele usu\u00e1rio estejam ativas na sess\u00e3o.  </p>"},{"location":"roteiro4/main/#parte-3-implementacao-e-verificacao-da-infraestrutura","title":"Parte 3: Implementa\u00e7\u00e3o e verifica\u00e7\u00e3o da infraestrutura","text":"<p>Para fazer a implementa\u00e7\u00e3o da infraestrutura, foram executados, por fim, os comandos abaixo:</p> <pre><code>terraform plan # Cria o plano de execu\u00e7\u00e3o antes do Terraform aplicar as configura\u00e7\u00f5es feitas\n\nterraform apply # Aplica as mudan\u00e7as necess\u00e1rias a partir do plano de execu\u00e7\u00e3o\n</code></pre> <p>  Com o intuito de verificar que tudo ocorreu normalmente ap\u00f3s os comandos acima, ainda foi executado o comando <code>openstack server list</code>, que devolveu as duas inst\u00e2ncias criadas (para cada usu\u00e1rio): </p> <pre><code>+--------------------------------------+-----------+--------+--------------------------+--------------------------+---------+\n| ID                                   | Name      | Status | Networks                 | Image                    | Flavor  |\n+--------------------------------------+-----------+--------+--------------------------+--------------------------+---------+\n| a99cbd57-eb44-4c0f-a441-6feaf73a16a6 | basic2    | ACTIVE | network_1=192.167.199.11 | jammy-amd64              | m1.tiny |\n| cd7a3480-5bd8-4d94-9746-deac0d3324a2 | basic     | ACTIVE | network_1=192.167.199.74 | jammy-amd64              | m1.small|\n+--------------------------------------+-----------+--------+---------------------+--------------------------+--------------+\n</code></pre>"},{"location":"roteiro4/main/#parte-4-checkpoint-por-usuario-apos-a-criacao-das-instancias","title":"Parte 4: Checkpoint por usu\u00e1rio ap\u00f3s a cria\u00e7\u00e3o das inst\u00e2ncias","text":"<p>Tarefa 1) Prints do Dashboard do OpenStack para o usu\u00e1rio aluno1</p> <p></p> <p>Aba Identity (Identidade) &gt; Projects (Projetos) do Dashboard do OpenStack para o aluno1 </p> <p></p> <p>Aba Identity (Identidade) &gt; Users (Usu\u00e1rios) do Dashboard do OpenStack para o aluno1</p> <p></p> <p>Aba Compute (Computa\u00e7\u00e3o) &gt; Overview (Vis\u00e3o geral) do Dashboard do OpenStack para o aluno1</p> <p></p> <p>Aba Compute (Computa\u00e7\u00e3o) &gt; Instances (Inst\u00e2ncias) do Dashboard do OpenStack para o aluno1</p> <p></p> <p>Aba de Topologia de Rede (Network Topology) do Dashboard do OpenStack para o aluno1</p> <p>Tarefa 2) Prints do Dashboard do OpenStack para o usu\u00e1rio aluno2</p> <p></p> <p>Aba Identity (Identidade) &gt; Projects (Projetos) do Dashboard do OpenStack para o aluno2 </p> <p></p> <p>Aba Identity (Identidade) &gt; Users (Usu\u00e1rios) do Dashboard do OpenStack para o aluno2</p> <p></p> <p>Aba Compute (Computa\u00e7\u00e3o) &gt; Overview (Vis\u00e3o geral) do Dashboard do OpenStack para o aluno2</p> <p></p> <p>Aba Compute (Computa\u00e7\u00e3o) &gt; Instances (Inst\u00e2ncias) do Dashboard do OpenStack para o aluno2</p> <p></p> <p>Aba de Topologia de Rede (Network Topology) do Dashboard do OpenStack para o aluno2</p>"},{"location":"roteiro4/main/#parte-5-criando-um-plano-de-disaster-recovery-e-sla-questoes","title":"Parte 5: Criando um plano de Disaster Recovery e SLA (QUEST\u00d5ES)","text":"<p>Exercise</p> <p><p> Voc\u00ea \u00e9 o CTO (Chief Technology Officer) de uma grande empresa com sede em v\u00e1rias capitais no Brasil e precisa implantar um sistema cr\u00edtico, de baixo custo e com dados sigilosos para a \u00e1rea operacional. </p></p> <p>a) Voc\u00ea escolheria Public Cloud ou Private Cloud?</p> <p>b) Agora explique para ao RH por que voc\u00ea precisa de um time de DevOps.</p> <p><p>  c) Considerando o mesmo sistema cr\u00edtico, agora sua equipe dever\u00e1 planejar e implementar um ambiente resiliente e capaz de mitigar poss\u00edveis interrup\u00e7\u00f5es/indisponibilidades. Para isso, identifiquem quais s\u00e3o as principais amea\u00e7as que podem colocar sua infraestrutura em risco, e descreva as principais a\u00e7\u00f5es que possibilitem o restabelecimento de todas as aplica\u00e7\u00f5es de forma r\u00e1pida e organizada caso algum evento cause uma interrup\u00e7\u00e3o ou incidente de seguran\u00e7a. Para isso monte um plano de DR e HA que considere entre as a\u00e7\u00f5es: </p></p> <ul> <li>Mapeamento das principais amea\u00e7as que podem colocar em riscos o seu ambiente.</li> <li>Elenque e priorize as a\u00e7\u00f5es para a recupera\u00e7\u00e3o de seu ambiente em uma poss\u00edvel interrup\u00e7\u00e3o/desastre.</li> <li>Como sua equipe ir\u00e1 tratar a pol\u00edtica de backup?</li> <li>Considerando poss\u00edveis instabilidades e problemas, descreva como alta disponibilidade ser\u00e1 implementada em sua infraestrutura.</li> </ul> <p> a) Para um sistema cr\u00edtico, de baixo custo e que lida com dados sigilosos na \u00e1rea operacional, a escolha mais adequada \u00e9: Private Cloud. Justamente por conta de se tratar de dados sens\u00edveis e haver a necessidade de estabelecer medidas de seguran\u00e7a espec\u00edficas ao contexto da empresa, a op\u00e7\u00e3o mais segura seria optar pela Cloud privada. </p> <p> b) E-mail para a equipe de RH </p> <p>E-mail para a equipe de RH</p> <p>Assunto: Solicita\u00e7\u00e3o de equipe DevOps</p> <p>Ol\u00e1, equipe de RH, tudo bem?</p> <p>No momento, n\u00f3s da equipe de implementa\u00e7\u00e3o do principal projeto da \u00e1rea de  tecnologia e infraestrutura da empresa estivemos criando o plano de a\u00e7\u00e3o para  como realizar podemos realizar este projeto da melhor forma poss\u00edvel, por\u00e9m utilizando somente os recursos estritamente necess\u00e1rios para a implementa\u00e7\u00e3o e chegamos a conclus\u00e3o de que, para entregar um sistema cr\u00edtico, seguro e de qualidade, precisamos formar um time de DevOps.</p> <p>Dado a escala do projeto, um time de DevOps seria essencial para podermos melhorar a qualidade da entrega, eliminando gargalos e criando um fluxo cont\u00ednuo de desenvolvimento, reduzindo drasticamente o tempo de entrega de novas funcionalidades e corre\u00e7\u00f5es cr\u00edticas. Isso traria um ambiente de trabalho mais eficiente e resultaria em uma maior seguran\u00e7a do sistema.</p> <p>Voc\u00eas poderiam abrir novas vagas ou realocar profissionais para esse time o quanto antes?</p> <p>Desde j\u00e1 agrade\u00e7o! Atenciosamente, CTO \u2013 Empresa TechnoCloud</p> <p> c) Para garantir que nosso sistema cr\u00edtico sobreviva a falhas e se recupere rapidamente, vamos estruturar um plano de recupera\u00e7\u00e3o de desastres (DR) e alta disponibilidade (HA). Vamos dividir este plano em quatro principais etapas:s </p> 1. Mapeamento das Principais Amea\u00e7as <ul> <li>Falhas de Hardware: Pane em servidores (CPU, mem\u00f3ria, disco) ou componentes de rede.</li> <li>Interrup\u00e7\u00f5es de Energia/Climat\u00e9ricas: Quedas de energia, picos de tens\u00e3o, enchentes, tempestades ou inc\u00eandios no datacenter.</li> <li>Falhas de Software: Bugs em atualiza\u00e7\u00f5es do SO ou m\u00f3dulos cr\u00edticos; corrup\u00e7\u00e3o de banco de dados.</li> <li>Cyberataques: Ransomware, DDoS, explora\u00e7\u00e3o de vulnerabilidades, acesso n\u00e3o autorizado.</li> <li>Erro Humano: Configura\u00e7\u00e3o incorreta de equipamentos, deploy incorreto de c\u00f3digo, remo\u00e7\u00e3o acidental de dados.</li> <li>Problemas de Rede/Telco: Lat\u00eancia ou queda de links de Internet/MPLS entre capitais.</li> </ul> 2. Plano de A\u00e7\u00f5es para Recupera\u00e7\u00e3o (Prioridades) <ul> <li>Prioridade 1 \u2013 Infraestrutura B\u00e1sica (RTO curto): <ul> <li>Fontes redundantes (UPS+gerador) e links de Internet m\u00faltiplos.</li> <li>Datacenter prim\u00e1rio + r\u00e9plica ativa/stand-by em outra capital.</li> <li>Sincroniza\u00e7\u00e3o cont\u00ednua de VMs/containers (Storage Replication).</li> </ul> </li> <li>Prioridade 2 \u2013 Camada de Aplica\u00e7\u00e3o e Banco (RTO m\u00e9dio): <ul> <li>Cluster de banco prim\u00e1rio com r\u00e9plica (s\u00edncrona/semiss\u00edncrona) em DC secund\u00e1rio.</li> <li>Load Balancer em modo ativo-ativo ou ativo-passivo; servidores de aplica\u00e7\u00e3o em m\u00faltiplas zonas.</li> </ul> </li> <li>Prioridade 3 \u2013 Integra\u00e7\u00f5es e Servi\u00e7os Externos (RTO estendido): <ul> <li>Cont\u00eaineres orquestrados replicados (Kubernetes/OpenShift) em ambas localidades.</li> <li>Circuit Breakers para degradar funcionalidades n\u00e3o cr\u00edticas em alta lat\u00eancia.</li> <li>Fila de mensageria (Kafka/RabbitMQ) replicada; pol\u00edticas de retry e DLQ.</li> </ul> </li> <li>Prioridade 4 \u2013 Servi\u00e7os de Suporte (RTO menos cr\u00edtico): <ul> <li>Elastic Stack (Elasticsearch+Kibana) espelhado entre DCs; Prometheus+Grafana replicados.</li> <li>Active Directory/LDAP redundante; failover autom\u00e1tico de DNS.</li> </ul> </li> </ul> 3. Pol\u00edtica de Backup <ul> <li>Escopo e Frequ\u00eancia: <ul> <li>Backup Di\u00e1rio (RPO \u2264 24h): Dump full nos bancos relacionais + incremental a cada 4h; arquivos cr\u00edticos (certificados, scripts) em reposit\u00f3rio versionado + snapshot di\u00e1rio.</li> <li>Backup Semanal (RPO \u2265 1 semana): Arquivos de auditoria e relat\u00f3rios em backup full semanal; reten\u00e7\u00e3o de 8 semanas.</li> <li>Backup Mensal (RPO \u2265 1 m\u00eas): Snapshots de VMs (templates/golden images) em fita virtual ou storage de arquivamento.</li> </ul> </li> <li>Local de Armazenamento: <ul> <li>On-Site: NAS com RAID + VLAN exclusiva para tr\u00e1fego de backup.</li> <li>Off-Site: C\u00f3pias di\u00e1rias criptografadas (AES-256) por VPN para DR site em outra capital; opcionalmente, terceiro datacenter geograficamente distante (~200 km).</li> </ul> </li> <li>Armazenamento e Reten\u00e7\u00e3o: <ul> <li>Full mensal com reten\u00e7\u00e3o de 12 meses; backups semanais retidos por 8 semanas; di\u00e1rios retidos por 14 dias.</li> <li>RPO m\u00e1ximo de 4 horas para dados cr\u00edticos; 24 h para configura\u00e7\u00f5es.</li> </ul> </li> <li>Testes de Restaura\u00e7\u00e3o: <ul> <li>Restaura\u00e7\u00e3o completa em homologa\u00e7\u00e3o a cada 3 meses.</li> <li>Simula\u00e7\u00e3o de perda total do datacenter prim\u00e1rio ao menos uma vez ao ano.</li> </ul> </li> </ul> 4. Implementa\u00e7\u00e3o de Alta Disponibilidade (HA) <ul> <li>Camada de Rede e Balanceamento: <ul> <li>Load Balancers redundantes (ativo-passivo ou ativo-ativo) em DCs diferentes com health checks a cada 30 s e failover de VIP.</li> <li>Multipath Routing via BGP multipath entre ISPs distintos.</li> </ul> </li> <li>Camada de Aplica\u00e7\u00e3o: <ul> <li>Cluster de cont\u00eaineres/VMs autoescal\u00e1veis em duas ou mais capitais, orquestrados por K8s/OpenShift/VMware Tanzu.</li> <li>Autoscaling baseado em m\u00e9tricas (CPU, mem\u00f3ria, tempo de resposta) para escalar horizontalmente.</li> </ul> </li> <li>Camada de Banco de Dados e Armazenamento: <ul> <li>Cluster prim\u00e1rio com r\u00e9plica s\u00edncrona em DC secund\u00e1rio; promo\u00e7\u00e3o autom\u00e1tica de r\u00e9plica se o prim\u00e1rio falhar.</li> <li>Storage compartilhado (NAS/SAN) replicado (GlusterFS, Ceph ou solu\u00e7\u00f5es dedicadas) com acesso em m\u00faltiplos hosts.</li> </ul> </li> <li>Camada de Servi\u00e7os de Suporte: <ul> <li>DNS din\u00e2mico apontando para m\u00faltiplos IPs de balanceadores com TTL baixo (30-60 s) para failover r\u00e1pido.</li> <li>Identity Provider redundante (LDAP/AD) sincronizando contas em tempo real; controlador de dom\u00ednio secund\u00e1rio assume automaticamente.</li> </ul> </li> <li>Monitoramento e Orquestra\u00e7\u00e3o de Failover: <ul> <li>Monitoramento 24\u00d77 (Zabbix/Prometheus) com alertas e dashboards (Grafana).</li> <li>Playbooks de resposta a incidentes (runbooks) para cada tipo de falha; drills trimestrais.</li> <li>Orquestra\u00e7\u00e3o de failover autom\u00e1tico (Pacemaker/Corosync, Keepalived, K8s Operators) para cen\u00e1rios de falha simples.</li> </ul> </li> </ul>"}]}