{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Computa\u00e7\u00e3o em Nuvem","text":"Edi\u00e7\u00e3o <p>2025.1</p>"},{"location":"#kit-a","title":"KIT-A","text":"<p>Antonio Lucas Michelon de Almeida</p> <p>Pedro Nery Affonso dos Santos</p>"},{"location":"#entregas","title":"Entregas","text":"<ul> <li> Roteiro 1 </li> <li> Roteiro 2</li> <li> Roteiro 3</li> <li> Roteiro 4</li> <li> Projeto</li> </ul>"},{"location":"#referencias","title":"Refer\u00eancias","text":"<p>Material for MkDocs</p>"},{"location":"projeto/main/","title":"Projeto 2025.1","text":"<p> O projeto consiste na constru\u00e7\u00e3o e desenvolvimento de uma API RESTful para cadastro e autentica\u00e7\u00e3o de usu\u00e1rios, utilizando ferramentas do framework FastAPI </p> <p> Ap\u00f3s a constru\u00e7\u00e3o da API, o projeto deve ser dockerizado, publicado no Docker Hub e, por fim, implantado no AWS. </p> <p> Para uma melhor organiza\u00e7\u00e3o do projeto, ele foi dividido em tr\u00eas entregas. </p>"},{"location":"projeto/main/#objetivo-da-avaliacao","title":"Objetivo da Avalia\u00e7\u00e3o","text":"<p> Avaliar o dom\u00ednio dos alunos em: </p> <ul> <li>Containeriza\u00e7\u00e3o local com Docker Compose</li> <li>Deploy em ambiente de nuvem com AWS Lightsail</li> <li>Conex\u00e3o segura com banco de dados</li> <li>Estrutura\u00e7\u00e3o de aplica\u00e7\u00e3o web com FastAPI</li> <li>Boas pr\u00e1ticas de c\u00f3digo, documenta\u00e7\u00e3o e custo</li> </ul>"},{"location":"projeto/main/#entrega-1","title":"Entrega 1","text":""},{"location":"projeto/main/#construcao-da-api","title":"Constru\u00e7\u00e3o da API","text":"<p> Este projeto consiste em desenvolver uma API RESTful em FastAPI, com tr\u00eas endpoints b\u00e1sicos: </p> <ul> <li> POST /registrar: Usu\u00e1rio entra com <code>nome</code>, <code>email</code> e <code>senha</code>.         <ul> <li>C\u00f3digo 200: (Sucesso) <code>email</code> n\u00e3o encontrado na base de dados, faz o registro de um novo usu\u00e1rio, com senha codificada em formato <code>HASH</code>, e retorna o JWT Token.</li> <li>C\u00f3digo 409: (Error) <code>email</code> j\u00e1 se enconta cadastrado, cancela opera\u00e7\u00e3o.</li> </ul> </li> <li> POST /login: Usu\u00e1rio entra com <code>email</code> e <code>senha</code>.         <ul> <li>C\u00f3digo 200: (Sucesso) Conta associada ao <code>email</code> \u00e9 encontrada e a <code>senha</code> recebida bate com a senha codificada no banco de dados, faz o login do usu\u00e1rio e retorna o JWT Token..</li> <li>C\u00f3digo 401: (Error) <code>email</code> recebido n\u00e3o \u00e9 encontrado na base de dados</li> <li>C\u00f3digo 401: (Error) <code>senha</code> n\u00e3o confere com a senha codificada na base</li> </ul> </li> <li> GET /consultar:          <ul> <li>C\u00f3digo 200: (Sucesso) Verifica\u00e7\u00e3o bem sucedida do JWT Token e requisi\u00e7\u00e3o feita com sucesso \u00e0 Awesome API, cuja resposta \u00e9 devolvida ao cliente: a cota\u00e7\u00e3o atual do Euro e do Dol\u00e1r em rela\u00e7\u00e3o ao Real</li> <li>C\u00f3digo 403: (Error) JWT Token inv\u00e1lido ou ausente no header.</li> </ul> </li> </ul> <p></p> <p>Documenta\u00e7\u00e3o dos endpoints da API</p> <p> Ap\u00f3s a implementa\u00e7\u00e3o de cada um dos endpoints, tratando cada caso poss\u00edvel mencionado anteriormente, e integra\u00e7\u00e3o com o banco de dados PostgreSQL, \u00e9 necess\u00e1rio testar os endpoints para passar para a pr\u00f3xima etapa. </p> V\u00eddeo mostrando funcionalidades da API (Testes) <p></p> <p>Teste do endpoint \"/registrar\" (Success: Cadastro realizado)</p> <p></p> <p>Teste do endpoint \"/registrar\" (Fail: Email j\u00e1 registrado)</p> <p></p> <p>Teste do endpoint \"/login\" (Success: Login bem-sucedido)</p> <p></p> <p>Teste do endpoint \"/login\" (Fail: Email n\u00e3o registrado)</p> <p></p> <p>Teste do endpoint \"/login\" (Fail: Senha incorreta)</p> <p></p> <p>Teste do endpoint \"/consultar\" (Success: [Retorna cota\u00e7\u00e3o Dolar e Euro])</p> <p></p> <p>Teste do endpoint \"/consultar\" (Fail: JWT ausente ou inv\u00e1lido)</p>"},{"location":"projeto/main/#dockerizing","title":"Dockerizing","text":"<p> Com o c\u00f3digo da API pronto, a pr\u00f3xima etapa \u00e9 dockerizar/containerizar dos 2 servi\u00e7os juntos: a aplica\u00e7\u00e3o e o banco de dados. Dentro do docker, \u00e9 necess\u00e1rio a cria\u00e7\u00e3o de um arquivo <code>Dockerfile</code> (de acordo com a linguagem e ambiente de execu\u00e7\u00e3o escolhidos) e <code>compose.yaml</code> para a execu\u00e7\u00e3o da aplica\u00e7\u00e3o, sendo capaz de se conectar ao banco de dados e realizar as opera\u00e7\u00f5es de CRUD.  </p> <p> N\u00e3o s\u00f3 isso, mas a aplica\u00e7\u00e3o deve ser autocontida, ou seja, deve ser poss\u00edvel executar a aplica\u00e7\u00e3o apenas com o comando <code>docker compose up</code>. </p> <p> Ao final da Dockeriza\u00e7\u00e3o, a sa\u00edda desse comando devolve: </p> <pre><code>&gt; docker compose up -d\n[+] Running 2/2\n \u2714 Container inspercloud-projeto-db-1   Running\n \u2714 Container inspercloud-projeto-app-1  Running  \n\n&gt; docker compose ps\nNAME                        IMAGE                   SERVICE   STATUS         PORTS\ninspercloud-projeto-app-1   antoniolma/app:v1.0.1   app       Up 3 seconds   0.0.0.0:8080-&gt;80/tcp\ninspercloud-projeto-db-1    postgres:17             db        Up 8 seconds   0.0.0.0:5432-&gt;5432/tcp\n</code></pre> <p> A organiza\u00e7\u00e3o do diret\u00f3rio ao final do projeto: </p> <pre><code>\u251c\u2500\u2500 api/\n\u2502   \u251c\u2500\u2500 Dockerfile          # Dockerfile que instala depend\u00eancias e copia api/app/\n\u2502   \u251c\u2500\u2500 requirements.txt    # depend\u00eancias para o python\n\u2502   \u2514\u2500\u2500 app/                # c\u00f3digo com FastAPI\n\u2502       \u251c\u2500\u2500 app.py\n\u2502       \u2514\u2500\u2500 models.py\n\u251c\u2500\u2500 .env                    # vari\u00e1veis de ambiente (POSTGRES_*, DATABASE_URL, SECRET_KEY\u2026)\n\u2514\u2500\u2500 docker-compose.yaml     # orquestra\u00e7\u00e3o dos servi\u00e7os db + app\n</code></pre> <p> Para subir o container no Docker Hub, foram utilizados os seguintes comandos: </p> <ol> <li> Login no Docker Hub:     <pre><code>docker login\n</code></pre> </li> <li> Build da imagem e inicializa\u00e7\u00e3o dos containers:     <pre><code>docker compose up --build\n</code></pre> </li> <li> Push da vers\u00e3o atual para o Docker Hub:     <pre><code>docker push antoniolma/app  \n</code></pre> </li> </ol> <p> Por\u00e9m, para apenas utilizar a aplica\u00e7\u00e3o, ao baixar o projeto no reposit\u00f3rio do Github, basta utilizar o comando: </p> <pre><code>docker compose up\n</code></pre> <p> Caso tenha curiosidade, \u00e9 poss\u00edvel encontrar o projeto nos links: <ul> <li> Link projeto Github </li> <li> Link projeto DockerHub </li> </ul> </p>"},{"location":"roteiro1/main/","title":"Roteiro 1","text":""},{"location":"roteiro1/main/#introducao-e-objetivo","title":"Introdu\u00e7\u00e3o e Objetivo","text":"<p>O roteiro 1 contempla a funda\u00e7\u00e3o de toda a infraestrutura da Cloud que foi montada a partir de um KIT que contava com os seguintes componentes:</p> <ul> <li>1 NUC (main) com 10Gb e 1 SSD (120 Gb)</li> <li>1 NUC (server1) com 12Gb e 1 SSD (120 Gb)</li> <li>1 NUC (server2) com 16Gb e 2 SSD (120 Gb + 120 Gb)</li> <li>3 NUCs (server3, server4 e server5) com 32 Gb e 2 SSD (120 Gb + 120 Gb)</li> <li>1 Switch DLink DSG-1210-28 de 28 portas</li> <li>1 Roteador TP-Link TL-R470T+</li> </ul> <p>Ao longo deste roteiro, passaremos pelos seguintes passos:</p> <ul> <li> <p>Configura\u00e7\u00e3o do KIT via cabo</p> <ul> <li>Instala\u00e7\u00e3o do Ubuntu Server</li> <li>Instala\u00e7\u00e3o do MAAS</li> <li>Configura\u00e7\u00e3o do MAAS</li> <li>Reconfigura\u00e7\u00e3o do DHCP</li> <li>Cadastro dos servidores via MAAS</li> <li>Cria\u00e7\u00e3o das pontes OVS    </li> </ul> </li> <li> <p>Configura\u00e7\u00e3o do KIT via acesso remoto</p> <ul> <li>Configura\u00e7\u00e3o de um servidor de banco de dados Postgres</li> <li>Deploy de uma aplica\u00e7\u00e3o Django</li> <li>Utiliza\u00e7\u00e3o do Ansible</li> <li>...</li> </ul> </li> </ul> <p> Ao final deste roteiro, o objetivo principal \u00e9 termos, portanto, uma Cloud com um primeiro gerenciador de deploy instalado. A partir disso, o cliente j\u00e1 ser\u00e1 capaz de realizar requisi\u00e7\u00f5es ao servidor se estiver conectado \u00e0 rede Wi-Fi do Insper.  </p>"},{"location":"roteiro1/main/#montagem-do-roteiro","title":"Montagem do Roteiro","text":"<p> Todo roteiro apresenta uma primeira parte denonimnada Infra e uma segunda chamada de App. Os pontos tarefas dentro de cada parte s\u00e3o os passos seguidos para a realiza\u00e7\u00e3o do roteiro.  Este modelo de organiza\u00e7\u00e3o orientado por partes e tarefas ser\u00e1 utilizado em todos os roteiros. </p>"},{"location":"roteiro1/main/#infra","title":"Infra","text":""},{"location":"roteiro1/main/#parte-0-enderecos-mac-das-nucs-e-ip-do-roteador","title":"Parte 0: Endere\u00e7os MAC das NUCs e IP do roteador","text":"<p> Antes de iniciar qualquer instala\u00e7\u00e3o, foi essencial capturar imagens de todos os endere\u00e7os MAC dos servidores (1 a 5).  </p> <p> Essa a\u00e7\u00e3o ocorreu simultaneamente \u00e0 Tarefa 1, pois, a partir da Tarefa 2, as fontes de alimenta\u00e7\u00e3o n\u00e3o podem mais ser desconectadas e, devido \u00e0 forma como o kit foi montado, n\u00e3o seria poss\u00edvel remover as NUCs de suas posi\u00e7\u00f5es para visualizar os endere\u00e7os sem deslig\u00e1-las da tomada. </p> <p> Al\u00e9m disso, foi realizado um pr\u00e9-roteiro relativo \u00e0 montagem de um cabo de rede. Com a conex\u00e3o ethernet, foi acessada a interface do roteador do KIT e alterou-se o seu endere\u00e7o de IP para 172.16.0.1. Mais detalhes sobre a escolha deste IP ser\u00e3o fornecidos na Tarefa 1.  </p> <p></p> <p>Tela de configura\u00e7\u00e3o de IP do roteador e de m\u00e1scara de subrede</p>"},{"location":"roteiro1/main/#parte-1-instalacao-do-ubuntu-server","title":"Parte 1: Instala\u00e7\u00e3o do Ubuntu Server","text":"<p>Para a realiza\u00e7\u00e3o desta primeira tarefa, foram seguidos os passos descritos a seguir para configurar a NUC main:</p> <ol> <li> <p>Download da imagem do Ubuntu Server </p> <ul> <li>Download da vers\u00e3o 22.04 LTS do Ubuntu Server a partir do site oficial do Ubuntu.</li> </ul> </li> <li> <p>Cria\u00e7\u00e3o um pendrive boot\u00e1vel </p> <ul> <li>Uso do software Rufus para gravar a imagem no pendrive.</li> </ul> </li> <li> <p>Acesso \u00e0 BIOS </p> <ul> <li>Com o Pendrive conectado, a BIOS foi acessada a partir a tecla <code>F12</code> durante a inicializa\u00e7\u00e3o.  </li> <li>Configura\u00e7\u00e3o de ordem de boot para priorizar o pendrive.  </li> </ul> </li> <li> <p>Configura\u00e7\u00f5es iniciais de instala\u00e7\u00e3o do Ubuntu Server </p> <ul> <li>Reinicializa\u00e7\u00e3o da NUC.  </li> <li>Sele\u00e7\u00e3o da op\u00e7\u00e3o \"Install Ubuntu Server\" no menu inicial.  </li> <li>Sele\u00e7\u00e3o de idioma e layout do teclado.</li> </ul> </li> <li> <p>Configura\u00e7\u00f5es de subrede, IP, Gateway e DNS </p> <ul> <li>M\u00e1scara de Rede: 172.16.0.0/20</li> <li>Endere\u00e7o IP da NUC main</li> <li>Gateway: IP do roteador</li> <li>Name servers: DNS do Insper</li> </ul> </li> </ol> <p></p> <p>Tela de configura\u00e7\u00e3o de M\u00e1scara de Rede, IP, Gateway (roteador) e DNS do Insper  </p> <p>Explica\u00e7\u00e3o te\u00f3rica das configura\u00e7\u00f5es feitas:</p> <p> Para que dispositivos possam estabelecer uma comunica\u00e7\u00e3o entre si, \u00e9 primordial que eles se encontrem na mesma rede. A configura\u00e7\u00e3o da subrede, IP, gateway e servidores DNS permite a correta comunica\u00e7\u00e3o entre os dispositivos e o acesso \u00e0 internet ou a outros servi\u00e7os de rede. </p> <p> A m\u00e1scara de rede define o intervalo de endere\u00e7os IP dispon\u00edveis dentro da subrede. No caso da configura\u00e7\u00e3o com a m\u00e1scara `/20`, a rede pode conter at\u00e9 4.096 endere\u00e7os IP, garantindo escalabilidade para futuras expans\u00f5es. </p> <p> O endere\u00e7o IP da NUC main \u00e9 um IP est\u00e1tico atribu\u00eddo manualmente ao servidor principal (nesse caso, 172.16.0.3, pois o switch assumiu o IP 172.16.0.2), garantindo que ele tenha sempre o mesmo endere\u00e7o na rede local, facilitando a administra\u00e7\u00e3o e a comunica\u00e7\u00e3o com outros dispositivos. </p> <p> O gateway corresponde ao IP do roteador (que foi configurado na Tarefa 0 como 172.16.0.1), que atua como a ponte entre a rede interna e redes externas, como a internet. Sem a configura\u00e7\u00e3o correta do gateway, os dispositivos na rede local n\u00e3o conseguiriam acessar servi\u00e7os externos. </p> <p> Os name servers (DNS) s\u00e3o respons\u00e1veis pela resolu\u00e7\u00e3o de nomes de dom\u00ednio, convertendo endere\u00e7os amig\u00e1veis, como `www.google.com`, em endere\u00e7os IP. Utilizar os servidores DNS do Insper garante uma resolu\u00e7\u00e3o eficiente e confi\u00e1vel dentro do ambiente da institui\u00e7\u00e3o. </p> <p>Na tela seguinte, referente ao archive mirror, foram aceitas as configura\u00e7\u00f5es que vieram por padr\u00e3o, conforme a print a seguir:</p> <p></p> <p>Tela de configura\u00e7\u00e3o padr\u00e3o do Ubuntu archive mirror </p> <p>Passo final: Cria\u00e7\u00e3o de usu\u00e1rio e configura\u00e7\u00f5es finais </p> <pre><code>hostname: main\n\nlogin: cloud\n\nsenha: clouda\n\nName Servers (DNS): DNS do Insper\n</code></pre> <p></p> <p>Tela de configura\u00e7\u00e3o do usu\u00e1rio da NUC main segundo as especifica\u00e7\u00f5es passadas</p> <p> Ap\u00f3s a conclus\u00e3o de todos os passos da instala\u00e7\u00e3o, foi realizado um reboot da NUC main e removido o pendrive.  </p>"},{"location":"roteiro1/main/#parte-2-maas-acesso-local","title":"Parte 2: MAAS - acesso local","text":""},{"location":"roteiro1/main/#instalacao","title":"Instala\u00e7\u00e3o","text":"<p> Para a instala\u00e7\u00e3o do MAAS na NUC main (que agora tem um sistema operacional), optou-se pela vers\u00e3o 3.5.3. No terminal do Ubuntu Server, foram utilizados os comandos a seguir: </p> <pre><code>$ sudo apt update &amp;&amp; sudo apt upgrade -y\n\n$ sudo snap install maas --channel=3.5/stable\n\n$ sudo snap install maas-test-db\n</code></pre> <p>Para verificar o devido funcionamento do MAAS instalado, foram realizados dois testes com o comando ping, ilustrados na foto a seguir:</p> <p></p> <p>Tela de teste de funcionamento por meio de pings</p> <p>Ap\u00f3s o teste feito com sucesso, foi realizado um acesso da NUC main via SSH com o comando a seguir (ssh usuario@IP):</p> <pre><code>$ ssh cloud@172.16.0.3\n</code></pre>"},{"location":"roteiro1/main/#configuracao","title":"Configura\u00e7\u00e3o","text":"<p> Dentro da rede local, o MAAS foi inicializado e criou-se o administrador cloud, que ser\u00e1 necess\u00e1rio para poteriormente ser poss\u00edvel acessar o dashboard. Antes da inicializa\u00e7\u00e3o foi necess\u00e1rio realizar um reboot. </p> <pre><code>$ sudo maas init region+rack --maas-url http://172.16.0.3:5240/MAAS --database-uri maas-test-db:///\n\n$ sudo maas createadmin\n</code></pre> <p>Em seguida, foi gerado um par de chaves para autentica\u00e7\u00e3o. Ap\u00f3s gerada, a chave p\u00fablica foi copiada.</p> <pre><code>$ ssh-keygen -t rsa\n\n$ cat ./.ssh/id_rsa.pub\n</code></pre> <p> Utilizando o IP atribu\u00eddo \u00e0 NUC main e a porta padr\u00e3o do MAAS, foi poss\u00edvel acessar o Dashboard via protocolo HTTP (http://172.16.0.3:5240/MAAS). O login foi realizado atrav\u00e9s do admin criado nos passos anteriores. </p> <p></p> <p>Dashboard do MAAS</p> <p> Conforme ilustrado acima, a tela inicial do MAAS apresenta um dashboard com informa\u00e7\u00f5es sobre o estado atual dos servidores gerenciados. O dashboard \u00e9 composto por diversos pain\u00e9is, cada um exibindo informa\u00e7\u00f5es sobre um aspecto espec\u00edfico do ambiente gerenciado.  </p> <p> Utilizando a interface gr\u00e1fica do Dashboard, primeiramente, foi configurado um DNS forwarder utilizando o DNS do Insper (Networking &gt; DNS). </p> <p> Em seguida, foram importadas imagens do Ubuntu 22.04 LTS e Ubuntu 20.04 LTS em Configuration &gt; Images &gt; Ubuntu Releases e feito o upload da chave copiada no terminal SSH.  </p> <p> Por fim, foi passado o par\u00e2metro kernel net.ifnames=0 em Settings &gt; Configuration &gt; Kernel Parameters. </p>"},{"location":"roteiro1/main/#chaveando-o-dhcp","title":"Chaveando o DHCP","text":"<p> O DHCP (Dynamic Host Configuration Protocol) \u00e9 um protocolo de rede que permite a configura\u00e7\u00e3o autom\u00e1tica de dispositivos em uma rede IP. Ele \u00e9 principalmente respons\u00e1vel por atribuir dinamicamente endere\u00e7os IP, eliminando a necessidade de configura\u00e7\u00e3o manual, mas tamb\u00e9m pode assumir fun\u00e7\u00f5es como, por exemplo, definir a m\u00e1scara de sub-rede e fornecer servidores DNS. </p> <p> At\u00e9 o momento, o dispositivo da nossa sub-rede contendo este protocolo \u00e9 o roteador. Por\u00e9m, nesta etapa isto foi modificado. </p> <p> Primeiramente, dentro do MAAS Controller, o DHCP foi habilitado na NUC Main e, conforme ilustra a imagem a seguir, o Reserved Range foi alterado para iniciar em 172.16.11.1 e acabar em 172.16.14.255. </p> <p></p> <p>Tela de configura\u00e7\u00e3o dos Reserved Ranges dentro do MAAS Controller</p> <p> Al\u00e9m disso, como mais de um dispositivo n\u00e3o pode conter o protocolo DHCP dentro de uma mesma sub-rede (mais de um dispositivo tentando atribuir um IP a outro dispositivo automaticamente), tamb\u00e9m foi necess\u00e1rio desativar o DHCP no roteador. </p> <p></p> <p>Tela de desabilita\u00e7\u00e3o do DHCP no roteador</p> <p>A sa\u00fade do sistema tamb\u00e9m foi verificada a partir da p\u00e1gina de Controladores no Dashboard, ilustrada a seguir.</p> <p></p> <p>Tela de verifica\u00e7\u00e3o da sa\u00fade do sistema</p>"},{"location":"roteiro1/main/#comissionando-servidores","title":"Comissionando servidores","text":"<p> Com o DHCP agora devidamente chaveado, os servers 1 a 5 foram cadastrados como machines no Dashboard do MAAS.  Para tanto, foram resgatados os endere\u00e7os MAC capturados na Tarefa 0, alterada a op\u00e7\u00e3o Power Type para Intel AMT, configurada a senha `CloudComp6s!` para todos os servidores e o IP 172.16.15.X (X sendo o n\u00famero do servidor configurado). </p> <p> Ap\u00f3s a comiss\u00e3o autom\u00e1tica, todos os n\u00f3s apareceram com o status Ready e as especifica\u00e7\u00f5es de armazenamento das NUCs foram confirmadas. Al\u00e9m disso, o roteador foi adicionado como device. </p> <p></p> <p>Adicionando roteador como device</p>"},{"location":"roteiro1/main/#ovs-bridge","title":"OVS Bridge","text":"<p> Antes de possibilitar o acesso remoto ao KIT, um passo final foi criar, para cada servidor, uma ponte Open vSwitch (OVS). Todas as pontes tiveram o nome \"br-ex\" atribu\u00eddo a elas. A seguir, tem-se uma ilustra\u00e7\u00e3o de como a bridge ficou configurada para o server 1, na interface do dashboard do MAAS. </p> <p></p> <p>Interface do Server 1 ap\u00f3s a cria\u00e7\u00e3o da ponte Open vSwitch (OVS)</p>"},{"location":"roteiro1/main/#parte-3-maas-acesso-remoto","title":"Parte 3: MAAS - acesso remoto","text":"<p>Para que seja poss\u00edvel acessar o KIT remotamente, e n\u00e3o mais atrav\u00e9s do cabo na rede local, \u00e9 necess\u00e1rio realizar a cria\u00e7\u00e3o de um gateway NAT. </p> <p> A inten\u00e7\u00e3o por tr\u00e1s do acesso remoto \u00e9 que o computador seja capaz de conversar com o servidor main dentro da subrede configurada at\u00e9 agora apenas por meio de uma conex\u00e3o com a Rede Wi-Fi do Insper.  </p> <p> O NAT \u00e9 justamente o servi\u00e7o que possibilita que dispositivos dentro de uma rede privada acessem redes externas (como a internet) atrav\u00e9s de um \u00fanico endere\u00e7o IP p\u00fablico. Ele traduz os endere\u00e7os IP privados da subrede para o endere\u00e7o IP do roteador ao enviar pacotes para fora da rede e realiza o processo inverso ao receber respostas, garantindo assim a comunica\u00e7\u00e3o adequada entre os dispositivos. </p> <p></p> <p>Ilustra\u00e7\u00e3o te\u00f3rica de funcionamento da subrede</p> <p>Na ilustra\u00e7\u00e3o acima, a linha tracejada verde representa a conex\u00e3o entre um computador (que se encontra conectado \u00e0 LAN do Insper) com a NUC main da subrede privada que foi configurada at\u00e9 o momento. </p> <p> Para que fosse estabelecida tal conex\u00e3o, foram seguidas as instru\u00e7\u00f5es contidas no manual de uso do roteador do KIT para possibilitar o redirecionamento do dispositivo que tentasse se conectar \u00e0 portas 22 (padr\u00e3o SSH) e 5240 (padr\u00e3o MAAS) a partir do IP p\u00fablico 10.103.1.10, que \u00e9 o endere\u00e7o do roteador fora da rede privada. </p> <p>Ao final da configura\u00e7\u00e3o do NAT, portanto, a interface do roteador ficou da seguinte forma:</p> <p></p> <p>Interface do roteador ap\u00f3s a configura\u00e7\u00e3o do NAT</p> <p>A partir deste momento, portanto, passou a ser poss\u00edvel conectar-se ao dashboard do MAAS e aos demais servidores por meio do seguinte comando SSH no terminal do computador:</p> <pre><code>$ ssh cloud@10.103.1.10\n</code></pre>"},{"location":"roteiro1/main/#app","title":"App","text":"<p>Na parte da aplica\u00e7\u00e3o deste primeiro roteiro, foi realizado um deploy manual de uma aplica\u00e7\u00e3o simples em Django nos servidores.</p>"},{"location":"roteiro1/main/#parte-1-criacao-do-banco-de-dados","title":"Parte 1: Cria\u00e7\u00e3o do banco de dados","text":"<p>Acessando o terminal do server 1 via SSH, foi criado um usu\u00e1rio (senha: cloud) por meio dos comandos:</p> <pre><code>$ sudo apt update\n\n$ sudo apt install postgresql postgresql-contrib -y\n\n$ sudo su - postgres\n\n$ createuser -s cloud -W\n</code></pre> <p>Em seguida, foi criado o banco de dados e exposto o servi\u00e7o para acesso:</p> <pre><code>$ createdb -O cloud tasks\n\n$ nano /etc/postgresql/14/main/postgresql.conf\n</code></pre> <p>Saindo do usu\u00e1rio postgres, liberou-se o firewall e o servi\u00e7o foi reiniciado:</p> <pre><code>$ sudo ufw allow 5432/tcp\n\n$ sudo systemctl restart postgresql\n</code></pre> <p>Tarefa 1.1) Dentro do server1, status do banco de dados se mostra ativo.</p> <pre><code>$ sudo systemctl status postgresql\n</code></pre> <p></p> <p>Status do PostgreSQL vendo do server1</p> <p>Tarefa 1.2) Inicia a sess\u00e3o e utiliza do computador/servi\u00e7o remotamente, atrav\u00e9s da porta 5240, na MAIN. Servi\u00e7o acess\u00edvel da MAIN.</p> <pre><code>$ telnet localhost 5240\n</code></pre> <p></p> <p>Conex\u00e3o estabelecida entre a MAIN e o servi\u00e7o remoto</p> <p>Tarefa 1.3) Servi\u00e7o acess\u00edvel na pr\u00f3pria m\u00e1quina onde o postgresql foi instalado.</p> <pre><code>$ sudo su - postgres\n</code></pre> <p></p> <p>PostgreSQL acess\u00edvel de dentro do server em que est\u00e1 alocado</p> <p>Tarefa 1.4) Acessando a configura\u00e7\u00e3o do postgresql, foi poss\u00edvel verificar a porta na sess\u00e3o \u2018CONNECTIONS AND AUTHENTICATION\u2019</p> <pre><code>Comando: $ nano /etc/postgresql/14/main/postgresql.conf\n</code></pre> <p></p> <p>Configura\u00e7\u00f5es do PostgreSQL (default)</p>"},{"location":"roteiro1/main/#parte-2-aplicacao-django","title":"Parte 2: Aplica\u00e7\u00e3o Django","text":"<p> Ap\u00f3s requisitar acesso a uma m\u00e1quina em nosso servidor (Comando 1), e inserir o token da aba \u2018API keys\u2019 presente no Dashboard, solicitamos ao MaaS a aloca\u00e7\u00e3o de uma m\u00e1quina (Comando 2) e realizamos o deploy da nossa aplica\u00e7\u00e3o (Comando 3), sendo \u2018system_id\u2019 o id do server alocado, vis\u00edvel no link do Dashboard ao clicar na m\u00e1quina desejada. </p> <pre><code>$ maas login [login] http://172.16.0.3:5240/MAAS/    # Requisi\u00e7\u00e3o de m\u00e1quina\n$ maas [login] machines allocate name=[server_name]  # Solicita aloca\u00e7\u00e3o a MAIN\n$ maas [login] machine deploy [system_id]            # Deploy da aplica\u00e7\u00e3o\n</code></pre> <p> Acessando o servidor via SSH, clonamos o reposit\u00f3rio onde teremos a aplica\u00e7\u00e3o Django. Entrando no diret\u00f3rio tasks, fazemos a instala\u00e7\u00e3o das depend\u00eancias do reposit\u00f3rio.  </p> <pre><code>$ git clone https://github.com/raulikeda/tasks.git\n\n$ ./install.sh # Instala\u00e7\u00e3o das depend\u00eancias\n</code></pre> <p> Ap\u00f3s um breve reboot da m\u00e1quina, iremos acessar o arquivo \u2018/etc/hosts\u2019 para darmos permiss\u00e3o a nossa MAIN de utilizar a aplica\u00e7\u00e3o como administrador. Podemos verificar a conex\u00e3o com a aplica\u00e7\u00e3o com o comando: </p> <pre><code>$ wget http://[IP server_app]:8080/admin/ # Verificando conex\u00e3o com a aplica\u00e7\u00e3o\n</code></pre> <p> Agora, ao acessar o MaaS podemos criar um t\u00fanel do servi\u00e7o do servidor da aplica\u00e7\u00e3o na porta 8080 para nosso localhost na porta 8001 usando a conex\u00e3o SSH, desde que a porta 8001 n\u00e3o esteja sendo utilizada. Podemos ent\u00e3o acessar a p\u00e1gina de administrador do Django acessando no navegador o link: http://localhost:8001/admin/. </p> <pre><code>$ ssh cloud@10.103.0.X -L 8001:[IP server_app]:8080 # conectando via SSH\n</code></pre> <p>Tarefa 2.1) Dashboard do MAAS com as m\u00e1quinas.</p> <p></p> <p>Servidores na interface do MAAS</p> <p>Tarefa 2.2) Aba imagens, com as imagens sincronizadas</p> <p></p> <p>Imagens sincronizadas na interface do MAAS</p> <p>Tarefa 2.3) Da Aba de cada maquina mostrando os testes de hardware e commissioning com Status \"OK\"</p> <p>Server 1:</p> <p></p> <p>Testes de hardware do server 1</p> <p></p> <p>Comissioning do server 1</p> <p>Server 2:</p> <p></p> <p>Testes de hardware do server 2</p> <p></p> <p>Comissioning do server 2</p> <p>Server 3:</p> <p></p> <p>Testes de hardware do server 3</p> <p></p> <p>Comissioning do server 3</p> <p>Server 4:</p> <p></p> <p>Testes de hardware do server 4</p> <p></p> <p>Comissioning do server 4</p> <p>Server 5:</p> <p></p> <p>Testes de hardware do server 5</p> <p></p> <p>Comissioning do server 5</p>"},{"location":"roteiro1/main/#parte-3-checkpoint-status-dos-servidores-apos-a-instalacao-manual-do-django","title":"Parte 3: CHECKPOINT - Status dos servidores ap\u00f3s a instala\u00e7\u00e3o manual do Django","text":"<p> Com as mudan\u00e7as feitas na parte 2, antes de continuarmos precisamos garantir algumas coisas: </p> <p>Tarefa 3.1) As 2 M\u00e1quinas se mostram ativas e com seus IPs definidos no Dashboard do MaaS</p> <p></p> <p>Servidores na interface do MAAS</p> <p>Tarefa 3.2) A aplica\u00e7\u00e3o Django se encontra no ar, est\u00e1 conectada ao server e acess\u00edvel a partir do t\u00fanel</p> <p></p> <p>Aplica\u00e7\u00e3o Django conectada ao server e acess\u00edvel atrav\u00e9s do tunel</p>"},{"location":"roteiro1/main/#parte-4-ansible-deploy-automatizado-de-aplicacao","title":"Parte 4: Ansible - deploy automatizado de aplica\u00e7\u00e3o","text":"<p> Vamos partir para uma abordagem diferente agora. At\u00e9 o momento, temos apenas uma aplica\u00e7\u00e3o Django que foi instalada manualmente apenas no servidor 2. Contudo, \u00e9 comum que uma mesma aplica\u00e7\u00e3o seja alocada em mais de uma m\u00e1quina, pois podemos dividir a carga de acesso entre os n\u00f3s e, al\u00e9m disso, se um node cair o outro est\u00e1 no ar, para que nosso cliente acesse. </p> <p> Dessa forma, nesta parte foi criada uma segunda aplica\u00e7\u00e3o Django no servidor 3 que compartilha com o servidor 2 o mesmo banco de dados criado no servidor 1. Por\u00e9m, em vez de realizar os mesmos passos da instala\u00e7\u00e3o manual feita na parte 2, optou-se por utilizar o Ansible, um gerenciador de deploy que traz benef\u00edcios que ser\u00e3o detalhados mais adiante. </p> <p> Ap\u00f3s feito o deploy do terceiro servidor, o Ansible foi instalado na MAIN e um playbook foi criado no n\u00f3 3 para a instala\u00e7\u00e3o do Django. Os comandos utilizados foram: </p> <pre><code>$ sudo apt install ansible # Instala\u00e7\u00e3o do Ansible\n\n$ wget https://raw.githubusercontent.com/raulikeda/tasks/master/tasks-install-playbook.yaml # Playbook do Ansible\n\n$ ansible-playbook tasks-install-playbook.yaml --extra-vars server=[IP server3] # Instala\u00e7\u00e3o do playbook na m\u00e1quina 3\n</code></pre> <p>Tarefa 4.1) Print da tela do Dashboard do MAAS com as 3 M\u00e1quinas e seus respectivos IPs.</p> <p></p> <p>Dashboard do MaaS com os tr\u00eas servidores com deploy feito e seus respectivos IPs</p> <p>Tarefa 4.2) Print da aplica\u00e7\u00e3o Django, provando a conex\u00e3o com o server2</p> <p></p> <p>Aplica\u00e7\u00e3o Django funcionando localmente via servidor 2</p> <p>Tarefa 4.3) Print da aplica\u00e7\u00e3o Django, provando a conex\u00e3o com o server3</p> <p></p> <p>Aplica\u00e7\u00e3o Django funcionando localmente via servidor 3</p> <p>Tarefa 4.4) Diferen\u00e7a entre instalar manualmente a aplica\u00e7\u00e3o Django e utilizando o Ansible</p> <p> A diferen\u00e7a principal entre instalar manualmente uma aplica\u00e7\u00e3o Django em um host e utilizar o Ansible est\u00e1 na automa\u00e7\u00e3o, repetibilidade e efici\u00eancia da configura\u00e7\u00e3o do ambiente. </p> <p> No lugar de ter que manualmente configurar um ambiente virtual, instalar as depend\u00eancias necess\u00e1rias (armazenadas em um requirements.txt) e pacotes como Python, pip e depend\u00eancias do Django, o Ansible permite automatizar todo esse processo atrav\u00e9s de playbooks. </p> <p> Al\u00e9m de automatizar todas as etapas da instala\u00e7\u00e3o e configura\u00e7\u00e3o, o Ansible segue instru\u00e7\u00f5es precisas que proporcionam um gerenciamento de m\u00faltiplos servidores ao mesmo tempo, que \u00e9 \u00fatil para escalar a aplica\u00e7\u00e3o, e garantem um processo id\u00eantico de configura\u00e7\u00e3o na necessidade de mais m\u00e1quinas que cuidam da aplica\u00e7\u00e3o.  </p> <p> A grande vantagem de utilizar esta ferramenta \u00e9, portanto, a sua capacidade de superar uma abordagem manual que \u00e9 propensa a erros humanos, demorada e dif\u00edcil de reproduzir no momento de expans\u00e3o para outros servidores. </p>"},{"location":"roteiro1/main/#parte-5-balancamento-de-carga-usando-proxy-reverso","title":"Parte 5: Balancamento de carga usando Proxy Reverso","text":"<p> Agora que fizemos a instala\u00e7\u00e3o do Django, tanto manualmente como utilizando o Ansible, podemos criar um servidor para agir como um ponto \u00fanico de entrada, verificando a disponibilidade de cada server e garantindo uma melhor experi\u00eancia de uso para o usu\u00e1rio. O nome deste mecanismo \u00e9 Loadbalancing e ele \u00e9 especialmente \u00fatil para realizar esta distribui\u00e7\u00e3o de tr\u00e1fego de entrada por v\u00e1rios servidores privados, garantindo toler\u00e2ncia a falhas e maior estabilidade. </p> <p> Para isso, vamos utilizar o NGINX, que usa o algortimo Round Robin para balanceamento de carga para um conjunto IPs dispon\u00edveis cadastrados. Ele \u00e9 relativamente simples de implementar e n\u00e3o considera fatores como tempo de resposta do servidor e a regi\u00e3o geogr\u00e1fica de acesso, mas ser\u00e1 suficiente para nossa aplica\u00e7\u00e3o. </p> <p> Iniciando, vamos fazer o deploy de um novo servidor (server4), que ir\u00e1 realizar fazer este trabalho de Loadbalancing para os servidores em que instalamos nossas aplica\u00e7\u00f5es Django, em nosso Dashboard do MaaS. </p> <p>Tarefa 5.1) Print da tela do Dashboard do MAAS com as 4 M\u00e1quinas e seus respectivos IPs.</p> <p></p> <p>Dashboard do MaaS com os quatro servidores com deploy feito e seus respectivos IPs</p> <p> Ap\u00f3s terminar o deploy, vamos acessar o servidor 4 via SSH e, em seu terminal, vamos realizar a instala\u00e7\u00e3o do nginx. </p> <pre><code>$ ssh ubuntu@{IP Server4}\n$ sudo apt-get install nginx\n</code></pre> <p> Para definir quais servidores o Loadbalancer Round Robin utilizar\u00e1 para o balanceamento de cargas, precisamos editar o arquivo de configura\u00e7\u00e3o de sites dispon\u00edveis do Nginx: </p> <pre><code>$ sudo nano /etc/nginx/sites-available/default\n</code></pre> <p> Primeiro, vamos adicionar, antes do bloco server, um bloco upstream para que o nginx saiba quais servidores ser\u00e3o aqueles que iremos utilizar como backend para tratar as requisi\u00e7\u00f5es recebidas. No nosso caso, deixaremos assim: </p> <pre><code>upstream backend {\n    server2 {IP server2}:8080;\n    server3 {IP server3}:8080;\n}\n</code></pre> <p> Depois, vamos substituir o bloco server presente no arquivo por padr\u00e3o e vamos configura-lo para ficar escutando a porta 80 e para utilizar um proxy_pass para direcionar as requisi\u00e7\u00f5es aos server backend. </p> <pre><code>server {\n    listen 80;\n\n    location / {\n        proxy_pass http://backend;\n    }\n}\n</code></pre> <p> Para registrar as altera\u00e7\u00f5es, precisamos dar um restart no nginx com o seguinte comando: </p> <pre><code>$ sudo service nginx restart\n</code></pre> <p> Pronto! O nginx j\u00e1 est\u00e1 configurado e pronto uso. </p> <p>  Agora, s\u00f3 nos falta verificar a conex\u00e3o com os servidores 2 e 3 a partir do servidor 4. Para isso vamos iniciar modificando o views.py de cada servidor onde instalamos o Django. Vamos trocar o print presente, para podermos identificar se estamos conectados no servidor 2 ou no servidor 3. Veja um exemplo para o servidor 2: </p> <pre><code>$ sudo nano tasks/tasks/views.py\n</code></pre> <pre><code># Exemplo de resolu\u00e7\u00e3o\nfrom django.shortcuts import render\n\nfrom django.http import HttpResponse\n\ndef index(request):\n\n  return HttpResponse(\"Hello from server 2!\")\n</code></pre> <p>Tarefa 5.2) Conte\u00fado da mensagem contida na fun\u00e7\u00e3o <code>index</code> do arquivo <code>tasks/views.py</code> de cada server para distinguir ambos os servers.</p> <p></p> <p>Conex\u00e3o com o server2 a partir do t\u00fanel</p> <p></p> <p>Conex\u00e3o com o server3 a partir do t\u00fanel</p> <p>  Ap\u00f3s fazer a mesma coisa para o servidor 3, vamos sair dos servidores, incluindo a main, usando o comando: </p> <pre><code>$ exit\n</code></pre> <p>  Vamos acessar novamente a main, por\u00e9m utilizando uma pipeline para o server4: </p> <pre><code>$ ssh cloud@{IP server1} -L 8081:{IP server4}:80\n</code></pre> <p>  Este comando, basicamente, cria uma conex\u00e3o SSH com redirecionamento de porta, se conectando com o server1 (ssh cloud@{IP server1}), com redirecionamento de porta local (-L) e fazendo um t\u00fanel que: </p> <li>  Toda conex\u00e3o feita na porta 8081 da m\u00e1quina local \u00e9 encaminhada para o server1. </li> <li>  A partir de server1, o tr\u00e1fego \u00e9 redirecionado para o server4 na porta 80. </li> <p> OBS: Todas as portas mencionadas aqui, foram setadas e configuradas previamente no roteiro. </p> <p>  Com a nginx configurado e o t\u00fanel criado, podemos acessar ambos os servidores em nosso navegador com o link: </p> <li> localhost:8081/tasks/ </li> <p>  Pronto! Agora podemos verificar que ao acessar este link, uma hora somos recebidos com a mensagem do server 2, outra hora com a mensagem do server 3. </p> <p>Tarefa 5.3) Prints das respostas de cada request, provando que voce est\u00e1 conectado ao server 4, que \u00e9 o Proxy Reverso e que ele bate cada vez em um server diferente server2 e server3.</p> <p></p> <p>Conex\u00e3o com o server2 a partir do t\u00fanel</p> <p></p> <p>Conex\u00e3o com o server3 a partir do t\u00fanel</p>"},{"location":"roteiro2/main/","title":"Roteiro 2","text":""},{"location":"roteiro2/main/#introducao-e-objetivo","title":"Introdu\u00e7\u00e3o e Objetivo","text":"<p> O roteiro 2 contempla uma abordagem nova em rela\u00e7\u00e3o ao roteiro 1: a utiliza\u00e7\u00e3o de uma nova plataforma de gerenciamento de aplica\u00e7\u00f5es distribu\u00eddas. Assim, em vez de realizar as instala\u00e7\u00f5es de toda a infraestrutura manualmente (conforme foi feito no roteiro anterior), ser\u00e1 utilizado o Juju, um outro orquestrador de deploy que integra com o MaaS. </p> <p> Por conta desta nova abordagem, todas as m\u00e1quinas contendo as modifica\u00e7\u00f5es do roteiro 1 foram liberadas, retornando para o status \"Ready\" no dashboard do MaaS. </p> <p> Ao final deste roteiro, o objetivo principal \u00e9 termos, portanto, uma Cloud com um novo gerenciador de deploy instalado. </p>"},{"location":"roteiro2/main/#montagem-do-roteiro","title":"Montagem do Roteiro","text":"<p> Todo roteiro apresenta uma primeira parte denonimnada Infra e uma segunda chamada de App. Os pontos tarefas dentro de cada parte s\u00e3o os passos seguidos para a realiza\u00e7\u00e3o do roteiro.  Este modelo de organiza\u00e7\u00e3o orientado por partes e tarefas ser\u00e1 utilizado em todos os roteiros. </p>"},{"location":"roteiro2/main/#infra","title":"Infra","text":""},{"location":"roteiro2/main/#parte-1-instalacao-do-juju","title":"Parte 1: Instala\u00e7\u00e3o do Juju","text":"<p> A instala\u00e7\u00e3o do Juju foi realizada na m\u00e1quina main, acessada via SSH. O seguinte comando foi utilizado: </p> <pre><code>$ sudo snap install juju --channel 3.6\n</code></pre>"},{"location":"roteiro2/main/#parte-2-arquivos-de-definicao-de-cloud","title":"Parte 2: Arquivos de defini\u00e7\u00e3o de cloud","text":"<p> Como o Juju utilizar\u00e1 o MaaS como provedor de m\u00e1quinas e sistema operacional, inicialmente foi necess\u00e1rio garantir que o Juju reconhecesse o MaaS como um provedor de recursos v\u00e1lido. </p> <p>Ap\u00f3s essa verifica\u00e7\u00e3o, criou-se um arquivo de configura\u00e7\u00e3o chamado maas-cloud.yaml com o seguinte conte\u00fado:</p> <pre><code>  clouds:\n    maas-one:\n      type: maas\n      auth-types: [oauth1]\n      endpoint: http://192.168.0.3:5240/MAAS/\n</code></pre> <p>Em seguida, foi adicionada a cloud, utilizando o seguinte comando:</p> <pre><code>$ juju add-cloud --client -f maas-cloud.yaml maas-one\n</code></pre> <p>Por fim, foi necess\u00e1rio adicionar as credenciais MaaS para que o Juju pudesse interagir com a nova cloud adicionada. Um novo arquivo, denominado como maas-creds.yaml, foi criado com esta finalidade:</p> <pre><code>credentials:\n  maas-one:\n  anyuser:\n    auth-type: oauth1\n    maas-oauth: &lt;API KEY gerado no menu do usu\u00e1rio do MaaS&gt;\n</code></pre> <p>Essas credenciais foram aplicadas com o comando:</p> <pre><code>$ juju add-credential --client -f maas-creds.yaml maas-one\n</code></pre>"},{"location":"roteiro2/main/#parte-3-criacao-do-controlador","title":"Parte 3: Cria\u00e7\u00e3o do controlador","text":"<p>Para finalizar a infraestrutura necess\u00e1ria, foi criado um controlador no server 1 da nossa rede privada.</p> <p> Para que o Juju saiba em qual servidor o controlador ir\u00e1 ficar, criou-se a tag 'juju' no server 1 atrav\u00e9s dashboard do Maas e, em seguida, foi executado o comando a seguir: </p> <pre><code>$ juju bootstrap --bootstrap-series=jammy --constraints tags=juju maas-one maas-controller\n</code></pre> <p> Esse comando utiliza a s\u00e9rie jammy e define o nome do controlador como maas-controller, vinculado \u00e0 cloud maas-one. </p>"},{"location":"roteiro2/main/#app","title":"App","text":"<p>Para uma melhor visualiza\u00e7\u00e3o do passo a passo feito a seguir, recomendamos manter um terminal aberto ao lado rodando o seguinte comando:</p> <pre><code>$ watch -n 1 -c juju status --color\n</code></pre> <p>Nele, ser\u00e1 poss\u00edvel visualizar cada mudan\u00e7a feita na infraestrutura em tempo real.</p>"},{"location":"roteiro2/main/#parte-1-instalacao-do-juju-dashboard-para-o-controller","title":"Parte 1: Instala\u00e7\u00e3o do Juju dashboard para o controller","text":"<p> Para a instala\u00e7\u00e3o do juju dashboard, deve-se, primeiro, ter certeza de que estamos no controlador e, sem seguida, executar o comando necess\u00e1rio: </p> <pre><code>$ juju switch maas-controller:admin/maas\n\n$ juju deploy juju-dashboard dashboard\n</code></pre>"},{"location":"roteiro2/main/#parte-2-deploy-da-aplicacao-grafana-e-prometheus","title":"Parte 2: Deploy da aplica\u00e7\u00e3o Grafana e Prometheus","text":"<p>Com o Juju dashboard instalado, prosseguimos para a aplica\u00e7\u00e3o e a configura\u00e7\u00e3o do banco de dados.</p> <p> Neste contexto, ser\u00e1 utilizado o Prometheus como banco de dados e o Grafana como plataforma de apresenta\u00e7\u00e3o visual dos n\u00fameros (tipicamente gr\u00e1ficos e pain\u00e9is). </p> <p> O primeiro passo tomado foi a cria\u00e7\u00e3o um novo modelo, que chamaremos de <code>openstack</code>, onde iremos baixar e realizar as configura\u00e7\u00f5es das aplica\u00e7\u00f5es. </p> <pre><code>$ juju add-model --config default-series=jammy openstack\n\n$ juju switch openstack \n</code></pre> <p> Ap\u00f3s isso, foi criado uma pasta chamada charms para baixar o charm do Grafana e do Prometheus do reposit\u00f3rio charm-hub: </p> <pre><code>$ mkdir -p /home/cloud/charms\n\n$ cd /home/cloud/charms\n</code></pre> <p>Em seguida, foi realizado o download das duas ferramentas:</p> <pre><code>$ juju download grafana\n\n$ juju download prometheus2\n</code></pre> <p>E, por fim, o deploy dos charms de cada uma:</p> <pre><code>$ juju deploy ./grafana_r69.charm --base=ubuntu@20.04\n\n$ juju deploy ./prometheus2_r60.charm\n</code></pre> <p>Nota:  Nos \u00faltimos comandos, utilizamos o deploy do grafana em uma vers\u00e3o espec\u00edfica, para <code>ubuntu@20.04</code>, por conta de problemas de compatibilidade com vers\u00f5es mais novas.</p>"},{"location":"roteiro2/main/#parte-3-integracao-do-grafana-com-o-prometheus","title":"Parte 3: Integra\u00e7\u00e3o do Grafana com o Prometheus","text":"<p>Para que o Grafana mostre os dados contidos no Prometheus, foi utilizado o seguinte comando de integra\u00e7\u00e3o:</p> <pre><code>$ juju integrate grafana:grafana-source prometheus2:grafana-source\n</code></pre> <p></p> <p>Dashboard do MAAS com as m\u00e1quinas e seus respectivos IPs</p> <p></p> <p>Tela do comando \"juju status\" com o Grafana ativo</p> <p>Se desconectando da m\u00e1quina MAIN, demos um <code>ssh</code> para entrar na MAIN novamente, por\u00e9m fazendo um t\u00fanel da porta do Grafana (<code>3000</code> por padr\u00e3o) para uma porta da nossa localhost (<code>8001</code>):</p> <pre><code>$ ssh cloud@10.103.1.X -L 8001:{IP Server Grafana}:3000\n</code></pre> <p>Acessando o Dashboard em nosso navegador (localhost:8001/login), foi utilizado o seguinte procedimento para fazer o login:</p> <ul> <li>Login:        <ul> <li>             Por padr\u00e3o: <code>admin</code> </li> </ul> </li> <li>Senha:        <ul> <li>             Para obter a senha do Grafana, temos que pedir ao juju para rodar o comando <code>get-admin-password</code> na unidade onde est\u00e1 o servi\u00e7o:           </li> </ul> </li> </ul> <pre><code>$ juju run grafana/1 get-admin-password\n</code></pre> <p>Agora, dentro do Dashboard do <code>Grafana</code>, o \u00faltimo passo foi conferir se a integra\u00e7\u00e3o foi feita corretamente. Para isso, foi criado um dashboard dentro do Grafana e foi selecionado o <code>Prometheus</code> como source.</p> <p></p> <p>Tela do Dashboard do Grafana com o Prometheus aparecendo como source</p> <p></p> <p>Acesso ao Dashboard do Grafana a partir da rede do Insper</p> <p></p> <p>Aplica\u00e7\u00f5es sendo gerenciadas pelo Juju </p> <p> Para seguir adiante para o pr\u00f3ximo roteiro, o controlador foi deletado usando o comando juju destroy-controller main e o ambiente foi, novamente, reconfigurado para a estaca zero. </p>"},{"location":"roteiro3/main/","title":"Roteiro 3","text":""},{"location":"roteiro3/main/#introducao-e-objetivo","title":"Introdu\u00e7\u00e3o e Objetivo","text":"<p> O roteiro 3 contempla uma abordagem nova, mantendo a utliza\u00e7\u00e3o do Juju para gerenciamento de aplica\u00e7\u00f5es distribu\u00eddas mas introduzindo agora o openstack. </p> <p> Ao final deste roteiro, o objetivo principal \u00e9 termos, portanto, uma Cloud com um novo gerenciador de deploy instalado, utilizando o OpenStack como plataforma de nuvem privada.  </p> <p>  O roteiro \u00e9 dividido em tr\u00eas etapas: cria\u00e7\u00e3o da infraestrutura (deploy do OpenStack), configura\u00e7\u00e3o dos servi\u00e7os e redes da nuvem, e, por fim, a utiliza\u00e7\u00e3o da infraestrutura com o deployment de aplica\u00e7\u00f5es dentro de m\u00e1quinas virtuais gerenciadas pela nuvem OpenStack. </p>"},{"location":"roteiro3/main/#montagem-do-roteiro","title":"Montagem do Roteiro","text":"<p> Todo roteiro apresenta uma primeira parte denonimnada Infra e uma segunda chamada de App. No caso deste roteiro especificamente, tamb\u00e9m h\u00e1 uma se\u00e7\u00e3o intermedi\u00e1ria de configura\u00e7\u00e3o pr\u00e9via para o App, denominada de Setup. Os pontos tarefas dentro de cada parte s\u00e3o os passos seguidos para a realiza\u00e7\u00e3o do roteiro.  Este modelo de organiza\u00e7\u00e3o orientado por partes e tarefas ser\u00e1 utilizado em todos os roteiros. </p>"},{"location":"roteiro3/main/#infra","title":"Infra","text":""},{"location":"roteiro3/main/#parte-0-detalhamento-dos-servicos-implantados-na-cloud","title":"Parte 0: Detalhamento dos servi\u00e7os implantados na cloud","text":"<p> Foi implantada uma infraestrutura de nuvem privada utilizando OpenStack, orquestrada com MAAS e Juju, e com armazenamento distribu\u00eddo via Ceph. Antes de tudo, foi necess\u00e1rio garantir que todas as bridges da nossa nuvem estavam corretamente configuradas com a denomina\u00e7\u00e3o \"br-ex\", j\u00e1 que esta \u00e9 a interface de rede que conectar\u00e1 o OpenStack \u00e0 rede externa. </p> <p>  Tamb\u00e9m, foi utilizada a interface do MaaS para garantir as especifica\u00e7\u00f5es necess\u00e1rias para as nossas m\u00e1quinas f\u00edsicas que est\u00e3o listadas a seguir: </p> Servidor Tags CPUs NICs RAM Disks Storage server1 controller, juju 2 1 12 1 80 server2 reserva 2 1 16 2 80 server3 compute 2 1 32 2 80 server4 compute 2 1 32 2 80 server5 compute 2 1 32 2 80 <p>  Por fim, foi definido o modelo do deploy e seguido o tutorial oficial do OpenStack para instalar as seguintes depend\u00eancias:  </p> <pre><code>juju add-model --config default-series=jammy openstack\n\njuju switch maas-controller:openstack\n</code></pre> <ul> <li>Ceph OSD</li> <li>Nova Compute</li> <li>MySQL InnoDB Cluster</li> <li>Vault</li> <li>Neutron Networking</li> <li>Keystone </li> <li>RabbitMQ</li> <li>Nova Cloud Controller</li> <li>Placement</li> <li>Horizon - OpenStack Dashboard</li> <li>Glance </li> <li>Ceph Monitor</li> <li>Cinder</li> <li>Ceph RADOS Gateway</li> </ul> <p>Com as devidas integra\u00e7\u00f5es realizadas, foi executado um \u00faltimo comando de finaliza\u00e7\u00e3o da Infra:</p> <pre><code>juju config ceph-osd osd-devices='/dev/sdb'\n</code></pre>"},{"location":"roteiro3/main/#setup","title":"Setup","text":"<p>No setup, foi feita a configura\u00e7\u00e3o dos servi\u00e7os que controlam:</p> <ul> <li>as VMs (Nova);</li> <li>os volumes de disco (Cinder);</li> <li>a estrutura de rede virtual (Neutron).</li> </ul>"},{"location":"roteiro3/main/#parte-1-autenticacao","title":"Parte 1: Autentica\u00e7\u00e3o","text":""},{"location":"roteiro3/main/#parte-2-horizon","title":"Parte 2: Horizon","text":""},{"location":"roteiro3/main/#parte-3-imagens-e-flavors","title":"Parte 3: Imagens e Flavors","text":""},{"location":"roteiro3/main/#parte-4-rede-externa","title":"Parte 4: Rede Externa","text":""},{"location":"roteiro3/main/#parte-5-rede-interna-e-roteador","title":"Parte 5: Rede Interna e Roteador","text":""},{"location":"roteiro3/main/#parte-6-conexao","title":"Parte 6: Conex\u00e3o","text":""},{"location":"roteiro3/main/#parte-7-instancia","title":"Parte 7: Inst\u00e2ncia","text":"<p>Tarefa 3) Desenho da arquitetura de rede, desde a sua conex\u00e3o com o Insper at\u00e9 a inst\u00e2ncia alocada.</p> <p></p> <p>Arquitetura de rede</p>"},{"location":"roteiro3/main/#app","title":"App","text":""},{"location":"roteiro3/main/#preparacao-da-arquitetura","title":"Prepara\u00e7\u00e3o da arquitetura","text":"<p> A principal tarefa a ser completada no App foi a utiliza\u00e7\u00e3o da infraestrutura configurada at\u00e9 o momento para colocar o projeto da disciplina na cloud criada.  </p> <p>  Desenvolvido em paralelo, o projeto da disciplina \u2014 dispon\u00edvel neste reposit\u00f3rio \u2014 consistiu em uma aplica\u00e7\u00e3o FastAPI composta por tr\u00eas endpoints simples. Para simular uma nuvem mais pr\u00f3xima da realidade, foi adotada a seguinte topologia:  </p> <ul> <li>2 inst\u00e2ncias com a API desenvolvida;</li> <li>1 inst\u00e2ncia com banco de dados Postgres;</li> <li>1 inst\u00e2ncia com LoadBalancer, Nginx.</li> </ul> <p>  Em ambientes de produ\u00e7\u00e3o, utilizar duas inst\u00e2ncias da API \u00e9 considerado uma boa pr\u00e1tica, pois permite garantir balanceamento de carga, toler\u00e2ncia a falhas e alta disponibilidade. Dessa forma, em vez de a requisi\u00e7\u00e3o do cliente chegar diretamente a uma inst\u00e2ncia da API, ela \u00e9 encaminhada primeiro ao Load Balancer, que distribui as requisi\u00e7\u00f5es entre as inst\u00e2ncias dispon\u00edveis, as quais est\u00e3o conectadas ao banco de dados da aplica\u00e7\u00e3o. A seguir, apresenta-se um diagrama que ilustra essa arquitetura:  </p> <p></p> <p>Topologia de uso da infraestrutura</p> <p>  Para iniciar a implementa\u00e7\u00e3o, o dashboard do Horizon foi acessado (via NAT) e, nele, foram criadas as quatro inst\u00e2ncias com os seguintes nomes: load-balancer, api-1, api-2 e database. </p> <p></p> <p>Local de visualiza\u00e7\u00e3o e cria\u00e7\u00e3o de inst\u00e2ncias no dashboard do Horizon</p> <p>  Ap\u00f3s criadas as inst\u00e2ncias, foi necess\u00e1rio atribuir a cada uma um IP Flutuante para que fosse poss\u00edvel atingir todas por meio da NUC Main. Afinal, conforme mostrado no diagrama de topologia da rede anteriormente, as inst\u00e2ncias se encontram em uma subrede e possuem IPs que n\u00e3o s\u00e3o \u00fateis para conversar com m\u00e1quinas que se encontram na rede externa. </p> <p> Para isso, foram atribu\u00eddos IPs flutuantes para cada inst\u00e2ncia (uma por vez), reutilizando os comandos vistos no Setup:  </p> <pre><code>FLOATING_IP=$(openstack floating ip create -f value -c floating_ip_address ext_net)\n\nopenstack server add floating ip nome-da-instancia $FLOATING_IP\n</code></pre> <p> Al\u00e9m disso, vale ressaltar que, a fim de baratear o custo de manuten\u00e7\u00e3o da arquitetura ao m\u00e1ximo sem prejudicar o desempenho do cliente, optou-se pelo flavor m1.tiny para todas as inst\u00e2ncias.  </p> <p> Contudo, para aplica\u00e7\u00f5es com maior demanda e um volume de dados maior para armazenamento, seria prov\u00e1vel a necessidade de escolher flavors maiores do que uma aplica\u00e7\u00e3o em ambiente controlado de aprendizado. Afinal, em uma cloud comercial, o custo \u00e9 proporcional ao tamanho do flavor e seu tempo de uso. </p>"},{"location":"roteiro3/main/#configuracao-da-instancia-loadbalancer","title":"Configura\u00e7\u00e3o da inst\u00e2ncia LoadBalancer","text":"<p> Primeiramente, foi instalado o Nginx por meio do seguinte comandos: </p> <pre><code>sudo apt-get install nginx\n</code></pre> <p> Em seguida, foi necess\u00e1rio editar o arquivo que permitiria o nginx a enxergar as duas inst\u00e2ncias para as quais o Load Balancer apontaria, conforme o diagrama ilustrado no in\u00edcio do relat\u00f3rio: </p> <pre><code># Acessando arquivo de configura\u00e7\u00e3o do nginx\nsudo nano /etc/nginx/sites-available/default\n</code></pre> <p> Dentro do arquivo, as altera\u00e7\u00f5es feitas foram as seguintes: </p> <pre><code>upstream backend {\n        server [IP de subrede da inst\u00e2ncia da API 1]:8080;\n        server [IP de subrede da inst\u00e2ncia da API 2]:8080;\n}\n\nserver {\n        listen 80 default_server;\n        listen [::]:80 default_server;\n\n        location / { proxy_pass http://backend; \n}\n</code></pre> <p> Por fim, o servi\u00e7o do nginx foi devidamente reinicializado para salvar as altera\u00e7\u00f5es feitas no arquivo: </p> <pre><code>sudo service nginx restart\n</code></pre>"},{"location":"roteiro3/main/#instalacao-do-docker","title":"Instala\u00e7\u00e3o do Docker","text":"<p> Tanto para as inst\u00e2ncias de API quanto para o banco de dados, foi necess\u00e1rio realizar a instala\u00e7\u00e3o do docker. Afinal, puxaremos a mesma imagem para as duas inst\u00e2ncias de API do projeto pelo docker hub. Para isso, foi seguido o tutorial oficial no site do docker. Os comandos executados, portanto, foram: </p> <pre><code># Add Docker's official GPG key:\nsudo apt-get update\nsudo apt-get install ca-certificates curl\nsudo install -m 0755 -d /etc/apt/keyrings\nsudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc\nsudo chmod a+r /etc/apt/keyrings/docker.asc\n\n# Add the repository to Apt sources:\necho \\\n  \"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu \\\n  $(. /etc/os-release &amp;&amp; echo \"${UBUNTU_CODENAME:-$VERSION_CODENAME}\") stable\" | \\\n  sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null\nsudo apt-get update\n\nsudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin \n</code></pre>"},{"location":"roteiro3/main/#configuracao-das-instancias-de-api","title":"Configura\u00e7\u00e3o das inst\u00e2ncias de API","text":"<p> Primeiramente, foram configuradas as vari\u00e1veis de ambiente necess\u00e1rias no arquivo .env. Aqui, al\u00e9m da URL para a intera\u00e7\u00e3o com o banco de dados, tamb\u00e9m foram informadas as chaves necess\u00e1rias para fazer as requisi\u00e7\u00f5es para a API de cota\u00e7\u00e3o do d\u00f3lar e do euro (um dos endpoints da aplica\u00e7\u00e3o): </p> <pre><code>AWESOME_API_KEY=\"API_KEY_AQUI\"\nSECRET_KEY=\"SECRET_KEY_AQUI\"\nALGORITHM=HS256\n\nPOSTGRES_USER=cloud\nPOSTGRES_PASSWORD=senha\nPOSTGRES_DB=cloud\nDATABASE_URL=postgresql://cloud:senha@[IP de subrede do Banco de Dados]:5432/cloud\n</code></pre> <p> Em seguida, a imagem do projeto publicada no docker hub foi puxada e o container foi executada nas duas inst\u00e2ncias. </p> <pre><code>sudo docker pull antoniolma/app\nsudo docker run -p 8080:80 --env-file .env -d antoniolma/app\n\n# Verificando se o container est\u00e1 sendo executado na m\u00e1quina\nsudo docker ps -a\n</code></pre>"},{"location":"roteiro3/main/#configuracao-da-instancia-do-banco-de-dados","title":"Configura\u00e7\u00e3o da inst\u00e2ncia do Banco de Dados","text":"<p> Primeiramente, foi instalado o PostgreSQL por meio dos seguintes comandos: </p> <pre><code>sudo apt update\nsudo apt install postgresql postgresql-contrib -y\n</code></pre> <p> Em seguida, assim como feito nas APIs, foram configuradas as vari\u00e1veis de ambiente necess\u00e1rias no arquivo .env: </p> <pre><code>POSTGRES_USER=cloud\nPOSTGRES_PASSWORD=senha\nPOSTGRES_DB=cloud\n</code></pre> <p> Por fim, foi executado o docker na porta padr\u00e3o do postgres para que as inst\u00e2ncias de API possam enxergar o banco de dados: </p> <pre><code>sudo docker run -p 5432:5432 --env-file .env -d postgres\n</code></pre>"},{"location":"roteiro3/main/#verificacao-final","title":"Verifica\u00e7\u00e3o final","text":"<p> Para conferir o devido funcionamento da infraestrutura, foi criado um t\u00fanel SSH que conectasse o computador local a uma das inst\u00e2ncias de API criada. Para isso, foi executado o mesmo comando de t\u00fanel utilizado no roteiro 1, com as devidas adapta\u00e7\u00f5es necess\u00e1rias: </p> <pre><code>$ ssh cloud@10.103.1.10 -L 8080:[IP flutuante load-balancer]:80 \n</code></pre> <p>Tarefa 4.2) Arquitetura de rede da infraestrutura dentro do Dashboard do OpenStack</p> <p></p> <p>Arquitetura de rede final</p> <p>Tarefa 4.3) Lista de VMs utilizadas com nome e IPs alocados,</p> <p></p> <p>Visualiza\u00e7\u00e3o dos nomes e IPs alocados via dashboard do Horizon</p> <p>Tarefa 4.4) Dashboard do FastAPI conectado via m\u00e1quina Nginx/LB.</p> <p></p> <p>Dashboard do FastAPI acessado ap\u00f3s a aplica\u00e7\u00e3o do t\u00fanel SSH via m\u00e1quina Nginx</p> <p>Tarefa 4.5) Servidores (m\u00e1quina fisica) alocados para cada inst\u00e2ncia pelo OpenStack.</p> <p></p> <p>M\u00e1quina f\u00edsica alocada para o Load Balancer pelo OpenStack: Server 5</p> <p></p> <p>M\u00e1quina f\u00edsica alocada para a API 1 pelo OpenStack: Server 4</p> <p></p> <p>M\u00e1quina f\u00edsica alocada para a API 2 pelo OpenStack: Server 2</p> <p></p> <p>M\u00e1quina f\u00edsica alocada para o Banco de dados Postgres pelo OpenStack: Server 3</p>"},{"location":"roteiro4/main/","title":"Roteiro 4","text":""},{"location":"roteiro4/main/#introducao-e-objetivo","title":"Introdu\u00e7\u00e3o e Objetivo","text":"<p> O roteiro 4 contempla uma abordagem nova, mantendo a utliza\u00e7\u00e3o do Juju para gerenciamento de aplica\u00e7\u00f5es distribu\u00eddas e o openstack implementado no roteiro anterio, mas explorando agora o conceito de Infraestrutura como c\u00f3digo atrav\u00e9s do Terraform. </p>"},{"location":"roteiro4/main/#montagem-do-roteiro","title":"Montagem do Roteiro","text":"<p> Todo roteiro apresenta uma primeira parte denominada Infra e uma segunda chamada de App. Os pontos tarefas dentro de cada parte s\u00e3o os passos seguidos para a realiza\u00e7\u00e3o do roteiro.  Este modelo de organiza\u00e7\u00e3o orientado por partes e tarefas ser\u00e1 utilizado em todos os roteiros. </p>"},{"location":"roteiro4/main/#infra","title":"Infra","text":"<p> At\u00e9 o momento, foram utilizados o dashboard e a interface de linha de comando (CLI) para criar rede, subrede, inst\u00e2ncias, roteadorres e outros recursos. Para criar a infraestrutura necess\u00e1ria agora, conforme ser\u00e1 visto mais a frente no App, foi utilizado somente c\u00f3digo. </p> <p> Primeiramente, para n\u00e3o confundir os recursos de cada usu\u00e1rio, foi criada uma separa\u00e7\u00e3o l\u00f3gica de dois usu\u00e1rios inseridos em um mesmo dom\u00ednio (assim como deveria acontecer em nuvem): aluno1 e aluno2.  </p>"},{"location":"roteiro4/main/#parte-1-criando-um-unico-domain","title":"Parte 1: Criando um \u00fanico Domain","text":"<p> Via Horizon Dashboard, foi criado o dom\u00ednio AlunosDomain: </p> <p></p> <p>Cria\u00e7\u00e3o de dom\u00ednio novo do Horizon Dashboard</p> <p> Em seguida, o novo dom\u00ednio criado foi definido como o novo contexto de uso, conforme mostra a imagem a seguir: </p> <p></p> <p>Novo dom\u00ednio como contexto de uso</p>"},{"location":"roteiro4/main/#parte-2-criando-um-projeto-para-cada-aluno","title":"Parte 2: Criando um projeto para cada Aluno","text":"<p> Para separar o aluno1 do aluno2, foram feitos dois projetos que respeitassem o padr\u00e3o Kit + letra do kit + nome_do_aluno. </p> <p></p> <p>Interface de cria\u00e7\u00e3o dos projetos para cada usu\u00e1rio aluno</p> <p> Por fim, os dois usu\u00e1rios foram criados tamb\u00e9m utilizando a interface do Horizon Dashboard, dando aten\u00e7\u00e3o especial para garantir que ambos tivessem pap\u00e9is administrativos tanto no momento de cria\u00e7\u00e3o, quanto na configura\u00e7\u00e3o do dom\u00ednio. O dom\u00ednio AlunosDomain e os respectivos projetos criados foram informados para cada um. </p> <p></p> <p>Interface de cria\u00e7\u00e3o dos usu\u00e1rios</p> <p></p> <p>Concedendo papel administrativo para ambos os usu\u00e1rios no dom\u00ednio criado</p>"},{"location":"roteiro4/main/#app","title":"App","text":""},{"location":"roteiro4/main/#questionario-projeto-ou-plano","title":"Question\u00e1rio, Projeto ou Plano","text":"<p>Esse se\u00e7\u00e3o deve ser preenchida apenas se houver demanda do roteiro.</p>"},{"location":"roteiro4/main/#discussoes","title":"Discuss\u00f5es","text":"<p>Quais as dificuldades encontradas? O que foi mais f\u00e1cil? O que foi mais dif\u00edcil?</p>"},{"location":"roteiro4/main/#conclusao","title":"Conclus\u00e3o","text":"<p>O que foi poss\u00edvel concluir com a realiza\u00e7\u00e3o do roteiro?</p>"}]}