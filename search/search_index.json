{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Computa\u00e7\u00e3o em Nuvem","text":"Edi\u00e7\u00e3o <p>2025.1</p>"},{"location":"#kit-a","title":"KIT-A","text":"<p>Antonio Lucas Michelon de Almeida</p> <p>Pedro Nery Affonso dos Santos</p>"},{"location":"#entregas","title":"Entregas","text":"<ul> <li> Roteiro 1 </li> <li> Roteiro 2</li> <li> Roteiro 3</li> <li> Roteiro 4</li> <li> Projeto</li> </ul>"},{"location":"#referencias","title":"Refer\u00eancias","text":"<p>Material for MkDocs</p>"},{"location":"projeto/main/","title":"Projeto 2025.1","text":"<p> O projeto consiste na constru\u00e7\u00e3o e desenvolvimento de uma API RESTful para cadastro e autentica\u00e7\u00e3o de usu\u00e1rios, utilizando ferramentas do framework FastAPI </p> <p> Ap\u00f3s a constru\u00e7\u00e3o da API, o projeto deve ser dockerizado, publicado no Docker Hub e, por fim, implantado no AWS. </p> <p> Para uma melhor organiza\u00e7\u00e3o do projeto, ele foi dividido em tr\u00eas entregas. </p>"},{"location":"projeto/main/#objetivo-da-avaliacao","title":"Objetivo da Avalia\u00e7\u00e3o","text":"<p> Avaliar o dom\u00ednio dos alunos em: </p> <ul> <li>Containeriza\u00e7\u00e3o local com Docker Compose</li> <li>Deploy em ambiente de nuvem com AWS Lightsail</li> <li>Conex\u00e3o segura com banco de dados</li> <li>Estrutura\u00e7\u00e3o de aplica\u00e7\u00e3o web com FastAPI</li> <li>Boas pr\u00e1ticas de c\u00f3digo, documenta\u00e7\u00e3o e custo</li> </ul>"},{"location":"projeto/main/#entrega-1","title":"Entrega 1","text":""},{"location":"projeto/main/#construcao-da-api","title":"Constru\u00e7\u00e3o da API","text":"<p> Este projeto consiste em desenvolver uma API RESTful em FastAPI, com tr\u00eas endpoints b\u00e1sicos: </p> <ul> <li> POST /registrar: Usu\u00e1rio entra com <code>nome</code>, <code>email</code> e <code>senha</code>.         <ul> <li>C\u00f3digo 200: (Sucesso) <code>email</code> n\u00e3o encontrado na base de dados, faz o registro de um novo usu\u00e1rio, com senha codificada em formato <code>HASH</code>, e retorna o JWT Token.</li> <li>C\u00f3digo 409: (Error) <code>email</code> j\u00e1 se enconta cadastrado, cancela opera\u00e7\u00e3o.</li> </ul> </li> <li> POST /login: Usu\u00e1rio entra com <code>email</code> e <code>senha</code>.         <ul> <li>C\u00f3digo 200: (Sucesso) Conta associada ao <code>email</code> \u00e9 encontrada e a <code>senha</code> recebida bate com a senha codificada no banco de dados, faz o login do usu\u00e1rio e retorna o JWT Token..</li> <li>C\u00f3digo 401: (Error) <code>email</code> recebido n\u00e3o \u00e9 encontrado na base de dados</li> <li>C\u00f3digo 401: (Error) <code>senha</code> n\u00e3o confere com a senha codificada na base</li> </ul> </li> <li> GET /consultar:          <ul> <li>C\u00f3digo 200: (Sucesso) Verifica\u00e7\u00e3o bem sucedida do JWT Token e requisi\u00e7\u00e3o feita com sucesso \u00e0 Awesome API, cuja resposta \u00e9 devolvida ao cliente: a cota\u00e7\u00e3o atual do Euro e do Dol\u00e1r em rela\u00e7\u00e3o ao Real</li> <li>C\u00f3digo 403: (Error) JWT Token inv\u00e1lido ou ausente no header.</li> </ul> </li> </ul> <p></p> <p>Documenta\u00e7\u00e3o dos endpoints da API</p> <p> Ap\u00f3s a implementa\u00e7\u00e3o de cada um dos endpoints, tratando cada caso poss\u00edvel mencionado anteriormente, e integra\u00e7\u00e3o com o banco de dados PostgreSQL, \u00e9 necess\u00e1rio testar os endpoints para passar para a pr\u00f3xima etapa. </p> V\u00eddeo mostrando funcionalidades da API (Testes) <p></p> <p>Teste do endpoint \"/registrar\" (Success: Cadastro realizado)</p> <p></p> <p>Teste do endpoint \"/registrar\" (Fail: Email j\u00e1 registrado)</p> <p></p> <p>Teste do endpoint \"/login\" (Success: Login bem-sucedido)</p> <p></p> <p>Teste do endpoint \"/login\" (Fail: Email n\u00e3o registrado)</p> <p></p> <p>Teste do endpoint \"/login\" (Fail: Senha incorreta)</p> <p></p> <p>Teste do endpoint \"/consultar\" (Success: [Retorna cota\u00e7\u00e3o Dolar e Euro])</p> <p></p> <p>Teste do endpoint \"/consultar\" (Fail: JWT ausente ou inv\u00e1lido)</p>"},{"location":"projeto/main/#dockerizing","title":"Dockerizing","text":"<p> Com o c\u00f3digo da API pronto, a pr\u00f3xima etapa \u00e9 dockerizar/containerizar dos 2 servi\u00e7os juntos: a aplica\u00e7\u00e3o e o banco de dados. Dentro do docker, \u00e9 necess\u00e1rio a cria\u00e7\u00e3o de um arquivo <code>Dockerfile</code> (de acordo com a linguagem e ambiente de execu\u00e7\u00e3o escolhidos) e <code>compose.yaml</code> para a execu\u00e7\u00e3o da aplica\u00e7\u00e3o, sendo capaz de se conectar ao banco de dados e realizar as opera\u00e7\u00f5es de CRUD.  </p> <p> N\u00e3o s\u00f3 isso, mas a aplica\u00e7\u00e3o deve ser autocontida, ou seja, deve ser poss\u00edvel executar a aplica\u00e7\u00e3o apenas com o comando <code>docker compose up</code>. </p> <p> Ao final da Dockeriza\u00e7\u00e3o, a sa\u00edda desse comando devolve: </p> <pre><code>&gt; docker compose up -d\n[+] Running 2/2\n \u2714 Container inspercloud-projeto-db-1   Running\n \u2714 Container inspercloud-projeto-app-1  Running  \n\n&gt; docker compose ps\nNAME                        IMAGE                   SERVICE   STATUS         PORTS\ninspercloud-projeto-app-1   antoniolma/app:v1.0.1   app       Up 3 seconds   0.0.0.0:8080-&gt;80/tcp\ninspercloud-projeto-db-1    postgres:17             db        Up 8 seconds   0.0.0.0:5432-&gt;5432/tcp\n</code></pre> <p> A organiza\u00e7\u00e3o do diret\u00f3rio ao final do projeto: </p> <pre><code>\u251c\u2500\u2500 api/\n\u2502   \u251c\u2500\u2500 Dockerfile          # Dockerfile que instala depend\u00eancias e copia api/app/\n\u2502   \u251c\u2500\u2500 requirements.txt    # depend\u00eancias para o python\n\u2502   \u2514\u2500\u2500 app/                # c\u00f3digo com FastAPI\n\u2502       \u251c\u2500\u2500 app.py\n\u2502       \u2514\u2500\u2500 models.py\n\u251c\u2500\u2500 .env                    # vari\u00e1veis de ambiente (POSTGRES_*, DATABASE_URL, SECRET_KEY\u2026)\n\u2514\u2500\u2500 docker-compose.yaml     # orquestra\u00e7\u00e3o dos servi\u00e7os db + app\n</code></pre> <p> Para subir o container no Docker Hub, foi utilizado os seguintes comandos: </p> <ol> <li> Login no Docker Hub:     <pre><code>docker login\n</code></pre> </li> <li> Build da imagem e inicializa\u00e7\u00e3o dos containers:     <pre><code>docker compose up --build\n</code></pre> </li> <li> Push da vers\u00e3o atual para o Docker Hub:     <pre><code>docker push antoniolma/app  \n</code></pre> </li> </ol> <p> Por\u00e9m, para apenas utilizar a aplica\u00e7\u00e3o, ao baixar o projeto no reposit\u00f3rio do Github, basta utilizar o comando: </p> <pre><code>docker compose up\n</code></pre> <p> Caso tenha curiosidade, \u00e9 poss\u00edvel encontrar o projeto nos links: <ul> <li> Link projeto Github </li> <li> Link projeto DockerHub </li> </ul> </p>"},{"location":"roteiro1/main/","title":"Roteiro 1","text":""},{"location":"roteiro1/main/#introducao-e-objetivo","title":"Introdu\u00e7\u00e3o e Objetivo","text":"<p>O roteiro 1 contempla a funda\u00e7\u00e3o de toda a infraestrutura da Cloud que foi montada a partir de um KIT que contava com os seguintes componentes:</p> <ul> <li>1 NUC (main) com 10Gb e 1 SSD (120 Gb)</li> <li>1 NUC (server1) com 12Gb e 1 SSD (120 Gb)</li> <li>1 NUC (server2) com 16Gb e 2 SSD (120 Gb + 120 Gb)</li> <li>3 NUCs (server3, server4 e server5) com 32 Gb e 2 SSD (120 Gb + 120 Gb)</li> <li>1 Switch DLink DSG-1210-28 de 28 portas</li> <li>1 Roteador TP-Link TL-R470T+</li> </ul> <p>Ao longo deste roteiro, passaremos pelos seguintes passos:</p> <ul> <li> <p>Configura\u00e7\u00e3o do KIT via cabo</p> <ul> <li>Instala\u00e7\u00e3o do Ubuntu Server</li> <li>Instala\u00e7\u00e3o do MAAS</li> <li>Configura\u00e7\u00e3o do MAAS</li> <li>Reconfigura\u00e7\u00e3o do DHCP</li> <li>Cadastro dos servidores via MAAS</li> <li>Cria\u00e7\u00e3o das pontes OVS    </li> </ul> </li> <li> <p>Configura\u00e7\u00e3o do KIT via acesso remoto</p> <ul> <li>Configura\u00e7\u00e3o de um servidor de banco de dados Postgres</li> <li>Deploy de uma aplica\u00e7\u00e3o Django</li> <li>Utiliza\u00e7\u00e3o do Ansible</li> <li>...</li> </ul> </li> </ul> <p> Ao final deste roteiro, o objetivo principal \u00e9 termos, portanto, uma Cloud com um primeiro gerenciador de deploy instalado. A partir disso, o cliente j\u00e1 ser\u00e1 capaz de realizar requisi\u00e7\u00f5es ao servidor se estiver conectado \u00e0 rede Wi-Fi do Insper.  </p>"},{"location":"roteiro1/main/#montagem-do-roteiro","title":"Montagem do Roteiro","text":"<p> Todo roteiro apresenta uma primeira parte denonimnada Infra e uma segunda chamada de App. Os pontos tarefas dentro de cada parte s\u00e3o os passos seguidos para a realiza\u00e7\u00e3o do roteiro.  Este modelo de organiza\u00e7\u00e3o orientado por partes e tarefas ser\u00e1 utilizado em todos os roteiros. </p>"},{"location":"roteiro1/main/#infra","title":"Infra","text":""},{"location":"roteiro1/main/#parte-0-enderecos-mac-das-nucs-e-ip-do-roteador","title":"Parte 0: Endere\u00e7os MAC das NUCs e IP do roteador","text":"<p> Antes de iniciar qualquer instala\u00e7\u00e3o, foi essencial capturar imagens de todos os endere\u00e7os MAC dos servidores (1 a 5).  </p> <p> Essa a\u00e7\u00e3o ocorreu simultaneamente \u00e0 Tarefa 1, pois, a partir da Tarefa 2, as fontes de alimenta\u00e7\u00e3o n\u00e3o podem mais ser desconectadas e, devido \u00e0 forma como o kit foi montado, n\u00e3o seria poss\u00edvel remover as NUCs de suas posi\u00e7\u00f5es para visualizar os endere\u00e7os sem deslig\u00e1-las da tomada. </p> <p> Al\u00e9m disso, foi realizado um pr\u00e9-roteiro relativo \u00e0 montagem de um cabo de rede. Com a conex\u00e3o ethernet, foi acessada a interface do roteador do KIT e alterou-se o seu endere\u00e7o de IP para 172.16.0.1. Mais detalhes sobre a escolha deste IP ser\u00e3o fornecidos na Tarefa 1.  </p> <p></p> <p>Tela de configura\u00e7\u00e3o de IP do roteador e de m\u00e1scara de subrede</p>"},{"location":"roteiro1/main/#parte-1-instalacao-do-ubuntu-server","title":"Parte 1: Instala\u00e7\u00e3o do Ubuntu Server","text":"<p>Para a realiza\u00e7\u00e3o desta primeira tarefa, foram seguidos os passos descritos a seguir para configurar a NUC main:</p> <ol> <li> <p>Download da imagem do Ubuntu Server </p> <ul> <li>Download da vers\u00e3o 22.04 LTS do Ubuntu Server a partir do site oficial do Ubuntu.</li> </ul> </li> <li> <p>Cria\u00e7\u00e3o um pendrive boot\u00e1vel </p> <ul> <li>Uso do software Rufus para gravar a imagem no pendrive.</li> </ul> </li> <li> <p>Acesso \u00e0 BIOS </p> <ul> <li>Com o Pendrive conectado, a BIOS foi acessada a partir a tecla <code>F12</code> durante a inicializa\u00e7\u00e3o.  </li> <li>Configura\u00e7\u00e3o de ordem de boot para priorizar o pendrive.  </li> </ul> </li> <li> <p>Configura\u00e7\u00f5es iniciais de instala\u00e7\u00e3o do Ubuntu Server </p> <ul> <li>Reinicializa\u00e7\u00e3o da NUC.  </li> <li>Sele\u00e7\u00e3o da op\u00e7\u00e3o \"Install Ubuntu Server\" no menu inicial.  </li> <li>Sele\u00e7\u00e3o de idioma e layout do teclado.</li> </ul> </li> <li> <p>Configura\u00e7\u00f5es de subrede, IP, Gateway e DNS </p> <ul> <li>M\u00e1scara de Rede: 172.16.0.0/20</li> <li>Endere\u00e7o IP da NUC main</li> <li>Gateway: IP do roteador</li> <li>Name servers: DNS do Insper</li> </ul> </li> </ol> <p></p> <p>Tela de configura\u00e7\u00e3o de M\u00e1scara de Rede, IP, Gateway (roteador) e DNS do Insper  </p> <p>Explica\u00e7\u00e3o te\u00f3rica das configura\u00e7\u00f5es feitas:</p> <p> Para que dispositivos possam estabelecer uma comunica\u00e7\u00e3o entre si, \u00e9 primordial que eles se encontrem na mesma rede. A configura\u00e7\u00e3o da subrede, IP, gateway e servidores DNS permite a correta comunica\u00e7\u00e3o entre os dispositivos e o acesso \u00e0 internet ou a outros servi\u00e7os de rede. </p> <p> A m\u00e1scara de rede define o intervalo de endere\u00e7os IP dispon\u00edveis dentro da subrede. No caso da configura\u00e7\u00e3o com a m\u00e1scara `/20`, a rede pode conter at\u00e9 4.096 endere\u00e7os IP, garantindo escalabilidade para futuras expans\u00f5es. </p> <p> O endere\u00e7o IP da NUC main \u00e9 um IP est\u00e1tico atribu\u00eddo manualmente ao servidor principal (nesse caso, 172.16.0.3, pois o switch assumiu o IP 172.16.0.2), garantindo que ele tenha sempre o mesmo endere\u00e7o na rede local, facilitando a administra\u00e7\u00e3o e a comunica\u00e7\u00e3o com outros dispositivos. </p> <p> O gateway corresponde ao IP do roteador (que foi configurado na Tarefa 0 como 172.16.0.1), que atua como a ponte entre a rede interna e redes externas, como a internet. Sem a configura\u00e7\u00e3o correta do gateway, os dispositivos na rede local n\u00e3o conseguiriam acessar servi\u00e7os externos. </p> <p> Os name servers (DNS) s\u00e3o respons\u00e1veis pela resolu\u00e7\u00e3o de nomes de dom\u00ednio, convertendo endere\u00e7os amig\u00e1veis, como `www.google.com`, em endere\u00e7os IP. Utilizar os servidores DNS do Insper garante uma resolu\u00e7\u00e3o eficiente e confi\u00e1vel dentro do ambiente da institui\u00e7\u00e3o. </p> <p>Na tela seguinte, referente ao archive mirror, foram aceitas as configura\u00e7\u00f5es que vieram por padr\u00e3o, conforme a print a seguir:</p> <p></p> <p>Tela de configura\u00e7\u00e3o padr\u00e3o do Ubuntu archive mirror </p> <p>Passo final: Cria\u00e7\u00e3o de usu\u00e1rio e configura\u00e7\u00f5es finais </p> <pre><code>hostname: main\n\nlogin: cloud\n\nsenha: clouda\n\nName Servers (DNS): DNS do Insper\n</code></pre> <p></p> <p>Tela de configura\u00e7\u00e3o do usu\u00e1rio da NUC main segundo as especifica\u00e7\u00f5es passadas</p> <p> Ap\u00f3s a conclus\u00e3o de todos os passos da instala\u00e7\u00e3o, foi realizado um reboot da NUC main e removido o pendrive.  </p>"},{"location":"roteiro1/main/#parte-2-maas-acesso-local","title":"Parte 2: MAAS - acesso local","text":""},{"location":"roteiro1/main/#instalacao","title":"Instala\u00e7\u00e3o","text":"<p> Para a instala\u00e7\u00e3o do MAAS na NUC main (que agora tem um sistema operacional), optou-se pela vers\u00e3o 3.5.3. No terminal do Ubuntu Server, foram utilizados os comandos a seguir: </p> <pre><code>$ sudo apt update &amp;&amp; sudo apt upgrade -y\n\n$ sudo snap install maas --channel=3.5/stable\n\n$ sudo snap install maas-test-db\n</code></pre> <p>Para verificar o devido funcionamento do MAAS instalado, foram realizados dois testes com o comando ping, ilustrados na foto a seguir:</p> <p></p> <p>Tela de teste de funcionamento por meio de pings</p> <p>Ap\u00f3s o teste feito com sucesso, foi realizado um acesso da NUC main via SSH com o comando a seguir (ssh usuario@IP):</p> <pre><code>$ ssh cloud@172.16.0.3\n</code></pre>"},{"location":"roteiro1/main/#configuracao","title":"Configura\u00e7\u00e3o","text":"<p> Dentro da rede local, o MAAS foi inicializado e criou-se o administrador cloud, que ser\u00e1 necess\u00e1rio para poteriormente ser poss\u00edvel acessar o dashboard. Antes da inicializa\u00e7\u00e3o foi necess\u00e1rio realizar um reboot. </p> <pre><code>$ sudo maas init region+rack --maas-url http://172.16.0.3:5240/MAAS --database-uri maas-test-db:///\n\n$ sudo maas createadmin\n</code></pre> <p>Em seguida, foi gerado um par de chaves para autentica\u00e7\u00e3o. Ap\u00f3s gerada, a chave p\u00fablica foi copiada.</p> <pre><code>$ ssh-keygen -t rsa\n\n$ cat ./.ssh/id_rsa.pub\n</code></pre> <p> Utilizando o IP atribu\u00eddo \u00e0 NUC main e a porta padr\u00e3o do MAAS, foi poss\u00edvel acessar o Dashboard via protocolo HTTP (http://172.16.0.3:5240/MAAS). O login foi realizado atrav\u00e9s do admin criado nos passos anteriores. </p> <p></p> <p>Dashboard do MAAS</p> <p> Conforme ilustrado acima, a tela inicial do MAAS apresenta um dashboard com informa\u00e7\u00f5es sobre o estado atual dos servidores gerenciados. O dashboard \u00e9 composto por diversos pain\u00e9is, cada um exibindo informa\u00e7\u00f5es sobre um aspecto espec\u00edfico do ambiente gerenciado.  </p> <p> Utilizando a interface gr\u00e1fica do Dashboard, primeiramente, foi configurado um DNS forwarder utilizando o DNS do Insper (Networking &gt; DNS). </p> <p> Em seguida, foram importadas imagens do Ubuntu 22.04 LTS e Ubuntu 20.04 LTS em Configuration &gt; Images &gt; Ubuntu Releases e feito o upload da chave copiada no terminal SSH.  </p> <p> Por fim, foi passado o par\u00e2metro kernel net.ifnames=0 em Settings &gt; Configuration &gt; Kernel Parameters. </p>"},{"location":"roteiro1/main/#chaveando-o-dhcp","title":"Chaveando o DHCP","text":"<p> O DHCP (Dynamic Host Configuration Protocol) \u00e9 um protocolo de rede que permite a configura\u00e7\u00e3o autom\u00e1tica de dispositivos em uma rede IP. Ele \u00e9 principalmente respons\u00e1vel por atribuir dinamicamente endere\u00e7os IP, eliminando a necessidade de configura\u00e7\u00e3o manual, mas tamb\u00e9m pode assumir fun\u00e7\u00f5es como, por exemplo, definir a m\u00e1scara de sub-rede e fornecer servidores DNS. </p> <p> At\u00e9 o momento, o dispositivo da nossa sub-rede contendo este protocolo \u00e9 o roteador. Por\u00e9m, nesta etapa isto foi modificado. </p> <p> Primeiramente, dentro do MAAS Controller, o DHCP foi habilitado na NUC Main e, conforme ilustra a imagem a seguir, o Reserved Range foi alterado para iniciar em 172.16.11.1 e acabar em 172.16.14.255. </p> <p></p> <p>Tela de configura\u00e7\u00e3o dos Reserved Ranges dentro do MAAS Controller</p> <p> Al\u00e9m disso, como mais de um dispositivo n\u00e3o pode conter o protocolo DHCP dentro de uma mesma sub-rede (mais de um dispositivo tentando atribuir um IP a outro dispositivo automaticamente), tamb\u00e9m foi necess\u00e1rio desativar o DHCP no roteador. </p> <p></p> <p>Tela de desabilita\u00e7\u00e3o do DHCP no roteador</p> <p>A sa\u00fade do sistema tamb\u00e9m foi verificada a partir da p\u00e1gina de Controladores no Dashboard, ilustrada a seguir.</p> <p></p> <p>Tela de verifica\u00e7\u00e3o da sa\u00fade do sistema</p>"},{"location":"roteiro1/main/#comissionando-servidores","title":"Comissionando servidores","text":"<p> Com o DHCP agora devidamente chaveado, os servers 1 a 5 foram cadastrados como machines no Dashboard do MAAS.  Para tanto, foram resgatados os endere\u00e7os MAC capturados na Tarefa 0, alterada a op\u00e7\u00e3o Power Type para Intel AMT, configurada a senha `CloudComp6s!` para todos os servidores e o IP 172.16.15.X (X sendo o n\u00famero do servidor configurado). </p> <p> Ap\u00f3s a comiss\u00e3o autom\u00e1tica, todos os n\u00f3s apareceram com o status Ready e as especifica\u00e7\u00f5es de armazenamento das NUCs foram confirmadas. Al\u00e9m disso, o roteador foi adicionado como device. </p> <p></p> <p>Adicionando roteador como device</p>"},{"location":"roteiro1/main/#ovs-bridge","title":"OVS Bridge","text":"<p> Antes de possibilitar o acesso remoto ao KIT, um passo final foi criar, para cada servidor, uma ponte Open vSwitch (OVS). Todas as pontes tiveram o nome \"br-ex\" atribu\u00eddo a elas. A seguir, tem-se uma ilustra\u00e7\u00e3o de como a bridge ficou configurada para o server 1, na interface do dashboard do MAAS. </p> <p></p> <p>Interface do Server 1 ap\u00f3s a cria\u00e7\u00e3o da ponte Open vSwitch (OVS)</p>"},{"location":"roteiro1/main/#parte-3-maas-acesso-remoto","title":"Parte 3: MAAS - acesso remoto","text":"<p>Para que seja poss\u00edvel acessar o KIT remotamente, e n\u00e3o mais atrav\u00e9s do cabo na rede local, \u00e9 necess\u00e1rio realizar a cria\u00e7\u00e3o de um gateway NAT. </p> <p> A inten\u00e7\u00e3o por tr\u00e1s do acesso remoto \u00e9 que o computador seja capaz de conversar com o servidor main dentro da subrede configurada at\u00e9 agora apenas por meio de uma conex\u00e3o com a Rede Wi-Fi do Insper.  </p> <p> O NAT \u00e9 justamente o servi\u00e7o que possibilita que dispositivos dentro de uma rede privada acessem redes externas (como a internet) atrav\u00e9s de um \u00fanico endere\u00e7o IP p\u00fablico. Ele traduz os endere\u00e7os IP privados da subrede para o endere\u00e7o IP do roteador ao enviar pacotes para fora da rede e realiza o processo inverso ao receber respostas, garantindo assim a comunica\u00e7\u00e3o adequada entre os dispositivos. </p> <p></p> <p>Ilustra\u00e7\u00e3o te\u00f3rica de funcionamento da subrede</p> <p>Na ilustra\u00e7\u00e3o acima, a linha tracejada verde representa a conex\u00e3o entre um computador (que se encontra conectado \u00e0 LAN do Insper) com a NUC main da subrede privada que foi configurada at\u00e9 o momento. </p> <p> Para que fosse estabelecida tal conex\u00e3o, foram seguidas as instru\u00e7\u00f5es contidas no manual de uso do roteador do KIT para possibilitar o redirecionamento do dispositivo que tentasse se conectar \u00e0 portas 22 (padr\u00e3o SSH) e 5240 (padr\u00e3o MAAS) a partir do IP p\u00fablico 10.103.1.10, que \u00e9 o endere\u00e7o do roteador fora da rede privada. </p> <p>Ao final da configura\u00e7\u00e3o do NAT, portanto, a interface do roteador ficou da seguinte forma:</p> <p></p> <p>Interface do roteador ap\u00f3s a configura\u00e7\u00e3o do NAT</p> <p>A partir deste momento, portanto, passou a ser poss\u00edvel conectar-se ao dashboard do MAAS e aos demais servidores por meio do seguinte comando SSH no terminal do computador:</p> <pre><code>$ ssh cloud@10.103.1.10\n</code></pre>"},{"location":"roteiro1/main/#app","title":"App","text":"<p>Na parte da aplica\u00e7\u00e3o deste primeiro roteiro, foi realizado um deploy manual de uma aplica\u00e7\u00e3o simples em Django nos servidores.</p>"},{"location":"roteiro1/main/#parte-1-criacao-do-banco-de-dados","title":"Parte 1: Cria\u00e7\u00e3o do banco de dados","text":"<p>Acessando o terminal do server 1 via SSH, foi criado um usu\u00e1rio (senha: cloud) por meio dos comandos:</p> <pre><code>$ sudo apt update\n\n$ sudo apt install postgresql postgresql-contrib -y\n\n$ sudo su - postgres\n\n$ createuser -s cloud -W\n</code></pre> <p>Em seguida, foi criado o banco de dados e exposto o servi\u00e7o para acesso:</p> <pre><code>$ createdb -O cloud tasks\n\n$ nano /etc/postgresql/14/main/postgresql.conf\n</code></pre> <p>Saindo do usu\u00e1rio postgres, liberou-se o firewall e o servi\u00e7o foi reiniciado:</p> <pre><code>$ sudo ufw allow 5432/tcp\n\n$ sudo systemctl restart postgresql\n</code></pre> <p>Tarefa 1.1) Dentro do server1, status do banco de dados se mostra ativo.</p> <pre><code>$ sudo systemctl status postgresql\n</code></pre> <p></p> <p>Status do PostgreSQL vendo do server1</p> <p>Tarefa 1.2) Inicia a sess\u00e3o e utiliza do computador/servi\u00e7o remotamente, atrav\u00e9s da porta 5240, na MAIN. Servi\u00e7o acess\u00edvel da MAIN.</p> <pre><code>$ telnet localhost 5240\n</code></pre> <p></p> <p>Conex\u00e3o estabelecida entre a MAIN e o servi\u00e7o remoto</p> <p>Tarefa 1.3) Servi\u00e7o acess\u00edvel na pr\u00f3pria m\u00e1quina onde o postgresql foi instalado.</p> <pre><code>$ sudo su - postgres\n</code></pre> <p></p> <p>PostgreSQL acess\u00edvel de dentro do server em que est\u00e1 alocado</p> <p>Tarefa 1.4) Acessando a configura\u00e7\u00e3o do postgresql, foi poss\u00edvel verificar a porta na sess\u00e3o \u2018CONNECTIONS AND AUTHENTICATION\u2019</p> <pre><code>Comando: $ nano /etc/postgresql/14/main/postgresql.conf\n</code></pre> <p></p> <p>Configura\u00e7\u00f5es do PostgreSQL (default)</p>"},{"location":"roteiro1/main/#parte-2-aplicacao-django","title":"Parte 2: Aplica\u00e7\u00e3o Django","text":"<p> Ap\u00f3s requisitar acesso a uma m\u00e1quina em nosso servidor (Comando 1), e inserir o token da aba \u2018API keys\u2019 presente no Dashboard, solicitamos ao MaaS a aloca\u00e7\u00e3o de uma m\u00e1quina (Comando 2) e realizamos o deploy da nossa aplica\u00e7\u00e3o (Comando 3), sendo \u2018system_id\u2019 o id do server alocado, vis\u00edvel no link do Dashboard ao clicar na m\u00e1quina desejada. </p> <pre><code>$ maas login [login] http://172.16.0.3:5240/MAAS/    # Requisi\u00e7\u00e3o de m\u00e1quina\n$ maas [login] machines allocate name=[server_name]  # Solicita aloca\u00e7\u00e3o a MAIN\n$ maas [login] machine deploy [system_id]            # Deploy da aplica\u00e7\u00e3o\n</code></pre> <p> Acessando o servidor via SSH, clonamos o reposit\u00f3rio onde teremos a aplica\u00e7\u00e3o Django. Entrando no diret\u00f3rio tasks, fazemos a instala\u00e7\u00e3o das depend\u00eancias do reposit\u00f3rio.  </p> <pre><code>$ git clone https://github.com/raulikeda/tasks.git\n\n$ ./install.sh # Instala\u00e7\u00e3o das depend\u00eancias\n</code></pre> <p> Ap\u00f3s um breve reboot da m\u00e1quina, iremos acessar o arquivo \u2018/etc/hosts\u2019 para darmos permiss\u00e3o a nossa MAIN de utilizar a aplica\u00e7\u00e3o como administrador. Podemos verificar a conex\u00e3o com a aplica\u00e7\u00e3o com o comando: </p> <pre><code>$ wget http://[IP server_app]:8080/admin/ # Verificando conex\u00e3o com a aplica\u00e7\u00e3o\n</code></pre> <p> Agora, ao acessar o MaaS podemos criar um t\u00fanel do servi\u00e7o do servidor da aplica\u00e7\u00e3o na porta 8080 para nosso localhost na porta 8001 usando a conex\u00e3o SSH, desde que a porta 8001 n\u00e3o esteja sendo utilizada. Podemos ent\u00e3o acessar a p\u00e1gina de administrador do Django acessando no navegador o link: http://localhost:8001/admin/. </p> <pre><code>$ ssh cloud@10.103.0.X -L 8001:[IP server_app]:8080 # conectando via SSH\n</code></pre> <p>Tarefa 2.1) Dashboard do MAAS com as m\u00e1quinas.</p> <p></p> <p>Servidores na interface do MAAS</p> <p>Tarefa 2.2) Aba imagens, com as imagens sincronizadas</p> <p></p> <p>Imagens sincronizadas na interface do MAAS</p> <p>Tarefa 2.3) Da Aba de cada maquina mostrando os testes de hardware e commissioning com Status \"OK\"</p> <p>Server 1:</p> <p></p> <p>Testes de hardware do server 1</p> <p></p> <p>Comissioning do server 1</p> <p>Server 2:</p> <p></p> <p>Testes de hardware do server 2</p> <p></p> <p>Comissioning do server 2</p> <p>Server 3:</p> <p></p> <p>Testes de hardware do server 3</p> <p></p> <p>Comissioning do server 3</p> <p>Server 4:</p> <p></p> <p>Testes de hardware do server 4</p> <p></p> <p>Comissioning do server 4</p> <p>Server 5:</p> <p></p> <p>Testes de hardware do server 5</p> <p></p> <p>Comissioning do server 5</p>"},{"location":"roteiro1/main/#parte-3-checkpoint-status-dos-servidores-apos-a-instalacao-manual-do-django","title":"Parte 3: CHECKPOINT - Status dos servidores ap\u00f3s a instala\u00e7\u00e3o manual do Django","text":"<p> Com as mudan\u00e7as feitas na parte 2, antes de continuarmos precisamos garantir algumas coisas: </p> <p>Tarefa 3.1) As 2 M\u00e1quinas se mostram ativas e com seus IPs definidos no Dashboard do MaaS</p> <p></p> <p>Servidores na interface do MAAS</p> <p>Tarefa 3.2) A aplica\u00e7\u00e3o Django se encontra no ar, est\u00e1 conectada ao server e acess\u00edvel a partir do t\u00fanel</p> <p></p> <p>Aplica\u00e7\u00e3o Django conectada ao server e acess\u00edvel atrav\u00e9s do tunel</p>"},{"location":"roteiro1/main/#parte-4-ansible-deploy-automatizado-de-aplicacao","title":"Parte 4: Ansible - deploy automatizado de aplica\u00e7\u00e3o","text":"<p> Vamos partir para uma abordagem diferente agora. At\u00e9 o momento, temos apenas uma aplica\u00e7\u00e3o Django que foi instalada manualmente apenas no servidor 2. Contudo, \u00e9 comum que uma mesma aplica\u00e7\u00e3o seja alocada em mais de uma m\u00e1quina, pois podemos dividir a carga de acesso entre os n\u00f3s e, al\u00e9m disso, se um node cair o outro est\u00e1 no ar, para que nosso cliente acesse. </p> <p> Dessa forma, nesta parte foi criada uma segunda aplica\u00e7\u00e3o Django no servidor 3 que compartilha com o servidor 2 o mesmo banco de dados criado no servidor 1. Por\u00e9m, em vez de realizar os mesmos passos da instala\u00e7\u00e3o manual feita na parte 2, optou-se por utilizar o Ansible, um gerenciador de deploy que traz benef\u00edcios que ser\u00e3o detalhados mais adiante. </p> <p> Ap\u00f3s feito o deploy do terceiro servidor, o Ansible foi instalado na MAIN e um playbook foi criado no n\u00f3 3 para a instala\u00e7\u00e3o do Django. Os comandos utilizados foram: </p> <pre><code>$ sudo apt install ansible # Instala\u00e7\u00e3o do Ansible\n\n$ wget https://raw.githubusercontent.com/raulikeda/tasks/master/tasks-install-playbook.yaml # Playbook do Ansible\n\n$ ansible-playbook tasks-install-playbook.yaml --extra-vars server=[IP server3] # Instala\u00e7\u00e3o do playbook na m\u00e1quina 3\n</code></pre> <p>Tarefa 4.1) Print da tela do Dashboard do MAAS com as 3 M\u00e1quinas e seus respectivos IPs.</p> <p></p> <p>Dashboard do MaaS com os tr\u00eas servidores com deploy feito e seus respectivos IPs</p> <p>Tarefa 4.2) Print da aplica\u00e7\u00e3o Django, provando a conex\u00e3o com o server2</p> <p></p> <p>Aplica\u00e7\u00e3o Django funcionando localmente via servidor 2</p> <p>Tarefa 4.3) Print da aplica\u00e7\u00e3o Django, provando a conex\u00e3o com o server3</p> <p></p> <p>Aplica\u00e7\u00e3o Django funcionando localmente via servidor 3</p> <p>Tarefa 4.4) Diferen\u00e7a entre instalar manualmente a aplica\u00e7\u00e3o Django e utilizando o Ansible</p> <p> A diferen\u00e7a principal entre instalar manualmente uma aplica\u00e7\u00e3o Django em um host e utilizar o Ansible est\u00e1 na automa\u00e7\u00e3o, repetibilidade e efici\u00eancia da configura\u00e7\u00e3o do ambiente. </p> <p> No lugar de ter que manualmente configurar um ambiente virtual, instalar as depend\u00eancias necess\u00e1rias (armazenadas em um requirements.txt) e pacotes como Python, pip e depend\u00eancias do Django, o Ansible permite automatizar todo esse processo atrav\u00e9s de playbooks. </p> <p> Al\u00e9m de automatizar todas as etapas da instala\u00e7\u00e3o e configura\u00e7\u00e3o, o Ansible segue instru\u00e7\u00f5es precisas que proporcionam um gerenciamento de m\u00faltiplos servidores ao mesmo tempo, que \u00e9 \u00fatil para escalar a aplica\u00e7\u00e3o, e garantem um processo id\u00eantico de configura\u00e7\u00e3o na necessidade de mais m\u00e1quinas que cuidam da aplica\u00e7\u00e3o.  </p> <p> A grande vantagem de utilizar esta ferramenta \u00e9, portanto, a sua capacidade de superar uma abordagem manual que \u00e9 propensa a erros humanos, demorada e dif\u00edcil de reproduzir no momento de expans\u00e3o para outros servidores. </p>"},{"location":"roteiro1/main/#parte-5-balancamento-de-carga-usando-proxy-reverso","title":"Parte 5: Balancamento de carga usando Proxy Reverso","text":"<p> Agora que fizemos a instala\u00e7\u00e3o do Django, tanto manualmente como utilizando o Ansible, podemos criar um servidor para agir como um ponto \u00fanico de entrada, verificando a disponibilidade de cada server e garantindo uma melhor experi\u00eancia de uso para o usu\u00e1rio. O nome deste mecanismo \u00e9 Loadbalancing e ele \u00e9 especialmente \u00fatil para realizar esta distribui\u00e7\u00e3o de tr\u00e1fego de entrada por v\u00e1rios servidores privados, garantindo toler\u00e2ncia a falhas e maior estabilidade. </p> <p> Para isso, vamos utilizar o NGINX, que usa o algortimo Round Robin para balanceamento de carga para um conjunto IPs dispon\u00edveis cadastrados. Ele \u00e9 relativamente simples de implementar e n\u00e3o considera fatores como tempo de resposta do servidor e a regi\u00e3o geogr\u00e1fica de acesso, mas ser\u00e1 suficiente para nossa aplica\u00e7\u00e3o. </p> <p> Iniciando, vamos fazer o deploy de um novo servidor (server4), que ir\u00e1 realizar fazer este trabalho de Loadbalancing para os servidores em que instalamos nossas aplica\u00e7\u00f5es Django, em nosso Dashboard do MaaS. </p> <p>Tarefa 5.1) Print da tela do Dashboard do MAAS com as 4 M\u00e1quinas e seus respectivos IPs.</p> <p></p> <p>Dashboard do MaaS com os quatro servidores com deploy feito e seus respectivos IPs</p> <p> Ap\u00f3s terminar o deploy, vamos acessar o servidor 4 via SSH e, em seu terminal, vamos realizar a instala\u00e7\u00e3o do nginx. </p> <pre><code>$ ssh ubuntu@{IP Server4}\n$ sudo apt-get install nginx\n</code></pre> <p> Para definir quais servidores o Loadbalancer Round Robin utilizar\u00e1 para o balanceamento de cargas, precisamos editar o arquivo de configura\u00e7\u00e3o de sites dispon\u00edveis do Nginx: </p> <pre><code>$ sudo nano /etc/nginx/sites-available/default\n</code></pre> <p> Primeiro, vamos adicionar, antes do bloco server, um bloco upstream para que o nginx saiba quais servidores ser\u00e3o aqueles que iremos utilizar como backend para tratar as requisi\u00e7\u00f5es recebidas. No nosso caso, deixaremos assim: </p> <pre><code>upstream backend {\n    server2 {IP server2}:8080;\n    server3 {IP server3}:8080;\n}\n</code></pre> <p> Depois, vamos substituir o bloco server presente no arquivo por padr\u00e3o e vamos configura-lo para ficar escutando a porta 80 e para utilizar um proxy_pass para direcionar as requisi\u00e7\u00f5es aos server backend. </p> <pre><code>server {\n    listen 80;\n\n    location / {\n        proxy_pass http://backend;\n    }\n}\n</code></pre> <p> Para registrar as altera\u00e7\u00f5es, precisamos dar um restart no nginx com o seguinte comando: </p> <pre><code>$ sudo service nginx restart\n</code></pre> <p> Pronto! O nginx j\u00e1 est\u00e1 configurado e pronto uso. </p> <p>  Agora, s\u00f3 nos falta verificar a conex\u00e3o com os servidores 2 e 3 a partir do servidor 4. Para isso vamos iniciar modificando o views.py de cada servidor onde instalamos o Django. Vamos trocar o print presente, para podermos identificar se estamos conectados no servidor 2 ou no servidor 3. Veja um exemplo para o servidor 2: </p> <pre><code>$ sudo nano tasks/tasks/views.py\n</code></pre> <pre><code># Exemplo de resolu\u00e7\u00e3o\nfrom django.shortcuts import render\n\nfrom django.http import HttpResponse\n\ndef index(request):\n\n  return HttpResponse(\"Hello from server 2!\")\n</code></pre> <p>Tarefa 5.2) Conte\u00fado da mensagem contida na fun\u00e7\u00e3o <code>index</code> do arquivo <code>tasks/views.py</code> de cada server para distinguir ambos os servers.</p> <p></p> <p>Conex\u00e3o com o server2 a partir do t\u00fanel</p> <p></p> <p>Conex\u00e3o com o server3 a partir do t\u00fanel</p> <p>  Ap\u00f3s fazer a mesma coisa para o servidor 3, vamos sair dos servidores, incluindo a main, usando o comando: </p> <pre><code>$ exit\n</code></pre> <p>  Vamos acessar novamente a main, por\u00e9m utilizando uma pipeline para o server4: </p> <pre><code>$ ssh cloud@{IP server1} -L 8081:{IP server4}:80\n</code></pre> <p>  Este comando, basicamente, cria uma conex\u00e3o SSH com redirecionamento de porta, se conectando com o server1 (ssh cloud@{IP server1}), com redirecionamento de porta local (-L) e fazendo um t\u00fanel que: </p> <li>  Toda conex\u00e3o feita na porta 8081 da m\u00e1quina local \u00e9 encaminhada para o server1. </li> <li>  A partir de server1, o tr\u00e1fego \u00e9 redirecionado para o server4 na porta 80. </li> <p> OBS: Todas as portas mencionadas aqui, foram setadas e configuradas previamente no roteiro. </p> <p>  Com a nginx configurado e o t\u00fanel criado, podemos acessar ambos os servidores em nosso navegador com o link: </p> <li> localhost:8081/tasks/ </li> <p>  Pronto! Agora podemos verificar que ao acessar este link, uma hora somos recebidos com a mensagem do server 2, outra hora com a mensagem do server 3. </p> <p>Tarefa 5.3) Prints das respostas de cada request, provando que voce est\u00e1 conectado ao server 4, que \u00e9 o Proxy Reverso e que ele bate cada vez em um server diferente server2 e server3.</p> <p></p> <p>Conex\u00e3o com o server2 a partir do t\u00fanel</p> <p></p> <p>Conex\u00e3o com o server3 a partir do t\u00fanel</p>"},{"location":"roteiro2/main/","title":"Roteiro 2","text":""},{"location":"roteiro2/main/#introducao-e-objetivo","title":"Introdu\u00e7\u00e3o e Objetivo","text":"<p> O roteiro 2 contempla uma abordagem nova em rela\u00e7\u00e3o ao roteiro 1: a utiliza\u00e7\u00e3o de uma nova plataforma de gerenciamento de aplica\u00e7\u00f5es distribu\u00eddas. Assim, em vez de realizar as instala\u00e7\u00f5es de toda a infraestrutura manualmente (conforme foi feito no roteiro anterior), ser\u00e1 utilizado o Juju, um outro orquestrador de deploy que integra com o MaaS. </p> <p> Por conta desta nova abordagem, todas as m\u00e1quinas contendo as modifica\u00e7\u00f5es do roteiro 1 foram liberadas, retornando para o status \"Ready\" no dashboard do MaaS. </p> <p> Ao final deste roteiro, o objetivo principal \u00e9 termos, portanto, uma Cloud com um novo gerenciador de deploy instalado. </p>"},{"location":"roteiro2/main/#montagem-do-roteiro","title":"Montagem do Roteiro","text":"<p> Todo roteiro apresenta uma primeira parte denonimnada Infra e uma segunda chamada de App. Os pontos tarefas dentro de cada parte s\u00e3o os passos seguidos para a realiza\u00e7\u00e3o do roteiro.  Este modelo de organiza\u00e7\u00e3o orientado por partes e tarefas ser\u00e1 utilizado em todos os roteiros. </p>"},{"location":"roteiro2/main/#infra","title":"Infra","text":""},{"location":"roteiro2/main/#parte-1-instalacao-do-juju","title":"Parte 1: Instala\u00e7\u00e3o do Juju","text":"<p> A instala\u00e7\u00e3o do Juju foi realizada na m\u00e1quina main, acessada via SSH. O seguinte comando foi utilizado: </p> <pre><code>$ sudo snap install juju --channel 3.6\n</code></pre>"},{"location":"roteiro2/main/#parte-2-arquivos-de-definicao-de-cloud","title":"Parte 2: Arquivos de defini\u00e7\u00e3o de cloud","text":"<p> Como o Juju utilizar\u00e1 o MaaS como provedor de m\u00e1quinas e sistema operacional, inicialmente foi necess\u00e1rio garantir que o Juju reconhecesse o MaaS como um provedor de recursos v\u00e1lido. </p> <p>Ap\u00f3s essa verifica\u00e7\u00e3o, criou-se um arquivo de configura\u00e7\u00e3o chamado maas-cloud.yaml com o seguinte conte\u00fado:</p> <pre><code>  clouds:\n    maas-one:\n      type: maas\n      auth-types: [oauth1]\n      endpoint: http://192.168.0.3:5240/MAAS/\n</code></pre> <p>Em seguida, foi adicionada a cloud, utilizando o seguinte comando:</p> <pre><code>$ juju add-cloud --client -f maas-cloud.yaml maas-one\n</code></pre> <p>Por fim, foi necess\u00e1rio adicionar as credenciais MaaS para que o Juju pudesse interagir com a nova cloud adicionada. Um novo arquivo, denominado como maas-creds.yaml, foi criado com esta finalidade:</p> <pre><code>credentials:\n  maas-one:\n  anyuser:\n    auth-type: oauth1\n    maas-oauth: &lt;API KEY gerado no menu do usu\u00e1rio do MaaS&gt;\n</code></pre> <p>Essas credenciais foram aplicadas com o comando:</p> <pre><code>$ juju add-credential --client -f maas-creds.yaml maas-one\n</code></pre>"},{"location":"roteiro2/main/#parte-3-criacao-do-controlador","title":"Parte 3: Cria\u00e7\u00e3o do controlador","text":"<p>Para finalizar a infraestrutura necess\u00e1ria, foi criado um controlador no server 1 da nossa rede privada.</p> <p> Para que o Juju saiba em qual servidor o controlador ir\u00e1 ficar, criou-se a tag 'juju' no server 1 atrav\u00e9s dashboard do Maas e, em seguida, foi executado o comando a seguir: </p> <pre><code>$ juju bootstrap --bootstrap-series=jammy --constraints tags=juju maas-one maas-controller\n</code></pre> <p> Esse comando utiliza a s\u00e9rie jammy e define o nome do controlador como maas-controller, vinculado \u00e0 cloud maas-one. </p>"},{"location":"roteiro2/main/#app","title":"App","text":"<p>Para uma melhor visualiza\u00e7\u00e3o do passo a passo feito a seguir, recomendamos manter um terminal aberto ao lado rodando o seguinte comando:</p> <pre><code>$ watch -n 1 -c juju status --color\n</code></pre> <p>Nele, ser\u00e1 poss\u00edvel visualizar cada mudan\u00e7a feita na infraestrutura em tempo real.</p>"},{"location":"roteiro2/main/#parte-1-instalacao-do-juju-dashboard-para-o-controller","title":"Parte 1: Instala\u00e7\u00e3o do Juju dashboard para o controller","text":"<p> Para a instala\u00e7\u00e3o do juju dashboard, deve-se, primeiro, ter certeza de que estamos no controlador e, sem seguida, executar o comando necess\u00e1rio: </p> <pre><code>$ juju switch maas-controller:admin/maas\n\n$ juju deploy juju-dashboard dashboard\n</code></pre>"},{"location":"roteiro2/main/#parte-2-deploy-da-aplicacao-grafana-e-prometheus","title":"Parte 2: Deploy da aplica\u00e7\u00e3o Grafana e Prometheus","text":"<p>Com o Juju dashboard instalado, prosseguimos para a aplica\u00e7\u00e3o e a configura\u00e7\u00e3o do banco de dados.</p> <p> Neste contexto, ser\u00e1 utilizado o Prometheus como banco de dados e o Grafana como plataforma de apresenta\u00e7\u00e3o visual dos n\u00fameros (tipicamente gr\u00e1ficos e pain\u00e9is). </p> <p> O primeiro passo tomado foi a cria\u00e7\u00e3o um novo modelo, que chamaremos de <code>openstack</code>, onde iremos baixar e realizar as configura\u00e7\u00f5es das aplica\u00e7\u00f5es. </p> <pre><code>$ juju add-model --config default-series=jammy openstack\n\n$ juju switch openstack \n</code></pre> <p> Ap\u00f3s isso, foi criado uma pasta chamada charms para baixar o charm do Grafana e do Prometheus do reposit\u00f3rio charm-hub: </p> <pre><code>$ mkdir -p /home/cloud/charms\n\n$ cd /home/cloud/charms\n</code></pre> <p>Em seguida, foi realizado o download das duas ferramentas:</p> <pre><code>$ juju download grafana\n\n$ juju download prometheus2\n</code></pre> <p>E, por fim, o deploy dos charms de cada uma:</p> <pre><code>$ juju deploy ./grafana_r69.charm --base=ubuntu@20.04\n\n$ juju deploy ./prometheus2_r60.charm\n</code></pre> <p>Nota:  Nos \u00faltimos comandos, utilizamos o deploy do grafana em uma vers\u00e3o espec\u00edfica, para <code>ubuntu@20.04</code>, por conta de problemas de compatibilidade com vers\u00f5es mais novas.</p>"},{"location":"roteiro2/main/#parte-3-integracao-do-grafana-com-o-prometheus","title":"Parte 3: Integra\u00e7\u00e3o do Grafana com o Prometheus","text":"<p>Para que o Grafana mostre os dados contidos no Prometheus, foi utilizado o seguinte comando de integra\u00e7\u00e3o:</p> <pre><code>$ juju integrate grafana:grafana-source prometheus2:grafana-source\n</code></pre> <p></p> <p>Dashboard do MAAS com as m\u00e1quinas e seus respectivos IPs</p> <p></p> <p>Tela do comando \"juju status\" com o Grafana ativo</p> <p>Se desconectando da m\u00e1quina MAIN, demos um <code>ssh</code> para entrar na MAIN novamente, por\u00e9m fazendo um t\u00fanel da porta do Grafana (<code>3000</code> por padr\u00e3o) para uma porta da nossa localhost (<code>8001</code>):</p> <pre><code>$ ssh cloud@10.103.1.X -L 8001:{IP Server Grafana}:3000\n</code></pre> <p>Acessando o Dashboard em nosso navegador (localhost:8001/login), foi utilizado o seguinte procedimento para fazer o login:</p> <ul> <li>Login:        <ul> <li>             Por padr\u00e3o: <code>admin</code> </li> </ul> </li> <li>Senha:        <ul> <li>             Para obter a senha do Grafana, temos que pedir ao juju para rodar o comando <code>get-admin-password</code> na unidade onde est\u00e1 o servi\u00e7o:           </li> </ul> </li> </ul> <pre><code>$ juju run grafana/1 get-admin-password\n</code></pre> <p>Agora, dentro do Dashboard do <code>Grafana</code>, o \u00faltimo passo foi conferir se a integra\u00e7\u00e3o foi feita corretamente. Para isso, foi criado um dashboard dentro do Grafana e foi selecionado o <code>Prometheus</code> como source.</p> <p></p> <p>Tela do Dashboard do Grafana com o Prometheus aparecendo como source</p> <p></p> <p>Acesso ao Dashboard do Grafana a partir da rede do Insper</p> <p></p> <p>Aplica\u00e7\u00f5es sendo gerenciadas pelo Juju </p> <p> Para seguir adiante para o pr\u00f3ximo roteiro, o controlador foi deletado usando o comando juju destroy-controller main e o ambiente foi, novamente, reconfigurado para a estaca zero. </p>"},{"location":"roteiro3/main/","title":"Roteiro 3","text":""},{"location":"roteiro3/main/#introducao-e-objetivo","title":"Introdu\u00e7\u00e3o e Objetivo","text":""},{"location":"roteiro3/main/#montagem-do-roteiro","title":"Montagem do Roteiro","text":""},{"location":"roteiro3/main/#infra","title":"Infra","text":""},{"location":"roteiro3/main/#app","title":"App","text":""},{"location":"roteiro3/main/#questionario-projeto-ou-plano","title":"Question\u00e1rio, Projeto ou Plano","text":"<p>Esse se\u00e7\u00e3o deve ser preenchida apenas se houver demanda do roteiro.</p>"},{"location":"roteiro3/main/#discussoes","title":"Discuss\u00f5es","text":"<p>Quais as dificuldades encontradas? O que foi mais f\u00e1cil? O que foi mais dif\u00edcil?</p>"},{"location":"roteiro3/main/#conclusao","title":"Conclus\u00e3o","text":"<p>O que foi poss\u00edvel concluir com a realiza\u00e7\u00e3o do roteiro?</p>"},{"location":"roteiro4/main/","title":"Roteiro 4","text":""},{"location":"roteiro4/main/#introducao-e-objetivo","title":"Introdu\u00e7\u00e3o e Objetivo","text":""},{"location":"roteiro4/main/#montagem-do-roteiro","title":"Montagem do Roteiro","text":""},{"location":"roteiro4/main/#infra","title":"Infra","text":""},{"location":"roteiro4/main/#app","title":"App","text":""},{"location":"roteiro4/main/#questionario-projeto-ou-plano","title":"Question\u00e1rio, Projeto ou Plano","text":"<p>Esse se\u00e7\u00e3o deve ser preenchida apenas se houver demanda do roteiro.</p>"},{"location":"roteiro4/main/#discussoes","title":"Discuss\u00f5es","text":"<p>Quais as dificuldades encontradas? O que foi mais f\u00e1cil? O que foi mais dif\u00edcil?</p>"},{"location":"roteiro4/main/#conclusao","title":"Conclus\u00e3o","text":"<p>O que foi poss\u00edvel concluir com a realiza\u00e7\u00e3o do roteiro?</p>"}]}